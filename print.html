<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AI Resources</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="preface.html"><strong aria-hidden="true">1.</strong> Preface</a></li><li class="chapter-item expanded "><a href="vocabulary.html"><strong aria-hidden="true">2.</strong> Vocabulary</a></li><li class="chapter-item expanded "><a href="topics.html"><strong aria-hidden="true">3.</strong> ML/DL Topics</a></li><li class="chapter-item expanded "><a href="nlp.html"><strong aria-hidden="true">4.</strong> NLP</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">5.</strong> To Production</a></li><li class="chapter-item expanded "><a href="frameworks.html"><strong aria-hidden="true">6.</strong> Frameworks</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">7.</strong> Books and Resources</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">8.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">9.</strong> Applications</a></li><li class="chapter-item expanded "><a href="refs.html"><strong aria-hidden="true">10.</strong> References</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#preface" id="preface">Preface</a></h1>
<h1><a class="header" href="#a" id="a">A</a></h1>
<h2><a class="header" href="#auto-encoder" id="auto-encoder">Auto-Encoder</a></h2>
<p>Autoencoders are unsupervised ANN that can learn data encodings, making the encoder generate those encodings 
specifically for reconstructing its own input (See figure below.) They convert their inputs to encoded vectors that lie
in a latent space that may not be continuous or allow easy interpolation. In the end, this means that regular autoencoders 
are mostly limited to be used to generate compressed representations of their inputs, allowing to regenerate the original
input with minimal loss.</p>
<p><img src="images/autoencoder.png" alt="Autoencoder (Source: https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)" /></p>
<h1><a class="header" href="#g" id="g">G</a></h1>
<h2><a class="header" href="#generative-adversarial-network-gan" id="generative-adversarial-network-gan">Generative Adversarial Network (GAN)</a></h2>
<p>A special architecture of ANN aimed to <em>generate new data</em> with similar statistics as of the ones found in a particular 
training set. The classical example of what GANs are used for, is the generation of new faces, by interpolating new
features from the data obtained from a pool of preexisting images of faces. The goal is to build the new images as real 
as possible, making them undistinguisable from real images for the human eye.</p>
<p>The idea is to train two models at the same time; the first one is the &quot;generative&quot; one, which serves as the &quot;trend 
gatherer&quot;, that is capturing the data distribution; the second one model, called &quot;discriminative&quot;, is trained to discern
if a particular sample comes from the training data or from the &quot;generative&quot; moidel {{#cite goodfellow_generative_2014}}</p>
<p><img src="images/gan.png" alt="GAN (Source: https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)" /></p>
<h3><a class="header" href="#related" id="related">Related</a></h3>
<ul>
<li><a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/">Ian Goodfellow Interview</a></li>
</ul>
<h1><a class="header" href="#v" id="v">V</a></h1>
<h2><a class="header" href="#variational-auto-encoder" id="variational-auto-encoder">Variational Auto-Encoder</a></h2>
<p>In contrast to a vanilla <a href="vocabulary.html#auto-encoder">autoencoder</a>, a Variational AutoEncoder (VAE) is a <em>generative model</em> 
that shares most of the architecture with a regular autoencoder, like Generative Adversarial Networks. Because of this, 
VAEs have relatively little to do with classical autoencoders (sparse or denoising autoencoders) from a mathematical
point of view.
VAEs have a special property (which we could call the &quot;creativity&quot; property) that makes them more interesting over 
regular autoencoders for generating outputs; their latent spaces are 
continuous by design, which allows random sampling and interpolation. In a generative model this is what you want in the
end; randomly sample from the continuous latent space in order to &quot;distort a bit&quot; the input image generating an image 
variation, similar to the original one, but definitely not the same.</p>
<p>A VAE tries to maximize the probability of each X in the training set under the entire generative process
according to \( P(X) = \int P(X|z; \theta)P(z)dz \)</p>
<p>\( P(X|z; \theta) \), allows making the dependence of X on z explicit by using the law of total probability. This<br />
framework, called &quot;maximum likelihood&quot;, allows to assume that if the model is likely to produce training set samples, 
then it is also likely to produce similar samples, and also unlikely to produce dissimilar ones.</p>
<p>According to {{#cite doersch_tutorial_2016}} VAEs are called &quot;autoencoders&quot; because the final training objective does
share the encoder/decoder architecture, so it resembles a traditional autoencoder.</p>
<h1><a class="header" href="#machinedeep-learning-topics" id="machinedeep-learning-topics">Machine/Deep Learning Topics</a></h1>
<h2><a class="header" href="#artificial-general-inteligence" id="artificial-general-inteligence">(Artificial) General Inteligence</a></h2>
<p>This has been, is, and will be for some more time at least, the dream of scientist/researchers/engineers in the field of 
artificial intelligence.</p>
<p>In short, achieving a general artificial intelligence assumes the ability of an agent to learn new tasks whilst maintaining 
a general capability on fulfilling previous learned tasks.
This artificial general intelligence requires that the agent does not forget what it has learn -what is called <a href="topics.html#catastrophic_forgetting">catastrophic forgetting</a>
in the literature- and assumes that the agent will continue learning new tasks -called <a href="topics.html#continual_learning">continual or lifelong learning</a>.</p>
<h2><a class="header" href="#feature-engineering" id="feature-engineering">Feature Engineering</a></h2>
<p>The classical/traditional way of &quot;massage&quot; the input to pass to a ML model (e.g. a classifier.) This was
an &quot;art&quot; in itself.
In the age of DL, this has been substituted by the DL models themselves, which represents
also the features on top of which the learning of a task is done. </p>
<h2><a class="header" href="#loss-function" id="loss-function">Loss Function</a></h2>
<p>Papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/2006.13593">Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks (KDD 2020)</a>	
Learns from prior training experiences in the form of DNN model states during to guide weight updates and improve DNN 
training performance. The retrospective loss seeks to ensure that the predictions at a particular training step are more 
similar to the ground truth than to the predictions from a previous training step (which has relatively
poorer performance). As training proceeds, minimizing this loss along with the task-specific loss, encourages the 
network parameters to move towards the optimal parameter state by pushing the training into tighter spaces around the 
optimum. Claims implementation is 3 lines of Pytorch code. Interesting paper</li>
</ul>
<h2><a class="header" href="#transfer-learning" id="transfer-learning">Transfer Learning</a></h2>
<p>Recent success of DL has been produced, among other reasons, for the big amount of labeled training data.
However, in general, this is not the approach to follow for AI solving different tasks.
Humans are good at learning from very few examples, so scientifically there's still &quot;something&quot; we still need to understand.</p>
<p>Approach of modern Transfer-Learning: </p>
<ol>
<li>Pre-train in large genral corpus, usually unsupervised (e.g. as in BERT)</li>
<li>Fine-Tunning on specific tasks with smaller (supervised) training sets.</li>
</ol>
<p>Approach of Hierarchical-Multilevel Classification:</p>
<ul>
<li><a href="https://www.aclweb.org/anthology/P19-1633/">Hierarchical Transfer Learning for Multi-label Text Classification (ACL 2019)</a></li>
<li><a href="https://arxiv.org/abs/2005.10996">Multi-Source Deep Domain Adaptation with Weak Supervision for Time-Series Sensor Data (KDD 2020)</a>
Interesting</li>
</ul>
<h3><a class="header" href="#surveys" id="surveys">Surveys</a></h3>
<ul>
<li><a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?ts=5c8d09e7#slide=id.g5888218f39_364_0">HuggingFace Presentation on Transfer Learning</a></li>
<li>2010 <a href="https://www.cse.ust.hk/%7Eqyang/Docs/2009/tkde_transfer_learning.pdf">A Survey on Transfer Learning</a></li>
<li>2018 <a href="https://arxiv.org/abs/1808.01974">A Survey on Deep Transfer Learning</a></li>
<li>2020 <a href="https://arxiv.org/abs/2007.04239">A Survey on Transfer Learning in Natural Language Processing</a></li>
</ul>
<h3><a class="header" href="#few-shot-learning-meta-learning" id="few-shot-learning-meta-learning">Few-shot Learning (Meta-learning)</a></h3>
<p>Meta-learning aims to train a general model in several learning tasks. The goal is that the resulting model has to be able to solve unseen tasks by using just 
a few training examples.</p>
<p>Concept of [shortcut learning]: You don't learn a task by completely understanding it but by taking &quot;shortcuts&quot; imitating.
e.g. when training on some math exercises in highschool because every year they had the same structure. </p>
<ul>
<li><a href="https://arxiv.org/abs/2004.07780">Shortcut Learning in Deep Neural Networks</a></li>
</ul>
<p>Meta-Learning -learning (how) to learn-: Find an algorithm <img src="https://render.githubusercontent.com/render/math?math=A"> that from a small input data (few-shot examples) <img src="https://render.githubusercontent.com/render/math?math=DS_{train}(x_i,y_i)"> can predict the output <img src="https://render.githubusercontent.com/render/math?math=y'"> of a new input 
<img src="https://render.githubusercontent.com/render/math?math=x'"></p>
<ul>
<li>
<p>Prototypical Networks <a href="">Prototypical Networks for Few-shot Learning</a>
Nearest centroid classification</p>
</li>
<li>
<p>MAML <a href="">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> 
Model-agnostic meta-learning algorithm compatible with any model trained with GD and applicable to a variety of different learning problems, including 
classification, regression, and reinforcement learning.</p>
</li>
<li>
<p><a href="">Selecting Relevant Features from a Multi-Domain Representation for Few-shot Learning</a></p>
</li>
</ul>
<p>Work from Hugo Larochelle at Google Brain:</p>
<ul>
<li><a href="">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</a></li>
<li><a href="">A Universal Representation Transformer Layer for Few-shot Image Classification</a></li>
</ul>
<h2><a class="header" href="#calibration" id="calibration">Calibration</a></h2>
<p>A measure of the confidence of the predictions of a model. Concept comes from weather forecast.</p>
<p>Main Methods:</p>
<ul>
<li>Platt Scaling</li>
<li>Matrix Vector Scaling</li>
<li>Temperature scaling</li>
</ul>
<p>Papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks (ICML 2017)</a></li>
<li><a href="https://papers.nips.cc/paper/9397-beyond-temperature-scaling-obtaining-well-calibrated-multi-class-probabilities-with-dirichlet-calibration.pdf">Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration (NeurIPS 2019)</a></li>
</ul>
<details>
  <summary>Blogs</summary>
 * [How and When to Use a Calibrated Classification Model with scikit-learn](https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/)
 * [Prediction & Calibration Techniques to Optimize Performance of Machine Learning Models](https://towardsdatascience.com/calibration-techniques-of-machine-learning-models-d4f1a9c7a9cf)
 * [Calibration in Machine Learning](https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555#:~:text=In%20this%20blog%20we%20will%20learn%20what%20is%20calibration%20and,when%20we%20should%20use%20it.&text=We%20calibrate%20our%20model%20when,output%20given%20by%20a%20system.)
 * [Calibration Tutorial (KDD 2020)](http://kdd2020.nplan.io/presentation) [Github](https://github.com/nplan-io/kdd2020-calibration)
</details>
<details>
  <summary>Videos</summary>
 * [Calibration Tutorial](https://www.youtube.com/watch?v=rhnqZV6eKlg&feature=youtu.be)
</details>
<h2><a class="header" href="#model-interpretability" id="model-interpretability">Model Interpretability</a></h2>
<p>Intellegibility and explanation are critical in many domains (health, crime prediction...) <a href="https://medium.com/analytics-vidhya/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae">Blog entry</a></p>
<p>Models as black box methods: Shap, LIME
Glass box modesl: Explainable Boosting Machine (EBMs) are the SotA</p>
<p>Papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/2004.13912">Neural Additive Models: Interpretable Machine Learning with Neural Nets</a></li>
<li><a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">A Unified Approach to Interpreting Model Predictions (NIPS 2017)</a></li>
</ul>
<p>Books:</p>
<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpreable Machine Learning Book A Guide for Making Black Box Models Explainable (Christoph Molnar, 2020)</a></li>
</ul>
<p>Tools:</p>
<ul>
<li><a href="https://github.com/marcotcr/lime">Lime</a></li>
<li><a href="https://github.com/slundberg/shap">Shap</a></li>
<li><a href="https://github.com/interpretml/interpret">Interpret ML</a> Reference impl of EBMs</li>
<li><a href="https://nbviewer.jupyter.org/github/slundberg/shap/blob/master/notebooks/general/Explainable%20AI%20with%20Shapley%20Values.ipynb">Shap Tutorial (Includes BERT examples)</a></li>
<li><a href="https://captum.ai/">Captum</a> For Pytorch</li>
</ul>
<h2><a class="header" href="#causality" id="causality">Causality</a></h2>
<ul>
<li><a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why (Judea Perl)</a></li>
<li><a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a>
Learning paradigm to estimate invariant correlations across multiple training distributions. IRM learns a data representation such that the optimal classifier, 
on top of that data representation, matches for all training distributions.</li>
</ul>
<h2><a class="header" href="#a-namecontinual_learningacontinual-learning-and-catastrophic-forgetting" id="a-namecontinual_learningacontinual-learning-and-catastrophic-forgetting"><a name="continual_learning"></a>Continual Learning and Catastrophic Forgetting</a></h2>
<p>In biology, <em>Continual Learning</em> refers to the process of continually gather, update, and transfer skills/knowledge 
throughout life (lifespan).</p>
<p>In ML, it is still a major research problem to solve the fact that neural networks use to catastrophically forget 
previously learned tasks when they are trained in new ones. This fact it is the main obstacle that prevents the 
equivalent of continual learning to be implemented in the field of artificial neural networks.</p>
<p><a href="https://www.aclweb.org/anthology/2020.coling-main.574.pdf">This is a summary of the recent (2021) advances in continual learning in NLP.</a></p>
<h3><a class="header" href="#a-namecatastrophic_forgettingaprotocolsstrategies-for-solving-catastrophic-forgetting-cf" id="a-namecatastrophic_forgettingaprotocolsstrategies-for-solving-catastrophic-forgetting-cf"><a name="catastrophic_forgetting"></a>Protocols/Strategies for Solving Catastrophic Forgetting (CF)</a></h3>
<p>One problem with all the different strategies proposed for solving CF is that the field lacks a framework for comparing
the effectiveness of the techniques. This has been addressed by studies like <a href="refs.html#vandeven2019">vandeven2019</a> and <a href="refs.html#vandeven2019b">vandeven2019b</a>.</p>
<p>The approaches to solve Catastrophic Forgetting can be classified in:</p>
<h4><a class="header" href="#regularization-approaches" id="regularization-approaches">Regularization Approaches</a></h4>
<p>https://arxiv.org/pdf/1612.00796.pdf</p>
<h4><a class="header" href="#generative-replay" id="generative-replay">Generative Replay</a></h4>
<ul>
<li><a href="refs.html#parisi2020">Continual Lifelong Learning with Neural Networks:A Review</a></li>
<li><a href="https://www.nature.com/articles/s41467-020-17866-2.epdf?sharing_token=bkJqxr4qptypBkYehsw_FtRgN0jAjWel9jnR3ZoTv0NoUJpE84DVnSx_jyG1N8KQimOuCCtJtaDabIpjOWE47UccZTsgeeOekV8ng2BR-omuTPXahD4aCOiCIIfIO2IOB-qJOABLKf7BlAYsTBE8rCeZYZcKd0yuWJjlzAEc1G8%3D">Brain-inspired replay for continual learning with artiÔ¨Åcial neural networks (Nature, 2020)</a></li>
<li><a href="https://arxiv.org/pdf/1809.10635v2.pdf">Generative replay with feedback connections as a general strategy for continual learning (ICLR2019)</a></li>
<li><a href="https://arxiv.org/pdf/1904.07734.pdf">Three Scenarios for Continual Learning</a></li>
<li><a href="https://openreview.net/pdf?id=rklnDgHtDS">Compositional language continual learning</a></li>
</ul>
<h1><a class="header" href="#nlp" id="nlp">NLP</a></h1>
<h2><a class="header" href="#frameworks" id="frameworks">Frameworks</a></h2>
<ul>
<li><a href="https://huggingface.co/">Huggingface</a></li>
<li><a href="https://github.com/NervanaSystems/nlp-architect">NLP Architect</a></li>
</ul>
<h1><a class="header" href="#blogs--repos" id="blogs--repos">Blogs &amp; Repos</a></h1>
<ul>
<li><a href="https://github.com/neubig/lowresource-nlp-bootcamp-2020">NLP Bootcamp</a> CMU lectures on NLP by visitors to the
Language Technologies Institute.</li>
</ul>
<h1><a class="header" href="#nlp-papers" id="nlp-papers">NLP Papers</a></h1>
<h2><a class="header" href="#recent-origins" id="recent-origins">Recent Origins</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1301.3781">Word2Vec</a></li>
<li><a href="https://arxiv.org/abs/1402.3722">word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method</a></li>
</ul>
<h2><a class="header" href="#attention" id="attention">Attention</a></h2>
<p>The concept of attention is taken, as many others, from cognitive sciences (e.g. psycology, neuroscience, education.) It 
describes the process of focusing on certain concrete stimulus/stimuli while ignoring the rest of stimuli in an 
environment. In the case of NLP for example, the context/environment can be a sentence and the stimulus a word.</p>
<ul>
<li><a href="">Attention and Memory-Augmented Networks for Dual-View Sequential Learning (KDD 2020)</a></li>
</ul>
<h2><a class="header" href="#transformers" id="transformers">Transformers</a></h2>
<h3><a class="header" href="#sesame-street-environment" id="sesame-street-environment">Sesame Street Environment</a></h3>
<ul>
<li><a href="https://arxiv.org/abs/1802.05365">ELMO</a> Improvement over Word2Vec. Tries to add context to word representations.
Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), pre-trained
on a large text corpus.</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT</a> The reference model for NLP since 2018.</li>
<li><a href="https://arxiv.org/pdf/1907.10529.pdf">SpanBERT</a>
Masks spans of words instead of random subwords. Spans of words refers to global entities or loca/domain-specific meaning (e.g. American Football)
Span Boundary Objective(SBO) predicts the span context from boundary token representations. Uses single sentence document-level inputs instead of
the two sentences in BERT.
Code: https://github.com/facebookresearch/SpanBERT</li>
<li><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>
Replication study of BERT pretraining that measures the impact of many key hyperparameters (Bigger Batch size and LR) and training data size (10X).
It shows improvements on most of the SotA results by BERT and followers. Questions the results of some post-BERT models.
It uses a single sentence for the document-level input like SpanBERT.
Code: https://github.com/pytorch/fairseq</li>
</ul>
<h4><a class="header" href="#sparse-transformers" id="sparse-transformers">Sparse Transformers</a></h4>
<ul>
<li><a href="refs.html#sparse">Sparse Tansformer</a> Self-attention complexity from O(n2) to O(n*sqrt(n)).</li>
<li><a href="refs.html#reformer">Reformer</a> Self-attention complexity O(L2) to O(LlogL), where L is the length of the sequence.</li>
<li><a href="refs.html#linformer">Linformer</a> Self-attention complexity from O(n2) to O(n) in both time and space.</li>
</ul>
<h4><a class="header" href="#small-modelssmall-devices" id="small-modelssmall-devices">Small Models/Small Devices</a></h4>
<ul>
<li><a href="refs.html#lite">Lite transformer with Long-Short Range Attention</a>
Uses Long-Short Range Attention (LSRA) in which a group of heads specializes in
the local context (using convolution) and another group specializes in the
long-distance relationships (ussing the attention mechanism.) Focus on edge (mobile) devices.</li>
</ul>
<details>
  <summary>Small Models</summary>
 * [Distilbert](https://arxiv.org/abs/1910.01108)
 Check it out in https://huggingface.co/
</details>
<details>
  <summary>Other Sesame Street Papers</summary>
<ul>
<li><a href="https://arxiv.org/abs/1908.10063">FinBERT</a> Bert applied to Financial Sentiment Analysis.
Code: https://github.com/ProsusAI/finBERT]</li>
<li><a href="refs.html#abcd">FinBERT</a></li>
</ul>
</details>
<h3><a class="header" href="#non-sesame-street-environment" id="non-sesame-street-environment">Non-Sesame Street Environment</a></h3>
<p>** <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing NLG</a>
&quot;Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP
tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and
research purposes. &lt;|endoftext|&gt;&quot; - Summary generated by itself.</p>
<h3><a class="header" href="#lifelong-learning-in-nlp" id="lifelong-learning-in-nlp">Lifelong learning in NLP</a></h3>
<ul>
<li><a name="biesialska"><a href="https://www.aclweb.org/anthology/2020.coling-main.574.pdf">Continual Lifelong Learning in Natural Language Processing: A Survey</a> Colling, 2020</a></li>
</ul>
<p>Pytorch conference:</p>
<h2><a class="header" href="#intro-conf" id="intro-conf">INTRO CONF</a></h2>
<p>jax and julia languages</p>
<p>2020 state of ai report</p>
<p>Tensor Statements (Experimental)</p>
<p>Facebook MMF Multimodal library</p>
<p>MLPerf a machine learning performane benchnark suite with broad industry and academia</p>
<p>Microsoft DeepSpeed Dist training
Google Brain: ML for system
DiffTaichi: MIT</p>
<p>Usability: Try to balance again to performance.</p>
<h2><a class="header" href="#state-of-pytorch" id="state-of-pytorch">State of pytorch</a></h2>
<p>-More operators/quantized operators driven by numpy compatibility
-More language support
-More platforms</p>
<p>In 2020:
-Composability
-Distributed RPC
-XLA</p>
<p>To come:
-Multitensor optimizers
-Lazy modules
-torch.fx python to python transformations instead with C++
-Vulkan and Metal hw accel to mobile
-Benchmarking tools
-Metatensors</p>
<p>Contrib:
-rfcs: Proposals bigger than an issue
-dev newsletter
-nightly checkout tool</p>
<h2><a class="header" href="#complex-numbers-in-pytorch" id="complex-numbers-in-pytorch">Complex numbers in pytorch</a></h2>
<ul>
<li>Quantum mechanics and singal processing
-Research: Deep Complex networks, On complex valued convolutional networks, etc...</li>
<li>Representation:
-- API easy to use based on a natural representation
-- Operations
-- Autograd</li>
</ul>
<p>-new datatypes: torch.complex64/128
-core properties: angle, abs, real, imag, polar, complex based on numpy
-complex differencitation</p>
<p>Future:
-JIT and distribute computing</p>
<h2><a class="header" href="#pytorch-marries-numpy" id="pytorch-marries-numpy">Pytorch marries Numpy</a></h2>
<p>-Compatible here means the same function can be called the same way to produce the same (or very similar) behaviour
-New functions added every day and plan to add linear algebra</p>
<h2><a class="header" href="#high-level-api-for-autograd" id="high-level-api-for-autograd">High Level API for Autograd</a></h2>
<p>torch.autograd.functional
-Takes a python function as an input
-Closer to functional API and mathematical definition</p>
<p>functional.jacobian(func, inputs,...)
functional.vjp(func, input,v,..) -- backprop
functional.jvp(...) -- forward mode
functinal.hessian(func, input,...) second order derivativeses
functinal.vhp(...) backward
functinal.hvp(...) forward</p>
<h2><a class="header" href="#pytorch-rpc" id="pytorch-rpc">Pytorch RPC</a></h2>
<p>DistributedDataParallel was the standard. The input was distributed but not works for large models as the model is centraliced</p>
<p>RPC in Pytorch: Flexible Low Level Tools
Features:</p>
<ul>
<li>Remote execution: Run user function or modules remotely
rpc_init,sync, async, return a remote reference (shared pointer), shutdown</li>
<li>Remote reference
points to objects in a distributed env and can be passed as rpc arguments to avoid communicating real data</li>
<li>Distributed autograd
Automatically stitch togethr local autograd to perform distributed differentiation across rpc boundaries</li>
</ul>
<p>Use cases:
-Parameter server multiple trainers and serevers
-Distributed model parallel
-Pipeline parallelism</p>
<h2><a class="header" href="#pytorch-ddp" id="pytorch-ddp">Pytorch DDP</a></h2>
<p>DDP and C10D merges with RPC</p>
<p>Future adds:
Add zero (DeepSpeed) style training framework for large models
intra layer perallelism (Used by megatron model)
Torchscript support for c10d apis
Auto tuning for DDP
Hybrid Parallelism
Auto Parallelism</p>
<h2><a class="header" href="#torchtext" id="torchtext">TorchText:</a></h2>
<p>-Raw text
Download, unzip, read
-Transforms
tokenize,vocabulary,sentencepiece
-Batch and sampler
DataLoader,Sampler
-Model
-Transformer,MultiheadAttention</p>
<p>Datasets
AG_NEWS, Sogounews</p>
<p>Tokenizer-&gt;vocab lookup-&gt;to tensor
Tokenizer-&gt;Vector Lookup</p>
<p>Multihead-attention container:
-Drop-in replacement
-torchscript support
-incremental decoding
-broadcast support
-research variants</p>
<h2><a class="header" href="#pyroch-and-tensorboard" id="pyroch-and-tensorboard">Pyroch and Tensorboard</a></h2>
<p>pr_curve!!!</p>
<p>hyperpararmeter dashboard</p>
<p>Future:
Perf plugin
plotly
alert</p>
<h2><a class="header" href="#performance" id="performance">Performance</a></h2>
<p>Better AMP
tf32</p>
<p>Metal - Apple GPU inference
Vulkan - Android GPU inferene</p>
<h2><a class="header" href="#android-and-pytorch" id="android-and-pytorch">Android and Pytorch</a></h2>
<p>Take advantage of GPUs/NPUs Neural Process Accelerators in mobile devices
Android Neural Networks API abstracts the different underlying vendor driver libs and chips
Pytorch supports NNAPI</p>
<h2><a class="header" href="#pytorch-for-other-languages" id="pytorch-for-other-languages">Pytorch for Other Languages</a></h2>
<ul>
<li>Libtorch (C++) Allows other programing languages binding to C++ have their own Torch-like library</li>
<li>Serialized representation of models like torchscript and onnx. This allows to exchange trained models between 
programming languages</li>
</ul>
<p>Torch for R or Hasktorch are examples of a library implemented on Libtorch</p>
<p>The Gradient Review: Frameworks don't just enable machine learning resetarch, they enable and restrict what researchers can do</p>
<h2><a class="header" href="#pytorch-for-graph-neural-networks" id="pytorch-for-graph-neural-networks">Pytorch for Graph Neural networks</a></h2>
<p>-Pythonic API
-For High Energy Physics
-Maybe for Quantum computing in the future?</p>
<h2><a class="header" href="#hyperparameter-tuning" id="hyperparameter-tuning">Hyperparameter tuning</a></h2>
<p>hyperparameterss:
layers, units per layer, lr, etc.. determine the performance. can determine the success or failure of the models.
preprossingaugmentation, image format</p>
<p>network trainerbatchs size image, optimizer chosen
hwlayer : fp3 smixed precission</p>
<p>Hyper-parameter optimization paper
Suervay of ML expeprimettal methods at neurips2019 and iclr2020
- 6% where using only HP optimizers</p>
<p>Avoid Curse of Dimensionality
An efficient approach for assesing hyperparameter importance</p>
<p>Optuma came as a result.</p>
<p>Best hyperparameters to tune:
LR is the most hyer parameter
Units in the firsr layer
the optimizer
dropout in the first layer</p>
<p>Hyperparameter evolution:</p>
<p>not tuning
manually play 
grid search
optuna
optupta weveraged with hyperparameter tuning</p>
<h2><a class="header" href="#deepspeed-interesting" id="deepspeed-interesting">DeepSpeed (Interesting)</a></h2>
<p>Training optimization library
Faster Transformer training
zero-offload: Democrazitzes big models
1-bit adam: 5x faster training</p>
<p>ONNX Runtime (ORT)
optimized for pytorch 
integrating collections of optimization techiniques from Deepspeed</p>
<h2><a class="header" href="#ml-perf" id="ml-perf">ML Perf</a></h2>
<p>GPU support in Pytorch. </p>
<p>NVIDIA trains BERT in 48.6s using 2048 A100 GPUs</p>
<p>Challenges:
Slow non-gpu work: Cuda graphs
model parallel utilities:<br />
automated optimizations: Automated operator fusion (collaborate witht the JIT team at FB)</p>
<p>Dedicated pytorch team in NVIDIA</p>
<h2><a class="header" href="#going-forward" id="going-forward">Going Forward</a></h2>
<p>-Run in mobile devices, embedded hw like Rapsberry Pi, new devices</p>
<ul>
<li>ONNX Runtime and TVM</li>
</ul>
<p>ONNX
-Need to expoert
-good perf out o f  the box
-files as means of exchange
should pytorch take ONNX?</p>
<p>TVM
-can take python JIT graph
-conversion implemented in Pythn
-typing one of the hard bits
can tune for new hw/optimization paterns</p>
<p>3ed option: hook into pytorch as torch vision ant trttorch</p>
<p>jit extension
allows to bind 3rd party libraries for benchmark or quick speedups on specifi ops
also fuse operation tugehter. trt requritest the jit grap in custom pass
you stay in pytorch
you cherry picks which things run externally</p>
<h2><a class="header" href="#reproducibility-ai-using-pytorch" id="reproducibility-ai-using-pytorch">Reproducibility AI using Pytorch</a></h2>
<p>MLFlow: Trochserve</p>
<h2><a class="header" href="#pytorchxla" id="pytorchxla">Pytorch/XLA</a></h2>
<p>TPUs</p>
<p>XLA compoiler based linear algrebra exec engine. targets cpu gpu and tpu</p>
<p>pytrch/xla is a layer eteen xla and pytorch</p>
<p>provides apis for out of the tree backend extensions.</p>
<p>Lazy tensor system . Build on top of the pytorch eager runtime.
stores the pytorch ops as intermidate representation
then pytroch xla to xla converting theIR node tho the HLOptimizer</p>
<h2><a class="header" href="#torchserve" id="torchserve">TorchServe</a></h2>
<p>Pytorch
Need for custom code to deploy and predict
Need for custom infrastructure to scale and monitor</p>
<p>Use version for A/B testing
Ensemble support (commint soon) and batch inferencing
REST API, grpc API (coming soon)</p>
<p>Future:</p>
<ul>
<li>Improve memory and resource usage for scalability</li>
<li>compliancy with serving APIs sun as KFServing (look at it)</li>
<li>integrating with Captum</li>
<li>Autoscalling for K8s</li>
</ul>
<h2><a class="header" href="#differential-privacy" id="differential-privacy">Differential Privacy</a></h2>
<p>Destroy a controlled amount of information</p>
<p>GDPR and used in the US census</p>
<p>Aggregates can be:
-count things
-a mean
-train a ML model</p>
<p>memorization violates privacy: has to do with losing memory</p>
<p>Differential privacy can be added to : data, training, model.zip, deployment
the gold standard is to add it to training: Opacus (limits memorization)</p>
<p>bit.ly/opacus-dev-day</p>
<h2><a class="header" href="#future-of-ai-tools" id="future-of-ai-tools">Future of AI Tools</a></h2>
<h2><a class="header" href="#what-was-build-in-2020" id="what-was-build-in-2020">What was build in 2020:</a></h2>
<p>Optuna</p>
<h1><a class="header" href="#productionizing-mldl" id="productionizing-mldl">Productionizing ML/DL</a></h1>
<p><a href="http://pages.cs.wisc.edu/%7Ewentaowu/papers/kdd20-ci-for-ml.pdf">Multimodal Learning with Incomplete Modalities by Knowledge Distillation (KDD 2020)</a>
Interesting</p>
<h2><a class="header" href="#model-size-vs-efficiency" id="model-size-vs-efficiency">Model Size vs Efficiency</a></h2>
<p>Big models -&gt; Big problem for company at deploy time. Not to speak about deploying an ensemble of models, even if this
shows better performance overall. Several techniques, such as knowledge distillation, pruning and quantization, have 
been identified to reduce the number of parameters of a model without impacting significantly the quality of the model. 
In the end, most of the techniques described below, result in slightly degraded prediction metrics.</p>
<ul>
<li><a href="https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/">How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs (Roblox)</a></li>
</ul>
<h3><a class="header" href="#distilation" id="distilation">Distilation</a></h3>
<p>Hinton, Vinyals and Dean showed in {{#cite hinton_distilling_2015}} how to apply Caruana's model compression techniques described in {{#cite bucilua_model_2006}}.
Caruana et all showed how to take advantage of the property of ANN of being <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators</a>,
to train an ANN to mimic the function learned by an ensemble of models. The idea behind the universal approximator theorem
is that, with enough neurons and training data, a NN can approximate any function with enough precision. To do that,
basically they take a brand new (and usually big) <em>unlabeled</em> dataset and they label it using the ensemble. Then they
train an ANN using this brand new large (and recently labeled dataset,) so the resulting model mimics the ensemble, and
which, as they demonstrate, performs much better than the same ANN trained on the original dataset. </p>
<p>Hinton et all, in the aforementioned paper, prove Caruana's ensemble model distillation on MNIST and in a commercial
acoustic model. They also add a new composite ensemble with several specialist models (which can also be trained in 
parallel) that learn to distinguish classes that the full models confuse.</p>
<h3><a class="header" href="#pruning" id="pruning">Pruning</a></h3>
<h3><a class="header" href="#quantization" id="quantization">Quantization</a></h3>
<ul>
<li>Focused on inference</li>
<li>Focused on small devices/IoT</li>
</ul>
<p>Papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/1910.06188">Q8BERT: Quantized 8Bit BERT (2019</a></li>
<li><a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">DYNAMIC QUANTIZATION ON BERT (BETA)</a></li>
</ul>
<p>It turns out that quantization is now now possible in ONNX models:</p>
<pre><code class="language-python">import onnx
from quantize import quantize, QuantizationMode
...
# Load onnx model
onnx_model = onnx.load('XXX_path')
# Quantize following a specific mode from https://github.com/microsoft/onnxruntime/tree/e26e11b9f7f7b1d153d9ce2ac160cffb241e4ded/onnxruntime/python/tools/quantization#examples-of-various-quantization-modes
q_onnx_model = quantize(onnx_model, quantization_mode=XXXXX)
# Save the quantized model
onnx.save(q_onnx_model, 'XXXX_path')
</code></pre>
<p><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-qat-networks">Tensor RT supports the quantized models so, it should work</a></p>
<ul>
<li><a href="https://github.com/microsoft/DeepSpeed">Deep Speed (Microsoft)</a> ZeRO redundancy memory optimizer: Addresses the problems with high memory consumption of 
large models with pure data parallelism and the problem of using model parallelism.</li>
<li><a href="https://www.youtube.com/watch?v=n4bESjZ-VaY&amp;feature=youtu.be">Training BERT with Deep Speed</a></li>
<li><a href="https://pytorch.org/elastic">Torch Elastic</a></li>
<li><a href="https://pytorch.org/docs/stable/rpc.html">PyTorch RPC</a></li>
<li><a href="https://pytorch.org/serve">PyTorch Serve</a></li>
</ul>
<h1><a class="header" href="#frameworks-1" id="frameworks-1">Frameworks</a></h1>
<ul>
<li><a href="">JAX</a></li>
<li><a href="">Caffe2</a></li>
</ul>
<h2><a class="header" href="#deep-learning" id="deep-learning">Deep Learning</a></h2>
<ul>
<li><a href="https://pytorch.org/">Pytorch</a>
Recent trend in Research Community. Improvig a lot to put into production</li>
<li><a href="https://www.tensorflow.org/">Tensorflow/Keras</a>
The big hit in 2015.</li>
<li><a href="https://mxnet.apache.org/versions/1.6/">MxNet</a> Apache</li>
</ul>
<h2><a class="header" href="#nlp-1" id="nlp-1">NLP</a></h2>
<ul>
<li><a href="https://huggingface.co/">Huggingface</a></li>
<li><a href="https://github.com/NervanaSystems/nlp-architect">NLP Architect</a></li>
<li><a href="https://github.com/dmlc/gluon-nlp">GluonNlp</a></li>
</ul>
<h2><a class="header" href="#distributed-environments" id="distributed-environments">Distributed Environments</a></h2>
<ul>
<li><a href="https://github.com/horovod/horovod">Horovod</a></li>
<li><a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a></li>
</ul>
<h2><a class="header" href="#datasets" id="datasets">Datasets</a></h2>
<ul>
<li><a href="https://huggingface.co/datasets">Huggingface Datasets</a></li>
</ul>
<h1><a class="header" href="#books" id="books">Books</a></h1>
<ul>
<li><a href="http://d2l.ai/">Dive into Deep Learning</a> Interactive book</li>
<li><a href="https://www.deeplearningbook.org/">Deep Learning</a> <em>Ian Goodfellow and Yoshua Bengio and Aaron Courville</em> </li>
</ul>
<h1><a class="header" href="#other-resources" id="other-resources">Other Resources</a></h1>
<ul>
<li><a href="https://thegradient.pub/">The Gradient</a> Magazine about research, recent developments and current/long-term trends in 
AI/ML. Origin: 2017 by students and researchers @ Stanford Artificial Intelligence Laboratory (SAIL).
Status: non-profit and volunteers in the AI community.</li>
</ul>
<h1><a class="header" href="#main-aimldldata-science-conferences" id="main-aimldldata-science-conferences">Main AI/ML/DL/Data Science Conferences</a></h1>
<ul>
<li><a href="https://nips.cc/">NIPS/Neurips</a></li>
<li><a href="https://www.kdd.org/">KDD (SIGKDD) ACM</a></li>
<li><a href="https://iclr.cc/">ICLR (International Conference on Learning Representations)</a> Dedicated to advances of the
representation learning a.k.a. deep learning.</li>
<li><a href="https://cikm2020.org/">International Conference on Information and Knowledge Management</a></li>
</ul>
<h2><a class="header" href="#nlplinguistics" id="nlplinguistics">NLP/Linguistics</a></h2>
<ul>
<li><a href="https://www.aclweb.org/">ACL (Association of Computational Linguistics)</a></li>
<li><a href="https://2020.emnlp.org/">Conference on Empirical Methods in Natural Language Processing</a></li>
</ul>
<h1><a class="header" href="#applications" id="applications">Applications</a></h1>
<ul>
<li><a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf">DeepFace</a> always remind me of how Schwarzenegger is framed for the murder of almost a hundred unarmed civilians in <a href="https://www.imdb.com/title/tt0093894/">The Running Man</a></li>
</ul>
<p>This is an example of citation {{#cite ven_brain-inspired_2020}} in an mdBook </p>
<h2><a class="header" href="#prediction" id="prediction">Prediction</a></h2>
<h2><a class="header" href="#regression" id="regression">Regression</a></h2>
<h2><a class="header" href="#classification" id="classification">Classification</a></h2>
<h2><a class="header" href="#recommender-systems" id="recommender-systems">Recommender systems</a></h2>
<h2><a class="header" href="#recognition-images-audio-text" id="recognition-images-audio-text">Recognition (images, audio, text)</a></h2>
<h2><a class="header" href="#computer-vision" id="computer-vision">Computer vision</a></h2>
<h2><a class="header" href="#clustering-and-anomaly-detection" id="clustering-and-anomaly-detection">Clustering and anomaly detection</a></h2>
<h2><a class="header" href="#natural-language-processing-generation-and-understanding" id="natural-language-processing-generation-and-understanding">Natural language processing, generation, and understanding</a></h2>
<h2><a class="header" href="#translation" id="translation">Translation</a></h2>
<p>Along 2016/2017 Google switched from sentence-based/linguistic expert-based algorithmic approach to deep-learning based 
methods (what is called Neural Machine Translation, or NMT.) The leap in quality of the transations was massive:</p>
<ul>
<li><a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/">Found in translation: More accurate, fluent sentences in Google Translate (Barak Turovsky, Google Translate Product Lead, Nov 2016</a></li>
<li><a href="https://www.blog.google/products/translate/higher-quality-neural-translations-bunch-more-languages/">Higher quality neural translations for a bunch more languages (Barak Turovsky, Google Translate Product Lead, Mar 2017)</a></li>
<li><a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html">A Neural Network for Machine Translation, at Production Scale</a></li>
<li><a href="https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/">The Shallowness of Google Translate (Douglas Hofstadter)</a></li>
</ul>
<h2><a class="header" href="#medicine" id="medicine">Medicine</a></h2>
<p>Dermatology - Detect skin cancer or problematic skin ir</p>
<h2><a class="header" href="#legallaw" id="legallaw">Legal/Law</a></h2>
<h3><a class="header" href="#programing-languages" id="programing-languages">Programing Languages</a></h3>
<ul>
<li><a href="https://www.oreilly.com/radar/automated-coding-and-the-future-of-programming/?sfmc_id=85378584&amp;utm_medium=email&amp;utm_source=platform+b2b&amp;utm_campaign=engagement&amp;utm_content=whats+new+thinking+20200831">Automated Coding article (O'Reilly)</a></li>
<li><a href="https://arxiv.org/pdf/2006.03511.pdf">Unsupervised Translation of Programming Languages</a></li>
</ul>
<ul>
<li>
<p><a name="finbert"><a href="https://arxiv.org/abs/1908.10063">FinBERT</a> Bert applied to Financial Sentiment Analysis.</a>
Code: https://github.com/ProsusAI/finBERT]</p>
</li>
<li>
<p><a name="reformer"><a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a>
2019</a></p>
</li>
<li>
<p><a name="reformer"><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a>
2020</a></p>
</li>
<li>
<p><a name="linformer"><a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear Complexity</a>
2020</a></p>
</li>
<li>
<p><a name="lite"><a href="https://arxiv.org/pdf/2004.11886.pdf">Lite transformer with Long-Short Range Attention</a>
2020</a></p>
</li>
<li>
<p><a name="mccloskey1989">Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. The psychology of learning and motivation, 24(109-165):92, 1989.</a></p>
</li>
<li>
<p><a name="biesialska"><a href="https://www.aclweb.org/anthology/2020.coling-main.574.pdf">Continual Lifelong Learning in Natural Language Processing: A Survey</a> Colling, 2020</a></p>
</li>
<li>
<p><a name="parisi2020"><a href="https://arxiv.org/abs/1802.07569">Continual Lifelong Learning with Neural Networks: A Review</a> Tech Report, 2019</a></p>
</li>
<li>
<p><a name="vandeven2019"><a href="https://arxiv.org/pdf/1809.10635v2.pdf">Generative replay with feedback connections as a general strategy for continual learning</a> ICLR 2019</a></p>
</li>
<li>
<p><a name="vandeven2019b"><a href="https://arxiv.org/pdf/1904.07734.pdf">Three Scenarios for Continual Learning</a> Neurips 2019</a></p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>

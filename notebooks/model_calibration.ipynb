{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.calibration import calibration_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "console=True\n",
    "real_gt_filepath='../ground_truth.txt'\n",
    "gt_filepath='../test_gt_yes_no.txt'\n",
    "predictions_filepath='../../test_results.tsv'  # This should point to the test_results.txt file\n",
    "examples_to_spit_out=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Ground Truth and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ground_truth(file, label_list, label_separator, columns, label_column_idx=-1, delimiter=\"\\t\", to_console=False, examples_to_spit_out=10):\n",
    "    def convert_to_digit(elem):\n",
    "        assert isinstance(elem, str)\n",
    "        digit = -1\n",
    "        if elem == \"yes\" or elem == \"no\":\n",
    "            digit = 1 if elem == \"yes\" else 0\n",
    "        else:\n",
    "            digit = int(elem)\n",
    "        return digit\n",
    "\n",
    "\n",
    "    grouth_truth_labels = []\n",
    "    examples_per_label = collections.Counter()\n",
    "    examples_read = 0\n",
    "    expected_columns=len(columns)\n",
    "    print(f\"Getting ground truth from file: {file}\")\n",
    "    with open(file, \"r\") as reader:\n",
    "        while True:\n",
    "            line_attributes = reader.readline().rstrip().split(delimiter)\n",
    "            if len(line_attributes) != expected_columns:\n",
    "                break\n",
    "\n",
    "            example_labels = line_attributes[label_column_idx]\n",
    "            example_label_list = example_labels.split(label_separator)\n",
    "            example_label_list = map(lambda x: convert_to_digit(x), example_label_list)\n",
    "            example_active_labels = list(set(label_list) & set(example_label_list))\n",
    "            if to_console and examples_read < examples_to_spit_out:\n",
    "                line_to_print=\"\"\n",
    "                for i, col_name in enumerate(columns):\n",
    "                    if i == len(columns) - 1 or i==label_column_idx:\n",
    "                        line_to_print+=f\" {col_name} -> {line_attributes[i][:100]}\"\n",
    "                    else:\n",
    "                        line_to_print+=f\" {i}) {col_name} = {line_attributes[i][:100]}...\"\n",
    "                print(line_to_print)\n",
    "            examples_read += 1\n",
    "            for label in example_active_labels:\n",
    "                examples_per_label[label] += 1\n",
    "            grouth_truth_labels.append(example_active_labels if len(example_active_labels) > 1 else example_active_labels[0])\n",
    "    print(f\"Examples per label transformed ground truth: {examples_per_label}. Sum: {sum(examples_per_label.values())}\")\n",
    "    return grouth_truth_labels, examples_per_label\n",
    "\n",
    "def read_binary_preds(file, gt, delimiter=\"\\t\", pos_label_idx=0, threshold=0.5, to_console=False):\n",
    "    y_preds_scores_pos_label = []\n",
    "    y_preds_probas = np.empty((0, 2))\n",
    "\n",
    "    with open(file, \"r\") as reader:\n",
    "        while True:\n",
    "            line_attributes = reader.readline().rstrip().split(delimiter)\n",
    "            if len(line_attributes) != 2:\n",
    "                break\n",
    "            np.testing.assert_almost_equal(float(line_attributes[0]) + float(line_attributes[1]), 1.0)\n",
    "            if float(line_attributes[pos_label_idx]) >= threshold:\n",
    "                y_preds_scores_pos_label.append(1)\n",
    "            else:\n",
    "                y_preds_scores_pos_label.append(0)\n",
    "\n",
    "            y_probas = [float(line_attributes[0]), float(line_attributes[1])]\n",
    "            y_preds_probas = np.append(y_preds_probas, [y_probas], axis=0)\n",
    "    if to_console:\n",
    "        for i in range(0, 10):\n",
    "            print(f\"GT / Prediction Pos Label / Probas: {gt[i]} / {y_preds_scores_pos_label[i]} / {y_preds_probas[i]}\")\n",
    "\n",
    "    return y_preds_scores_pos_label, y_preds_probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels, examples_per_label = read_ground_truth(gt_filepath, [0, 1], \",\", [\"url\", \"title\", \"body\", \"labels\"], to_console=console, examples_to_spit_out=examples_to_spit_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_perc = examples_per_label[1]/(examples_per_label[0] + examples_per_label[1])\n",
    "no_perc = examples_per_label[0]/(examples_per_label[0] + examples_per_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Yes/No %: {yes_perc}/{no_perc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_preds_pos_label, y_probas = read_binary_preds(predictions_filepath, gt_labels, to_console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas_pos_label = y_probas[:,0]  # Probability of the \"yes\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas_pos_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As Binary problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the values of calibration curve for bin pos_label vs all\n",
    "prob_true_binary, prob_pred_binary = calibration_curve(gt_labels, y_probas_pos_label, n_bins=10, normalize=False)\n",
    "\n",
    "def plot_reliability_diagram(prob_true, prob_pred, model_name, ax=None):\n",
    "    # Plot the calibration curve for ResNet in comparison with what a perfectly calibrated model would look like\n",
    "    if ax==None:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        plt.sca(ax)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color=\"#FE4A49\", linestyle=\":\", label=\"Perfectly calibrated model\")\n",
    "    plt.plot(prob_pred, prob_true, \"s-\", label=model_name, color=\"#162B37\")\n",
    "\n",
    "    plt.ylabel(\"Fraction of positives\", fontsize=16)\n",
    "    plt.xlabel(\"Mean predicted value\", fontsize=16,)\n",
    "\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.grid(True, color=\"#B2C7D9\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true_binary, prob_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(prob_true_binary, prob_pred_binary, \"pos class (Yes, 1) vs all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected calibration error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate ece\n",
    "def ece_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ece = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        ece = ece + (bin_sizes[m] / sum(bin_sizes)) * np.abs(prob_true[m] - prob_pred[m])\n",
    "    return ece\n",
    "\n",
    "# print the calculated ece\n",
    "n_bins_binary = len(prob_true_binary)\n",
    "pred_hist = np.histogram(a=y_preds_pos_label, range=(0, 1), bins=n_bins_binary)[0]\n",
    "print(ece_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mce_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    mce = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        mce = max(mce, np.abs(prob_true[m] - prob_pred[m]))\n",
    "    return mce\n",
    "\n",
    "#print the calculated mce\n",
    "print(mce_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsce_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE \n",
    "    rmsce = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        rmsce = rmsce + (bin_sizes[m] / sum(bin_sizes)) * (prob_true[m] - prob_pred[m]) ** 2\n",
    "    return np.sqrt(rmsce)\n",
    "\n",
    "# print the calculated rmsce\n",
    "print(rmsce_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As Multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt_multiclass = np.empty((0, 2))\n",
    "for y_gt in gt_labels:\n",
    "    if y_gt==1:\n",
    "        y_gt_multiclass = np.vstack((y_gt_multiclass, [1,0]))\n",
    "    else:\n",
    "        y_gt_multiclass = np.vstack((y_gt_multiclass, [0,1]))\n",
    "\n",
    "print(y_gt_multiclass.shape)\n",
    "print(y_gt_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ece_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    ece_bin = []\n",
    "    for a_class in range(y_true.shape[1]):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[:, a_class], y_pred[:, a_class], n_bins=10)\n",
    "        plot_reliability_diagram(prob_true, prob_pred, f\"Class {a_class}\")        \n",
    "        bin_sizes = np.histogram(a=y_pred[:, a_class], range=(0, 1), bins=len(prob_true))[0]\n",
    "        ece_bin.append(ece_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "#     return sum(ece_bin*class_weights)/n_classes\n",
    "    return sum(ece_bin*np.array([yes_perc, no_perc]))/2\n",
    "#     return np.mean(ece_bin)\n",
    "        \n",
    "    \n",
    "def mce_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    mce_bin = []\n",
    "    for a_class in range(y_true.shape[1]):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[:, a_class], y_pred[:, a_class], n_bins=10)\n",
    "        print(prob_true, prob_pred)\n",
    "        plot_reliability_diagram(prob_true, prob_pred, f\"Class {a_class}\")\n",
    "        bin_sizes = np.histogram(a=y_pred[:, a_class], range=(0, 1), bins=len(prob_true))[0]\n",
    "        mce_bin.append(mce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return sum(mce_bin*np.array([yes_perc, no_perc]))/2\n",
    "#     return np.mean(mce_bin)\n",
    "    \n",
    "def rmsce_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    rmsce_bin = []\n",
    "    for a_class in range(y_true.shape[1]):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[:, a_class], y_pred[:, a_class], n_bins=10)\n",
    "        plot_reliability_diagram(prob_true, prob_pred, f\"Class {a_class}\")\n",
    "        bin_sizes = np.histogram(a=y_pred[:, a_class], range=(0, 1), bins=len(prob_true))[0]\n",
    "        rmsce_bin.append(rmsce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return sum(rmsce_bin*np.array([yes_perc, no_perc]))/2    \n",
    "#     return np.mean(rmsce_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a check\n",
    "for x,y in y_probas:\n",
    "    np.testing.assert_almost_equal(x+y, 1.0, err_msg=\"Yay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_calculation_multiclass(y_gt_multiclass, y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mce_calculation_multiclass(y_gt_multiclass, y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsce_calculation_multiclass(y_gt_multiclass, y_probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
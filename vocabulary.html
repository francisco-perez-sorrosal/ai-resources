<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Vocabulary - AI Resources</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">2.</strong> Applications</a></li><li class="chapter-item expanded "><a href="multidisciplinary_approach.html"><strong aria-hidden="true">3.</strong> A Multidisciplinary Approach</a></li><li class="chapter-item expanded "><a href="approaches.html"><strong aria-hidden="true">4.</strong> Approaches</a></li><li class="chapter-item expanded "><a href="classical_ml.html"><strong aria-hidden="true">5.</strong> &quot;Classical&quot; Machine Learning</a></li><li class="chapter-item expanded "><a href="topics.html"><strong aria-hidden="true">6.</strong> Advanced Machine and Deep Learning Topics</a></li><li class="chapter-item expanded "><a href="nlp.html"><strong aria-hidden="true">7.</strong> NLP</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">8.</strong> To Production</a></li><li class="chapter-item expanded "><a href="tools_and_frameworks.html"><strong aria-hidden="true">9.</strong> Frameworks</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">10.</strong> Books and Resources</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">11.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="vocabulary.html" class="active"><strong aria-hidden="true">12.</strong> Vocabulary</a></li><li class="chapter-item expanded "><a href="people.html"><strong aria-hidden="true">13.</strong> People</a></li><li class="chapter-item expanded "><a href="history.html"><strong aria-hidden="true">14.</strong> Timeline/History</a></li><li class="chapter-item expanded affix "><a href="bibliography.html">Bibliography</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#a" id="a">A</a></h1>
<h2><a class="header" href="#auto-encoder" id="auto-encoder">Auto-Encoder</a></h2>
<p>Autoencoders are unsupervised ANN that can learn data encodings, making the encoder generate those encodings 
specifically for reconstructing its own input (See figure below.) They convert their inputs to encoded vectors that lie
in a latent space that may not be continuous or allow easy interpolation. In the end, this means that regular autoencoders 
are mostly limited to be used to generate compressed representations of their inputs, allowing to regenerate the original
input with minimal loss.</p>
<p><img src="images/autoencoder.png" alt="Autoencoder (Source: https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)" /></p>
<h1><a class="header" href="#b" id="b">B</a></h1>
<h2><a class="header" href="#backpropagation" id="backpropagation">Backpropagation</a></h2>
<p>A procedure to adjust the weights of a neural network by propagating the error obtained in the forward pass, backwards. 
After calculating the error in the output layer, e.g. by contrasting the output of the forward pass with the known 
so-called gold labels by means of a cost function, the gradient on the input weights of the last layer to the output units 
is calculated; then the weights of that layer are adjusted; this process is repeated backwards layer after layer until 
reaching the input layer. </p>
<h1><a class="header" href="#causality" id="causality">Causality</a></h1>
<p>Correlations is not causation. In ML causality tries to understand the relationships between data in order to create models
that generalize well.</p>
<h1><a class="header" href="#confusion-matrix" id="confusion-matrix">Confusion Matrix</a></h1>
<p>A confusion matrix is a tool used to evaluate the performance of a classification model by comparing the predicted labels to the actual labels. It provides a more detailed analysis of the model's performance than simple accuracy metrics. The confusion matrix is typically presented as a table with four main outcomes:</p>
<ul>
<li>True Positive (TP): Correctly predicted positive instances.</li>
<li>True Negative (TN): Correctly predicted negative instances.</li>
<li>False Positive (FP): Incorrectly predicted positive instances (Type I error).</li>
<li>False Negative (FN): Incorrectly predicted negative instances (Type II error).</li>
</ul>
<p>By examining the values in the confusion matrix, we can calculate various evaluation metrics such as precision, recall, and F1 score, which provide insights into the model's performance for different classes. The confusion matrix is an essential tool for understanding the strengths and weaknesses of a classification model.</p>
<p>Example of a confusion matrix:
<img src="images/conf_matrix_example.png" alt="Confusion Matrix Example" /></p>
<p>Main metrics derived from a confusion matrix:
<img src="images/metrics_from_conf_matrix.png" alt="Confusion Matrix Example" /></p>
<h1><a class="header" href="#e" id="e">E</a></h1>
<h2><a class="header" href="#expert-system" id="expert-system">Expert System</a></h2>
<h2><a class="header" href="#a-hrefvocabularyhtmlinterpretabilityexplainabilitya" id="a-hrefvocabularyhtmlinterpretabilityexplainabilitya"><a href="vocabulary.html#Interpretability">Explainability</a></a></h2>
<p><a href="#Interpretability">See Interpretability</a>.</p>
<h2><a class="header" href="#explanation" id="explanation">Explanation</a></h2>
<p>The explicit ability expected in an intelligent agent to give &quot;good&quot; explanations of its decisions to humans. As usual,
the problem here is to define what is a &quot;good&quot; explanation. If we try to mimic in the intelligent agent world, how 
humans proceed giving explanations to other humans, we have to acknowledge we'll have to deal with biases and/or social
expectations. Some researchers argue {{ cite graaf_how_2017 }} that the framework of explanation of those agents will need 
to be consistent with the conceptual framework and psychological mechanisms of human behavior explanation, as humans
will expect explanations falling under those two premises.</p>
<h1><a class="header" href="#f" id="f">F</a></h1>
<h2><a class="header" href="#feature" id="feature">Feature</a></h2>
<p>It's a property extracted from each data instance of a dataset being observed. An example would be the &quot;color&quot; attribute
extracted from row coding an outfit item example from a file called seasonal_outfits.csv). Features are
crucial for many ML procesess. Selecting those features it's an art, which even has a name: <a href="#feature_enginering">feature engineering</a>.
Some of the properties that a good feature should have for being selected are:</p>
<ul>
<li>Informative about the concept that they represent</li>
<li>Discriminating, to be able to separate one example instance from another </li>
<li>Independent, if possible from the other features extracted from the data instance</li>
</ul>
<h2><a class="header" href="#feature-engineering" id="feature-engineering">Feature Engineering</a></h2>
<p>The classical/traditional way of &quot;massage&quot; the input to pass to a ML model (e.g. a classifier.) It refers to the process 
of using domain knowledge to extract features from raw data. This was an &quot;art&quot; in itself usually done by domain experts.
In the age of DL, this has been substituted by the DL models themselves, which represents also the features on top of
which the learning of a task is done.</p>
<h2><a class="header" href="#few-shot-learning" id="few-shot-learning">Few-Shot Learning</a></h2>
<p>Humans, and more specifically children, are able to learn how to transfer their experience in similar tasks to a new one
by using only a small set of examples. For example a kid can learn how to multiply if she already knows how addition
works and with just a bunch of examples of  multiplications. It's even clearer in the world of images, in
which once they have learn to recognize a particular human face, children are able to identify the same face in a
bunch of photographs containing different faces.</p>
<p>In general, up to know, ML algorithms had to be trained in a supervised manner using a large number of examples to
learn. This is not the best scenario, as this poses many limitations, from the availability of datasets for the
task at hand to the energy consumption used for training those models. </p>
<p>Few-Shot Learning (FSL) [<a href="bibliography.html#fei-fei_one-shot_2006">fei-fei_one-shot_2006</a>] [<a href="bibliography.html#fink_object_2004">fink_object_2004</a>] are a type of ML problems in
which a model is trained -in what is called a meta -training phase- on
different related tasks. This step is supposed to give the model the ability to generalize adequately to unseen related 
supervised tasks using only a few bunch of new input data/examples in the testing/few-shot phase.</p>
<p>How many examples are considered few-shot training? [Unknown bib ref: rios_few_2018] mentions from 1 to 5.</p>
<p>A recent survey of few shot learning can be found in [<a href="bibliography.html#wang_generalizing_2020">wang_generalizing_2020</a>].
Different subdomains of FSL can be extrapolated from the regular ML domains such as:</p>
<ul>
<li>Few-shot classification</li>
<li>Few-shot regression</li>
<li>Few-shot reinforcement learning</li>
<li>...</li>
</ul>
<h1><a class="header" href="#g" id="g">G</a></h1>
<h2><a class="header" href="#generalization" id="generalization">Generalization</a></h2>
<p>The capability of an already trained ML model of adapting to previously unseen data taken from the same distribution as
the data used to train it.</p>
<h2><a class="header" href="#generative-adversarial-network-gan" id="generative-adversarial-network-gan">Generative Adversarial Network (GAN)</a></h2>
<p>A special architecture of ANN aimed to <em>generate new data</em> with similar statistics as of the ones found in a particular 
training set. The classical example of what GANs are used for, is the generation of new faces, by interpolating new
features from the data obtained from a pool of preexisting images of faces. The goal is to build the new images as real 
as possible, making them undistinguisable from real images for the human eye.</p>
<p>The idea is to train two models at the same time; the first one is the &quot;generative&quot; one, which serves as the &quot;trend 
gatherer&quot;, that is capturing the data distribution; the second one model, called &quot;discriminative&quot;, is trained to discern
if a particular sample comes from the training data or from the &quot;generative&quot; moidel [<a href="bibliography.html#goodfellow_generative_2014">goodfellow_generative_2014</a>]</p>
<p><img src="images/gan.png" alt="GAN (Source: https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)" /></p>
<h1><a class="header" href="#h" id="h">H</a></h1>
<h2><a class="header" href="#hallucination" id="hallucination">Hallucination</a></h2>
<p>In text generation tasks, it refers to misleading statements generated by the models when outputing their results
. Usually hallucinations have to do with the quality of the data the model was trained on; for example the</p>
<h1><a class="header" href="#i" id="i">I</a></h1>
<h2><a class="header" href="#interpretability" id="interpretability">Interpretability</a></h2>
<p>How well a human can understand the decisions (e.g. the output of a ML classifier) taken by an intelligent system in a given context.
This is related to the extent up to which humans can predict the results of a model. </p>
<h3><a class="header" href="#related" id="related">Related</a></h3>
<ul>
<li><a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/">Ian Goodfellow Interview</a></li>
</ul>
<h1><a class="header" href="#l" id="l">L</a></h1>
<h1><a class="header" href="#language-model" id="language-model">Language Model</a></h1>
<p>In any language, the words (or characters) in a sentence show certain correlations. Those correlations
capture and contextualize the underlying semantics and characteristics of the particular language. </p>
<p>Sequences of tokens can be found almost anywhere, being the words in a text, pixels in an image, the musical notes in
a score, etc. A language model could be defined as a statistical model that has learnt to predict the probability of a
sequence of tokens, capturing the correlation with other nearby tokens, either consecutive or not. In NLP, the tokens 
use to represent words or n-grams.</p>
<p>The calculation of the next token in the sequence \( x_n \) can be modeled as:</p>
<p>\( p(x_n | x_{n-1}, x_{n-2}...x_{1}) \)</p>
<p>where \( x_i \) represents the ith token in the sequence.</p>
<p>For more information see [<a href="bibliography.html#bengio_neural_2003">bengio_neural_2003</a>].</p>
<p>The best language model would be that which could best predict unseen data. To measure the quality of a language model
metrics such as <a href="#perplexity">Perplexity</a> can be used.</p>
<h1><a class="header" href="#neural-network" id="neural-network">Neural Network</a></h1>
<h1><a class="header" href="#neuromorphic-computing" id="neuromorphic-computing">Neuromorphic Computing</a></h1>
<p>A computing approach which model neurons as asynchronous and independent computation units which are stimulated by the
spikes triggered by other interconnected neurons, in a similar way as brain neurons behave.</p>
<p>Relying on asynchronous communication between the neurons, these neuromorphic systems do not need to rely on a system 
clock. So, this async communication of pulses to simulate neuron spikes is more enegy efficient, as it consumes less 
power.</p>
<h1><a class="header" href="#normalization" id="normalization">Normalization</a></h1>
<p>NN work best when input vectors/tensors are normalized, i.e. have lower mean and std in each dimension.
You can do input scaling and according weight initialization, but as the training goes, the mean and standard deviation
are blown up by the new inputs.</p>
<p>Layer normalization can be seen like a 'reset' of the weights between layers. </p>
<p>These are some references for weight norm [Unknown bib ref: saliman_weight_2016], batch norm [<a href="bibliography.html#ioffe_batch_2015">ioffe_batch_2015</a>], layer norm [<a href="bibliography.html#ba_layer_2016">ba_layer_2016</a>] and group norm [<a href="bibliography.html#wu_group_2018">wu_group_2018</a>].</p>
<h1><a class="header" href="#o" id="o">O</a></h1>
<h2><a class="header" href="#overfitting" id="overfitting">Overfitting</a></h2>
<p>The effect seen in a ML model when it seems to fit the training data so closely to the target goal that its unable to 
<a href="#Generalization">generalize</a> well to unseen data. When a model is said to be overfitted, usually we observe a low
error in the metrics from the train dataset and a high error in the metrics from the test dataset. </p>
<h1><a class="header" href="#p" id="p">P</a></h1>
<h2><a class="header" href="#perplexity" id="perplexity">Perplexity</a></h2>
<p>A measure for evaluating NLP models. It measures how good or bad a probability distribution/model predicts a sample.</p>
<p><img src="images/perplexity.png" alt="Perplexity (Source: https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3)" /></p>
<h1><a class="header" href="#r" id="r">R</a></h1>
<h2><a class="header" href="#roc-receiver-operating-characteristic-curve" id="roc-receiver-operating-characteristic-curve">RoC (Receiver Operating Characteristic) Curve</a></h2>
<p>The Receiver Operating Characteristic (ROC) curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for a binary classification model.</p>
<p>The true positive rate (TPR) is the proportion of actual positive samples that are correctly classified as positive, while the false positive rate (FPR) is the proportion of actual negative samples that are incorrectly classified as positive.</p>
<p>The ROC curve is created by plotting the TPR against the FPR. Each point on the curve represents a different threshold, so the curve provides a visual representation of the model's performance across all possible thresholds.</p>
<p>The shape of the ROC curve can also provide insights into the model's performance. A curve that is closer to the top-left corner indicates a better model; a curve closer to the diagonal line represents a weaker model.</p>
<p>The area under the ROC curve (AUC) is commonly used to evaluate the performance of a binary classification model. An AUC of 0.5 indicates that the model performs no better than random guessing, while an AUC of 1.0 indicates a perfect classifier.</p>
<p>A model with a higher AUC has better discrimination ability, which means it can better distinguish between positive and negative samples.</p>
<p><img src="images/roc_curve.png" alt="ROC Curve" /></p>
<h1><a class="header" href="#s" id="s">S</a></h1>
<h2><a class="header" href="#simulated-annealing" id="simulated-annealing">Simulated Annealing</a></h2>
<p>Inspired by the process of annealing in metal works, it describes a probabilistic approach to solve problems by 
&quot;heating&quot; them up and, subsequently, &quot;cooling&quot; them down. Let's see what this means.</p>
<p>The algorithmic solution of is applicable in large search domain problems with may contain several local optima points.
At the core of a simulated annealing algorithm, there's a temperature variable. This variable is set up with a high
value to simulate the heating process. As the algorithm proceeds with its iterations, the variable is allowed to be 
&quot;cooled down&quot;. While the temperature is high, the algorithm accepts solutions that are worse than the current solution;
that means in some way that is less risk averse. This allows the algorithm to jump out from locations with local optima
that may be appear early when executing. Gradually, as the temperature decreases, the probability of accepting worse 
solutions decreases, hopefully &quot;crystallizing&quot; on the area of the search space where the global optimum solution is located. </p>
<p>More info see [<a href="bibliography.html#kirkpatrick_optimization_1983">kirkpatrick_optimization_1983</a>].</p>
<h2><a class="header" href="#svm-support-vector-machine" id="svm-support-vector-machine">SVM (Support Vector Machine)</a></h2>
<p>Perceptron-based classifier. SVM learns how to separate points in the space by establishing the so-called decision boundaries.
When data is separable linearly, as it shown in many examples in the ML literature, it may seem a trivial task. However, 
data in the real world is not always linearly separable, being randomly distributed, making it hard the process of segregating
the different classes linearly. The kernel trick introduced by the SVM paper performs a mathematical trick to efficiently 
(in O(n)) map -for example- data from a 2-dimensional space to 3-dimensional space, where maybe it's possible to find a 
hyperplane that separates the different classes.</p>
<h1><a class="header" href="#v" id="v">V</a></h1>
<h2><a class="header" href="#variational-auto-encoder" id="variational-auto-encoder">Variational Auto-Encoder</a></h2>
<p>In contrast to a vanilla <a href="#auto-encoder">autoencoder</a>, a Variational AutoEncoder (VAE) is a <em>generative model</em> 
that shares most of the architecture with a regular autoencoder, like Generative Adversarial Networks. Because of this, 
VAEs have relatively little to do with classical autoencoders (sparse or denoising autoencoders) from a mathematical
point of view.
VAEs have a special property (which we could call the &quot;creativity&quot; property) that makes them more interesting over 
regular autoencoders for generating outputs; their latent spaces are 
continuous by design, which allows random sampling and interpolation. In a generative model this is what you want in the
end; randomly sample from the continuous latent space in order to &quot;distort a bit&quot; the input image generating an image 
variation, similar to the original one, but definitely not the same.</p>
<p>A VAE tries to maximize the probability of each X in the training set under the entire generative process
according to \( P(X) = \int P(X|z; \theta)P(z)dz \)</p>
<p>\( P(X|z; \theta) \), allows making the dependence of X on z explicit by using the law of total probability. This<br />
framework, called &quot;maximum likelihood&quot;, allows to assume that if the model is likely to produce training set samples, 
then it is also likely to produce similar samples, and also unlikely to produce dissimilar ones.</p>
<p>According to [<a href="bibliography.html#doersch_tutorial_2016">doersch_tutorial_2016</a>] VAEs are called &quot;autoencoders&quot; because the final training objective does
share the encoder/decoder architecture, so it resembles a traditional autoencoder.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="conferences.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="people.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="conferences.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="people.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>

<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>NLP - AI Resources</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">2.</strong> Applications</a></li><li class="chapter-item expanded "><a href="multidisciplinary_approach.html"><strong aria-hidden="true">3.</strong> A Multidisciplinary Approach</a></li><li class="chapter-item expanded "><a href="approaches.html"><strong aria-hidden="true">4.</strong> Approaches</a></li><li class="chapter-item expanded "><a href="topics.html"><strong aria-hidden="true">5.</strong> ML/DL Topics</a></li><li class="chapter-item expanded "><a href="nlp.html" class="active"><strong aria-hidden="true">6.</strong> NLP</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">7.</strong> To Production</a></li><li class="chapter-item expanded "><a href="tools_and_frameworks.html"><strong aria-hidden="true">8.</strong> Frameworks</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">9.</strong> Books and Resources</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">10.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="vocabulary.html"><strong aria-hidden="true">11.</strong> Vocabulary</a></li><li class="chapter-item expanded "><a href="people.html"><strong aria-hidden="true">12.</strong> People</a></li><li class="chapter-item expanded "><a href="history.html"><strong aria-hidden="true">13.</strong> Timeline/History</a></li><li class="chapter-item expanded affix "><a href="bibliography.html">Bibliography</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#nlp" id="nlp">NLP</a></h1>
<p>Initially, the NLP or Computational Linguistics field was focused on applying the generative grammar approach described 
by <a href="people.html#noam_comsky">Noam Chomsky</a> in <a href="https://en.wikipedia.org/wiki/Generative_grammar">the mid 60s</a>.
However, the results of applying that approach was never impressive.</p>
<p>Since then, other approachs like tagging Part of Speech (PoS) in sentences and applying statistical techniques have been
demostrated to be more successful in the NLP field. As it's usually described in any other modern ML field, the successful
application of these techniques has been only possible due to the increase in computing power and the tagging and recollection
of big datasets that have occurred in the last coupule of decades.</p>
<p>Nowadays, in the start of the third decade of the XXI century, the so called language models are predominant and applied 
in many of the problems related to NLP. </p>
<h2><a class="header" href="#frameworks" id="frameworks">Frameworks</a></h2>
<ul>
<li><a href="https://huggingface.co/">Huggingface</a> The de-facto standard framework for modern NLP.</li>
<li><a href="https://github.com/lucidrains/x-transformers">X-Transformers</a> A new repo, implementing also the later
advances in the spectrum of Transformer-based models.</li>
<li><a href="https://github.com/NervanaSystems/nlp-architect">NLP Architect</a></li>
</ul>
<h2><a class="header" href="#websites-blogs--repos" id="websites-blogs--repos">Websites, Blogs &amp; Repos</a></h2>
<ul>
<li><a href="https://paperswithcode.com/">Papers with Code</a></li>
<li><a href="http://nlpprogress.com/">NLP Progress</a></li>
<li><a href="https://github.com/neubig/lowresource-nlp-bootcamp-2020">NLP Bootcamp</a> CMU lectures on NLP by visitors to the
Language Technologies Institute.</li>
</ul>
<h1><a class="header" href="#nlp-topics" id="nlp-topics">NLP Topics</a></h1>
<h2><a class="header" href="#text-categorization" id="text-categorization">Text Categorization</a></h2>
<p>One of the classical problems in NLP.</p>
<p><strong>Goal</strong>: Assign labels/tags to text examples (e.g. sentences, paragraphs, documents...)
<strong>Options for doing text annotation</strong>:</p>
<ul>
<li>Manual - Reliess on humnans; Because of that fact this approachss doesn't scale, is costly, and error prone.</li>
<li>Automatic - The current trend due to the increasingly amount of text examples required for many applications in the
industry.
<ul>
<li>Rule-based methods
<ul>
<li>Use a set of predefined rules</li>
<li>Require domain knowledge from experts</li>
</ul>
</li>
<li>ML-driven methods
<ul>
<li>Use a set of prelabeled examples to train models.</li>
<li>Learn -during a training phase- based on observations of data contrasted against the true/gold labels already
tagged by domain experts for a certain number of the so-called train examples.</li>
<li>The final model obtained with this method has learned associations between the text and the labels</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We will focus on this approach only, and mainly on the ML-driven methods.</p>
<p><strong>Applications</strong>:</p>
<ol>
<li>Sentiment analysis</li>
<li>News classification</li>
<li>Content moderation</li>
<li>Spam filtering</li>
<li>Question-answering</li>
<li>Natural language inference
...</li>
</ol>
<h3><a class="header" href="#procedure" id="procedure">Procedure</a></h3>
<p>The traditional way of doing text classification consists of these steps:</p>
<ol start="0">
<li><strong>Dataset creation</strong> - Create (or download, if a well-know industry used dataset is considered to be used) at least
two datasets from the text examples available: train and test. See <a href="datasets.html">Datasets</a> section for more information.</li>
<li><strong>Preprocessing</strong> - Some handcrafted <a href="vocabulary.html#feature">features</a> are [extracted](vocabulary.md#feature
-engineering) from the train and test datasets. This may require also to do some transformations on the raw input data.</li>
<li><strong>Training</strong> - From each train example, use the features extracted + its associated label, as input to train a model
that will
learn associations from the features and the labels to make predictions on new input features.</li>
<li><strong>Testing</strong> - Feed a model with the features extracted from each test example to the train model to obtain a
prediction.</li>
<li><strong>Evaluation</strong> - Take each prediction obtained and contrast it with the corresponding true/gold label for test
examples and calculate the required <a href="metrics.html">metrics</a> for the classification problem at hand.</li>
</ol>
<p>Popular Algorithms used for text classification are <a href="algorithms_and_model_architectures.html#naive-bayes">Naive Bayes</a>,
<a href="algorithms_and_model_architectures.html#support-vector-machines">SVMs</a>, <a href="algorithms_and_model_architectures.html#hidden-markov-models">HMMs</a>,
<a href="algorithms_and_model_architectures.html#gradient-boosting-trees">GBTs</a> and <a href="algorithms_and_model_architectures.html#random-forests">random forests
</a></p>
<h2><a class="header" href="#entity-recognition" id="entity-recognition">Entity Recognition</a></h2>
<h2><a class="header" href="#question-answering" id="question-answering">Question-Answering</a></h2>
<h1><a class="header" href="#nlp-architectures" id="nlp-architectures">NLP Architectures</a></h1>
<h2><a class="header" href="#recent-origins" id="recent-origins">Recent Origins</a></h2>
<p>These papers influenced a paradigm shift towards what will be called <a href="vocabulary.html#deep-learning">Deep Learning</a>, which will imply the massive adoption of neural networks for ML tasks.</p>
<h3><a class="header" href="#word2vec" id="word2vec">Word2Vec</a></h3>
<p>&quot;Efficient Estimation of Word Representations in Vector Space&quot; [<a href="bibliography.html#mikolov_efficient_2013">mikolov_efficient_2013</a>] and &quot;Distributed 
Representations of Words and Phrases and their Compositionality&quot; [Unknown bib ref: mikolov_distributed_2013] or simply the
Word2Vec papers by Mikolov et al. at Google marked a paradigm shift in NLP, as it showed the potential of an embedding
model trained in large amounts of data (1.6 Billion data words). In particular, they showed the quality of the 
representations obtained after training by using a word similarity task.
A deeper explanation can be found in &quot;word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding
method&quot; {{ cite goldbert_word2vec_2014 }}</p>
<p><a href="https://code.google.com/archive/p/word2vec/">Google Source code</a> 
<a href="https://github.com/tmikolov/word2vec">Source code</a></p>
<p>The following is an amazing explanation of the W2V paper: <a href="https://jalammar.github.io/illustrated-word2vec/">Illustrated Word2Vec</a></p>
<h3><a class="header" href="#glove" id="glove">Glove</a></h3>
<p>Problem with Word Embeddings is that the representations that are spit out of them, despite they are powerful (e.g. 
you can do vector arithmetic with them), they are very shallow; shallow in the sense that only the first layer
(called the embedding layer) has seen all the huge corpus where where the vector representations were trained on. The
rest of the layers of the potentially deep NN model (e.g. LSTM or GRU) will be trained only on a (probably) way small
dataset for the application at hand.</p>
<p>So, why not pretrain more layers in order to learn grammar, disambiguate words based on context, etc.? This question
lead to the development of the so called <a href="vocabulary.html#language-model">Language Models</a>.</p>
<h3><a class="header" href="#elmo" id="elmo">Elmo</a></h3>
<p>&quot;Deep contextualized word representations&quot; {{ cite peters_deep_2018 }} a.k.a. the &quot;Elmo&quot; paper, improved the results
obtained by Word2Vec. The main difference is that Elmo adds context to word representations. Word vectors are
learned functions of the internal states of a deep bidirectional language model (biLM), pre-trained also on a large
text corpus. Essentially the model architecture is a stacked LSTM. This marked the start of the &quot;Sesame Street Saga&quot;.</p>
<p><a href="https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018">Slides</a></p>
<h3><a class="header" href="#ulmfit" id="ulmfit">ULMFit</a></h3>
<p>A 2018 LM from <a href="people.html#jeremy-howard">Jeremy Howard</a> and other <a href="http://fast.ai/">fast.ai</a> guys.</p>
<h2><a class="header" href="#attention" id="attention">Attention</a></h2>
<p>This was a game changer paper when it appeared in 2017 {{ cite vaswani_attention_2017 }}.
The concept of attention is taken, as many others, from cognitive sciences (e.g. psycology, neuroscience, education.) It 
describes the process of focusing on certain concrete stimulus/stimuli while ignoring the rest of stimuli in an 
environment. In the case of NLP for example, the context/environment can be a sentence and the stimulus a word.</p>
<ul>
<li><a href="">Attention and Memory-Augmented Networks for Dual-View Sequential Learning (KDD 2020)</a></li>
</ul>
<p>Encoder Components:</p>
<ol>
<li>(Masked) Self-Attention</li>
</ol>
<p>Input: Sequence of tensors (e.g. representing words in a sentence)
Output: Sequence of tensors; each one is a weighted sum of the input sequence</p>
<p>See <a href="vocabulary.html#normalization">Normalization</a> for more info on normalization.</p>
<ol start="2">
<li>Positional Encoding</li>
</ol>
<p>As word order is an important factor in LMs, Transformers combine word embeddings with position embeddings in its
input. This encoding will take into account the order of words when doing the computations. </p>
<ol start="3">
<li>Layer Normalization</li>
</ol>
<p>More resources on Transformers:</p>
<p><a href="http://peterbloem.nl/blog/transformers">Peter Bloem's Blog Entry</a></p>
<p>Apart from the attention mechanism, in there's been some recent research focus on how changing the attention layer
<a href="https://medium.com/syncedreview/google-replaces-bert-self-attention-with-fourier-transform-92-accuracy-7-times-faster-on-gpus-7a78e3e4ac0e">for a FFT</a>
can sped up the Transformer encoder architectures [Unknown bib ref: leethorp_fnet_2021]. </p>
<h4><a class="header" href="#sparse-transformers" id="sparse-transformers">Sparse Transformers</a></h4>
<ul>
<li><a href="refs.html#sparse">Sparse Tansformer</a> Self-attention complexity from O(n2) to O(n*sqrt(n)).</li>
<li><a href="refs.html#reformer">Reformer</a> Self-attention complexity O(L2) to O(LlogL), where L is the length of the sequence.</li>
<li><a href="refs.html#linformer">Linformer</a> Self-attention complexity from O(n2) to O(n) in both time and space.</li>
</ul>
<h2><a class="header" href="#transformers" id="transformers">Transformers</a></h2>
<p>Despite the Transformers architecture is very popular, it still has its drawbacks; for example it is
expensive to use with long sequences, e.g. with n &gt; 512. This makes this kind of models limited for certain NLP
tasks such as QA or summarization. More recent models such as Longformer, Performer, Reformer
, or Clustered attention have tried to address this problem by approximating the otherwise potentially huge attention
matrix. Out of those models, <a href="#big-bird">BigBird</a> seems to be the one that has achieve this goal more
effectively.</p>
<h3><a class="header" href="#sesame-street-saga" id="sesame-street-saga">Sesame Street Saga</a></h3>
<p>The <a href="#elmo">ELMO</a> paper started a trend to name many NLP model architectures and variations after the characters of
Sesame Street/Muppets. Some refer to this fenomenon as <a href="https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie">&quot;Muppetware&quot;</a>
These are the most relevant ones.</p>
<p>TODO mention at least <del>ELMo,</del> BERT, Grover, Big BIRD, Rosita, RoBERTa, ERNIEs, and KERMIT. </p>
<h4><a class="header" href="#bert" id="bert">BERT</a></h4>
<p>By 2018 Google developed what is still as of 2021, the SotA of embedding-based models for the majority of the industry.
Based on the Transformer architecture, it was trained on 3.3 billion words. Comming in two different flavours, base
and large, they mainly differ on the number of parameters.</p>
<p>BERT [<a href="bibliography.html#devlin_bert_2019">devlin_bert_2019</a>] has become the de-facto reference model for NLP since 2018.</p>
<h4><a class="header" href="#roberta-unknown-bib-ref-liu_roberta_2019" id="roberta-unknown-bib-ref-liu_roberta_2019">RoBERTa [Unknown bib ref: liu_roberta_2019]</a></h4>
<p>Replication study of BERT pretraining that measures the impact of many key hyperparameters (Bigger Batch size and LR) and training data size (10X).
It shows improvements on most of the SotA results by BERT and followers. Questions the results of some post-BERT models.
It uses a single sentence for the document-level input like SpanBERT.</p>
<p><a href="https://github.com/pytorch/fairseq">Original Code</a></p>
<h4><a class="header" href="#a-hrefhttpsarxivorgpdf190710529pdfspanberta" id="a-hrefhttpsarxivorgpdf190710529pdfspanberta"><a href="https://arxiv.org/pdf/1907.10529.pdf">SpanBERT</a></a></h4>
<p>Masks spans of words instead of random subwords. Spans of words refers to global entities or loca/domain-specific meaning (e.g. American Football)
Span Boundary Objective(SBO) predicts the span context from boundary token representations. Uses single sentence document-level inputs instead of
the two sentences in BERT.</p>
<p><a href="https://github.com/facebookresearch/SpanBERT">Original Code</a></p>
<h4><a class="header" href="#reformer" id="reformer">Reformer</a></h4>
<p>The main advantage of this model is that provides an attention mechanism for long sequences with O(Nlog(N))</p>
<h4><a class="header" href="#performer" id="performer">Performer</a></h4>
<h4><a class="header" href="#big-bird" id="big-bird">Big Bird</a></h4>
<p>The main questions according the authors of Big Bird that the paper addresses successfully are:</p>
<ul>
<li>&quot;Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products?&quot;</li>
<li>&quot;Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?&quot;</li>
</ul>
<p>This model [<a href="bibliography.html#zaheer_big_2020">zaheer_big_2020</a>] relies on what the authors call block sparse attention instead of the regular
O(N^2) attention mechanism. The attention in Big Bird is summarized in the following picture:</p>
<p><img src="images/big_bird_att.png" alt="Big Bird Attention" /></p>
<h5><a class="header" href="#small-modelssmall-devices" id="small-modelssmall-devices">Small Models/Small Devices</a></h5>
<ul>
<li><a href="refs.html#lite">Lite transformer with Long-Short Range Attention</a>
Uses Long-Short Range Attention (LSRA) in which a group of heads specializes in
the local context (using convolution) and another group specializes in the
long-distance relationships (ussing the attention mechanism.) Focus on edge (mobile) devices.</li>
</ul>
<details>
  <summary>Small Models</summary>
 * [Distilbert](https://arxiv.org/abs/1910.01108)
 Check it out in https://huggingface.co/
</details>
<details>
  <summary>Other Sesame Street Papers</summary>
<ul>
<li><a href="https://arxiv.org/abs/1908.10063">FinBERT</a> Bert applied to Financial Sentiment Analysis.
Code: https://github.com/ProsusAI/finBERT]</li>
<li><a href="refs.html#abcd">FinBERT</a></li>
</ul>
</details>
<h3><a class="header" href="#non-sesame-street-environment" id="non-sesame-street-environment">Non-Sesame Street Environment</a></h3>
<p>** <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing NLG</a>
&quot;Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP
tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and
research purposes. &lt;|endoftext|&gt;&quot; - Summary generated by itself.</p>
<h2><a class="header" href="#gpt-family" id="gpt-family">GPT Family</a></h2>
<p>Generative Pre-trained Transformer.
In the same way as <a href="#elmo">ELMO</a> and <a href="#umlfit">UMLFit</a> learns to predict the next word in a sentence. The main
difference with those two is that GPT, as BERT, uses an embedding layer and transformer layers instead of LSTMs.
It uses masked self-attention as it conditions only on preceding words.</p>
<p>GPT-2 was trained on 8M Web pages and comes in different sizes. It was considered at that time (2019) &quot;too 
dangerous&quot; to be publicly available. One of the tasks it was able to perform relatively well was the 
<a href="people.html#winograd">Winograd</a> Schema challenge for common sense reasoning, and in particular, pronoun dissambiguation.</p>
<p><a href="https://app.inferkit.com/demo">Talk to Transformer (Now inferkit)</a></p>
<h3><a class="header" href="#gpt-3" id="gpt-3">GPT-3</a></h3>
<p>175 Billion parameters, several thousands of GPUs to train and more than a month of training.
Weights weren't released due to concerns in potential misuses. There's a global effort by <a href="https://www.eleuther.ai/">EleutherAI</a> to make the
training and publish the weights open sourced.</p>
<h2><a class="header" href="#t5" id="t5">T5</a></h2>
<p>Text-To-Text Transfer Transformer, hence T5 [<a href="bibliography.html#raffel_exploring_2020">raffel_exploring_2020</a>]. And because of the Text-To-Text part
, input is a string and output is too.</p>
<p>Some of the figures are impressive:</p>
<ul>
<li>Released on early 2020</li>
<li>Trained on C4 corpus (100x bigger than Wikipedia)</li>
<li>11 Billion params</li>
<li>SotA in GLUE, SuperGlue and SQuAD</li>
</ul>
<p>Google Switch Transformer, has 1.6 Trillion parameters. </p>
<h3><a class="header" href="#lifelong-learning-in-nlp" id="lifelong-learning-in-nlp">Lifelong learning in NLP</a></h3>
<ul>
<li><a name="biesialska"><a href="https://www.aclweb.org/anthology/2020.coling-main.574.pdf">Continual Lifelong Learning in Natural Language Processing: A Survey</a> Colling, 2020</a></li>
</ul>
<h3><a class="header" href="#few-shot-learning-in-nlp" id="few-shot-learning-in-nlp">Few-Shot Learning in NLP</a></h3>
<p><a href="vocabulary.html#few-shot-learning">FSL</a> in NLP has been used successfully in applications like parsing translation
, sentence completion, sentiment classification from reviews, user intent classification for dialog systems, 
criminal charge prediction, word similarity tasks such as nonce definition, and multi-label text classification.
Recently, a new relation classification data set called FewRel [52] is released. </p>
<p>This compensates for the lack of benchmark data set for FSL tasks in natural language processing.</p>
<h2><a class="header" href="#text-generation" id="text-generation">Text Generation</a></h2>
<p>Data-to-Text Generation (DTG) or simpy, text generation is a subfield of NLP which pursues the automatic generation of
human-readable text using computational linguistics and AI.</p>
<p>Approaches to text generation use a <a href="vocabulary.html#language-model">language model (LM)</a> to generate the probability
distribution from we can sample to generate the next token in a sentence.</p>
<p>In the last few years, one of the most used generative models is the so-called Recurrent Neural Networks (RNN) that
have been used successfully also for other NLP task such as classification. </p>
<p>However, LMs per-se, despite promising for text generation, are limited in the control terms that humans
have for &quot;influencing&quot; the generated content. The problem relies on the fact that, once the models are trained
, it becomes difficult add control attributes without modifying the architecture to allow extra input
attributes or tuning with extra data. The prompts written by humans or generated automatically act just a starting
cue for the generator, but does not allow to control other properties such as define the topic of the generated
text.</p>
<p>Without these control attributes, models tend to <a href="vocabulary.html#hallucination">&quot;hallucinate&quot;</a>.
More recent approaches to text generation include these control mechanism: CTRL [Unknown bib ref: shirish_ctrl_2019] and PPLM
[<a href="bibliography.html#dathathri_plug_2020">dathathri_plug_2020</a>].</p>
<p><a href="https://github.com/salesforce/ctrl">Conditional Transformer Language (CTRL)</a> introduces control codes to condition the
language model. These control
codes govern
the style, content, and task-specific
behavior of the generated text. More specifically, the control codes allow 1) preserve the advantages of unsupervise
learning; 2) seamless
integration in the structure of the raw while providing the required control over generation, and 3) predict which
parts  of the training data are most likely given a sequence.</p>
<p>CTRL is trained on control codes that co-occur narurally with the original text typically used for training LMs.
Big datasets such Wikipedia are assigned with a domain-related control code; other smaller datasets (e.g. content from
specific online communities in Reddit are assigned to a broader domain name (Reddit) and with subdomain information
(e.g. r/politics.) All control codes can be traced to a particular subset of the training data.
Moreover, the codes can be combined with codes during generation to cross-over task-specific and domain/content
behaviors.</p>
<p>As CTRL, Plug and Play Language Model (PPLM) combines a pretrained LM with n &quot;attribute classifiers&quot; which allow to
drive the text generation process externally without architectural changes. This work was influenced by the
Plug &amp; Play Generative Networks (PPGN) work in computer vision (2017). In PPGN a discriminator (attribute model)
[ p(a|x) ] is plugged with a generative model p(x), so that sampling from the resulting [ p(x|a) \propto p(a|x
)p(x) ], effectively creates a generative model conditioned from the provided attribute a.
As the attribute is plugged post facto in the activation space, no further fine-tuning is required.</p>
<p>Another recent work is {{ cite rebuffel_controlling_2021 }}, based on RNNs. This work addresses hallucination by
treating it at a
word level, which is a more fine-grained approach than other works, which deal with hallucination at the instance
level. It proposes a procedure that consist of: 1) a word-level labeling procedure built on dependency parsing and
based on co-ocurrences and sentence structure; 2) a weighted multi-branch decoder which, guided by the alignment
labels from the previous step, will used them as word-level control factors. At a training time, the decoder
will learn generating descriptions without being misled by un-factual reference information. This is due to
the fact that the model will be able to distinguish between aligned and unaligned words.</p>
<h2><a class="header" href="#evaluation" id="evaluation">Evaluation</a></h2>
<p>In NLP, there are many different tasks that can be evaluated to test different aspects of the language. Some of
them are:</p>
<ul>
<li>Named Entity Recognition (NER): identify the different entities out of the words of a text: e.g. which words are a
proper name of a person or an organization.</li>
<li>Textual Entailment: when 2 sentences are provided to the LM, the first one entails or contradicts the other?</li>
<li>Coreference Resolution: when a pronoun appears in a text (e.g. “it”) that may possibly refer to multiple objects, try
to dissambiguate which object the pronoun refers to.</li>
</ul>
<p>The following are some of the main benchmarks used in NLP around this time (2021).</p>
<h3><a class="header" href="#squad" id="squad">SQuAD</a></h3>
<ul>
<li>Original paper: [<a href="bibliography.html#rajpurkar_squad_2016">rajpurkar_squad_2016</a>]</li>
<li>Task: Question answering</li>
<li>Details: 
<ul>
<li>100K question-answer pairs with the answer included in the question</li>
</ul>
</li>
<li>Example: </li>
</ul>
<p>&quot;In meteorology, precipitation is any product
of the condensation of atmospheric water vapor
that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... 
Precipitation forms as smaller
droplets coalesce via collision with other rain
drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are
called “showers”.</p>
<p>What causes precipitation to fall?
gravity&quot;</p>
<ul>
<li><a href="https://towardsdatascience.com/the-quick-guide-to-squad-cae08047ebee">Blog Entry about SQuAD</a></li>
</ul>
<h3><a class="header" href="#snli" id="snli">SNLI</a></h3>
<ul>
<li>Original paper: [<a href="bibliography.html#bowman_large_2015">bowman_large_2015</a>]</li>
<li>Task: Natural Language Inference</li>
<li>Details: 
<ul>
<li>Output the relationship between a piece of text and a hypothesis.</li>
<li>570K pairs</li>
</ul>
</li>
<li>Example:</li>
</ul>
<p>&quot;A black race car starts up in front of a crowd of people.</p>
<p>contradiction
C C C C C</p>
<p>A man is driving down a lonely road&quot;</p>
<h3><a class="header" href="#glue" id="glue">GLUE</a></h3>
<ul>
<li>Original Paper: [<a href="bibliography.html#wang_glue_2019">wang_glue_2019</a>]</li>
<li>Task: 9 tasks
<ul>
<li>Is sentence grammatical or not</li>
<li>Sentiment analysis (+,-,=)</li>
<li>Sentence B paraphrase of A?</li>
<li>Sentence similarity</li>
<li>Two questions similar?</li>
<li>Sentence Entailment</li>
<li>Entailment or contradiction?</li>
<li>B contains answer of question A?</li>
<li>Correct/incorrect referents (pronoums)?</li>
</ul>
</li>
<li>Details:
<ul>
<li><a href="https://mccormickml.com/2019/11/05/GLUE/">Blog Entry</a></li>
</ul>
</li>
</ul>
<h3><a class="header" href="#superglue" id="superglue">SuperGLUE</a></h3>
<p>TODO</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="topics.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="productionizing.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="topics.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="productionizing.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>

<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>NLP - AI Resources</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">2.</strong> Applications</a></li><li class="chapter-item expanded "><a href="multidisciplinary_approach.html"><strong aria-hidden="true">3.</strong> A Multidisciplinary Approach</a></li><li class="chapter-item expanded "><a href="approaches.html"><strong aria-hidden="true">4.</strong> Approaches</a></li><li class="chapter-item expanded "><a href="topics.html"><strong aria-hidden="true">5.</strong> ML/DL Topics</a></li><li class="chapter-item expanded "><a href="nlp.html" class="active"><strong aria-hidden="true">6.</strong> NLP</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">7.</strong> To Production</a></li><li class="chapter-item expanded "><a href="tools_and_frameworks.html"><strong aria-hidden="true">8.</strong> Frameworks</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">9.</strong> Books and Resources</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">10.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="people.html"><strong aria-hidden="true">11.</strong> People</a></li><li class="chapter-item expanded "><a href="vocabulary.html"><strong aria-hidden="true">12.</strong> Vocabulary</a></li><li class="chapter-item expanded affix "><a href="bibliography.html">Bibliography</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#nlp" id="nlp">NLP</a></h1>
<h2><a class="header" href="#frameworks" id="frameworks">Frameworks</a></h2>
<ul>
<li><a href="https://huggingface.co/">Huggingface</a> The de-facto standard framework for modern NLP.</li>
<li><a href="https://github.com/lucidrains/x-transformers">X-Transformers</a> A new repo, implementing also the later
advances in the spectrum of Transformer-based models.</li>
<li><a href="https://github.com/NervanaSystems/nlp-architect">NLP Architect</a></li>
</ul>
<h1><a class="header" href="#blogs--repos" id="blogs--repos">Blogs &amp; Repos</a></h1>
<ul>
<li><a href="https://github.com/neubig/lowresource-nlp-bootcamp-2020">NLP Bootcamp</a> CMU lectures on NLP by visitors to the
Language Technologies Institute.</li>
</ul>
<h1><a class="header" href="#nlp-topics" id="nlp-topics">NLP Topics</a></h1>
<h2><a class="header" href="#text-categorization" id="text-categorization">Text Categorization</a></h2>
<p>One of the classical problems in NLP.</p>
<p><strong>Goal</strong>: Assign labels/tags to text examples (e.g. sentences, paragraphs, documents...)
<strong>Options for doing text annotation</strong>:</p>
<ul>
<li>Manual - Reliess on humnans; Because of that fact this approachss doesn't scale, is costly, and error prone.</li>
<li>Automatic - The current trend due to the increasingly amount of text examples required for many applications in the
industry.
<ul>
<li>Rule-based methods
<ul>
<li>Use a set of predefined rules</li>
<li>Require domain knowledge from experts</li>
</ul>
</li>
<li>ML-driven methods
<ul>
<li>Use a set of prelabeled examples to train models.</li>
<li>Learn -during a training phase- based on observations of data contrasted against the true/gold labels already
tagged by domain experts for a certain number of the so-called train examples.</li>
<li>The final model obtained with this method has learned associations between the text and the labels</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We will focus on this approach only, and mainly on the ML-driven methods.</p>
<p><strong>Applications</strong>:</p>
<ol>
<li>Sentiment analysis</li>
<li>News classification</li>
<li>Content moderation</li>
<li>Spam filtering</li>
<li>Question-answering</li>
<li>Natural language inference
...</li>
</ol>
<h3><a class="header" href="#procedure" id="procedure">Procedure</a></h3>
<p>The traditional way of doing text classification consists of these steps:</p>
<ol start="0">
<li><strong>Dataset creation</strong> - Create (or download, if a well-know industry used dataset is considered to be used) at least
two datasets from the text examples available: train and test. See <a href="datasets.html">Datasets</a> section for more information.</li>
<li><strong>Preprocessing</strong> - Some handcrafted <a href="vocabulary.html#feature">features</a> are [extracted](vocabulary.md#feature
-engineering) from the train and test datasets. This may require also to do some transformations on the raw input data.</li>
<li><strong>Training</strong> - From each train example, use the features extracted + its associated label, as input to train a model
that will
learn associations from the features and the labels to make predictions on new input features.</li>
<li><strong>Testing</strong> - Feed a model with the features extracted from each test example to the train model to obtain a
prediction.</li>
<li><strong>Evaluation</strong> - Take each prediction obtained and contrast it with the corresponding true/gold label for test
examples and calculate the required <a href="metrics.html">metrics</a> for the classification problem at hand.</li>
</ol>
<p>Popular Algorithms used for text classification are <a href="algorithms_and_model_architectures.html#naive-bayes">Naive Bayes</a>,
<a href="algorithms_and_model_architectures.html#support-vector-machines">SVMs</a>, <a href="algorithms_and_model_architectures.html#hidden-markov-models">HMMs</a>,
<a href="algorithms_and_model_architectures.html#gradient-boosting-trees">GBTs</a> and <a href="algorithms_and_model_architectures.html#random-forests">random forests
</a></p>
<h2><a class="header" href="#entity-recognition" id="entity-recognition">Entity Recognition</a></h2>
<h2><a class="header" href="#question-answering" id="question-answering">Question-Answering</a></h2>
<h1><a class="header" href="#nlp-architectures" id="nlp-architectures">NLP Architectures</a></h1>
<h2><a class="header" href="#recent-origins" id="recent-origins">Recent Origins</a></h2>
<p>These papers influenced a paradigm shift towards what will be called <a href="vocabulary.html#deep-learning">Deep Learning</a>, which will imply the massive adoption of neural networks for ML tasks.</p>
<h3><a class="header" href="#word2vec" id="word2vec">Word2Vec</a></h3>
<p>&quot;Efficient Estimation of Word Representations in Vector Space&quot; {{ cite mikolov_efficient_2013 }} or simply the
Word2Vec paper by Mikolov et al. at Google marked a paradigm shift in NLP, as it showed the potential of an embedding
model trained in large amounts of data (1.6 Billion data words). In particular, they showed the quality of the 
representations obtained after training by using a word similarity task.
A deeper explanation can be found in &quot;word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding
method&quot; {{ cite goldbert_word2vec_2014 }}</p>
<p><a href="https://github.com/tmikolov/word2vec">Source code</a></p>
<h1><a class="header" href="#" id=""></a></h1>
<h3><a class="header" href="#elmo" id="elmo">Elmo</a></h3>
<p>&quot;Deep contextualized word representations&quot; {{ cite peters_deep_2018 }} a.k.a. the &quot;Elmo&quot; paper, improved the results
obtaineed by Word2Vec. The main differencde is that Elmo adds context to word representations. Word vectors are
learned functions of the internal states of a deep bidirectional language model (biLM), pre-trained also on a large
text corpus. This marked the start of the &quot;Sesame Street Saga&quot;</p>
<h2><a class="header" href="#attention" id="attention">Attention</a></h2>
<p>This was a game changer paper when it appeared in 2017 {{ cite vaswani_attention_2017 }}.
The concept of attention is taken, as many others, from cognitive sciences (e.g. psycology, neuroscience, education.) It 
describes the process of focusing on certain concrete stimulus/stimuli while ignoring the rest of stimuli in an 
environment. In the case of NLP for example, the context/environment can be a sentence and the stimulus a word.</p>
<ul>
<li><a href="">Attention and Memory-Augmented Networks for Dual-View Sequential Learning (KDD 2020)</a></li>
</ul>
<h4><a class="header" href="#sparse-transformers" id="sparse-transformers">Sparse Transformers</a></h4>
<ul>
<li><a href="refs.html#sparse">Sparse Tansformer</a> Self-attention complexity from O(n2) to O(n*sqrt(n)).</li>
<li><a href="refs.html#reformer">Reformer</a> Self-attention complexity O(L2) to O(LlogL), where L is the length of the sequence.</li>
<li><a href="refs.html#linformer">Linformer</a> Self-attention complexity from O(n2) to O(n) in both time and space.</li>
</ul>
<h2><a class="header" href="#transformers" id="transformers">Transformers</a></h2>
<h3><a class="header" href="#sesame-street-saga" id="sesame-street-saga">Sesame Street Saga</a></h3>
<p>The <a href="#elmo">ELMO</a> paper started a trend to name many NLP model architectures and variations after the characters of
Sesame Street/Muppets. Some refer to this fenomenon as <a href="https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie">&quot;Muppetware&quot;</a>
These are the most relevant ones.</p>
<p>TODO mention at least <del>ELMo,</del> BERT, Grover, Big BIRD, Rosita, RoBERTa, ERNIEs, and KERMIT. </p>
<h4><a class="header" href="#bert" id="bert">BERT</a></h4>
<p>By 2018 Google developed what is still as of 2021, the SotA of embedding-based models for the majority of the industry.
Based on the Transformer architecture, it was trained on 3.3 billion words. Comming in two different flavours, base
and large, they mainly differ on the number of parameters.</p>
<ul>
<li><a href="https://arxiv.org/abs/1810.04805">BERT</a> The reference model for NLP since 2018.</li>
<li><a href="https://arxiv.org/pdf/1907.10529.pdf">SpanBERT</a>
Masks spans of words instead of random subwords. Spans of words refers to global entities or loca/domain-specific meaning (e.g. American Football)
Span Boundary Objective(SBO) predicts the span context from boundary token representations. Uses single sentence document-level inputs instead of
the two sentences in BERT.
Code: https://github.com/facebookresearch/SpanBERT</li>
<li><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>
Replication study of BERT pretraining that measures the impact of many key hyperparameters (Bigger Batch size and LR) and training data size (10X).
It shows improvements on most of the SotA results by BERT and followers. Questions the results of some post-BERT models.
It uses a single sentence for the document-level input like SpanBERT.
Code: https://github.com/pytorch/fairseq</li>
</ul>
<h4><a class="header" href="#small-modelssmall-devices" id="small-modelssmall-devices">Small Models/Small Devices</a></h4>
<ul>
<li><a href="refs.html#lite">Lite transformer with Long-Short Range Attention</a>
Uses Long-Short Range Attention (LSRA) in which a group of heads specializes in
the local context (using convolution) and another group specializes in the
long-distance relationships (ussing the attention mechanism.) Focus on edge (mobile) devices.</li>
</ul>
<details>
  <summary>Small Models</summary>
 * [Distilbert](https://arxiv.org/abs/1910.01108)
 Check it out in https://huggingface.co/
</details>
<details>
  <summary>Other Sesame Street Papers</summary>
<ul>
<li><a href="https://arxiv.org/abs/1908.10063">FinBERT</a> Bert applied to Financial Sentiment Analysis.
Code: https://github.com/ProsusAI/finBERT]</li>
<li><a href="refs.html#abcd">FinBERT</a></li>
</ul>
</details>
<h3><a class="header" href="#non-sesame-street-environment" id="non-sesame-street-environment">Non-Sesame Street Environment</a></h3>
<p>** <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing NLG</a>
&quot;Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP
tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and
research purposes. &lt;|endoftext|&gt;&quot; - Summary generated by itself.</p>
<h3><a class="header" href="#lifelong-learning-in-nlp" id="lifelong-learning-in-nlp">Lifelong learning in NLP</a></h3>
<ul>
<li><a name="biesialska"><a href="https://www.aclweb.org/anthology/2020.coling-main.574.pdf">Continual Lifelong Learning in Natural Language Processing: A Survey</a> Colling, 2020</a></li>
</ul>
<h2><a class="header" href="#text-generation" id="text-generation">Text Generation</a></h2>
<p>Text generation is a subfield of NLP which pursues the automatic generation of human-readable text using
computational linguistics and AI.</p>
<p>Approaches to text generation use a <a href="vocabulary.html#language-model">language model</a> to generate the probability
distribution from we can sample to generate the next token in a sentence.</p>
<p>One of the most used generative models the so-called is Recurrent Neural Networks (RNN) </p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="topics.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="productionizing.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="topics.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="productionizing.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>

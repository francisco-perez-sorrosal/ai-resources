# AI as a Multidisciplinary Approach

This chapter is devoted to describe high level concepts, topics and discussions that other disciplines around ML are
providing.

# Brain -> Computer

With what we have now know about the brain, we can definitely say that does not operate on logical functions.
The brain is a compound of deeply intertwined subsystems based on neurons that mix the properties of analog and digital 
signals.

## Error Correction in the Brain

Apart from studying information [John von Neumman](people.md#John_von_Neumann) studied the brain and how can it be reliable
most of the time out of unreliable components (e.g. neurons misfiring in a particular region of the brain) He concluded
that redundancy was the key to make the brain correct mistakes. Redundancy is also key to build reliable centralized and distributed systems in computer 
software. 

# Algorithms and DL

In Chapter 13 of his [The Deep Learning Revolution] book, T. Segnosky talks about the work of [Stephen Wolfgram](people.md#Stephen_Wolfgram) 
and mentions what he calls Wofgram's law in the realm of neural networks. In April 2020, Wofgram presented his theory about
a new approach to find a fundamental theory of physics joint with an [accompanying project](https://www.wolframphysics.org/) 
to the world in an extensive [blog post](https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/).
He blog post points to with a document of more than 400 pages {{ cite wolfgram_class_2020 }} where he details all his ideas.

Since then, [researchers haven't been very receptive](https://www.scientificamerican.com/article/physicists-criticize-stephen-wolframs-theory-of-everything/#:~:text=Stephen%20Wolfram%20blames%20himself%20for%20not%20changing%20the%20face%20of%20physics%20sooner.&text=At%20its%20heart%2C%20Wolfram's%20new,resemble%20lines%20of%20computer%20code.) 
with his ideas. However, if from his theory we distill the idea that a meta-algorithm could be defined and used
to find other neural networks, that seems to smell to what [Quantum Computing](https://francisco-perez-sorrosal.github.io/qc-resources/)
may bring to the table in the future if it develops its full potential. In particular, Sejnowsky raises the question of
 whether it would be possible to find a region of the algorithm space faster than with gradient descent and how that
 may not be that crazy if it's compared with what nature has done to evolve organisms (he cites Stephen Jay Gould and Niles
Eldredge and the process they described in 1972 as ["punctuated equilibria"](https://en.wikipedia.org/wiki/Punctuated_equilibrium).

<iframe width='853' height='480' src='https://en.wikipedia.org/wiki/Punctuated_equilibrium#/media/File:Fossils_in_Evolutionary_Biology.png' frameborder='3' allowfullscreen>Example of Punctuated Equilibrium in Fossils</iframe>


# NLP

## Frameworks

* [Huggingface](https://huggingface.co/)
* [NLP Architect](https://github.com/NervanaSystems/nlp-architect)

# NLP Papers

## Recent Origins

* [Word2Vec](https://arxiv.org/abs/1301.3781)
* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)

## Transformers

### Sesame Street Environment

* [ELMO](https://arxiv.org/abs/1802.05365) Improvement over Word2Vec. Tries to add context to word representations.
Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), pre-trained on a large text corpus.
* [BERT](https://arxiv.org/abs/1810.04805) The reference model for NLP since 2018.
* [SpanBERT](https://arxiv.org/pdf/1907.10529.pdf)
 Masks spans of words instead of random subwords. Spans of words refers to global entities or loca/domain-specific meaning (e.g. American Football)
 Span Boundary Objective(SBO) predicts the span context from boundary token representations. Uses single sentence document-level inputs instead of
 the two sentences in BERT.
 Code: https://github.com/facebookresearch/SpanBERT
* [RoBERTa](https://arxiv.org/abs/1907.11692)
Replication study of BERT pretraining that measures the impact of many key hyperparameters (Bigger Batch size and LR) and training data size (10X).
It shows improvements on most of the SotA results by BERT and followers. Questions the results of some post-BERT models.
It uses a single sentence for the document-level input like SpanBERT.
Code: https://github.com/pytorch/fairseq

<details>
  <summary>Small Models</summary>
 * [Distilbert](https://arxiv.org/abs/1910.01108)
 Check it out in https://huggingface.co/
</details>

<details>
  <summary>Other Sesame Street Papers</summary>

 * [FinBERT](https://arxiv.org/abs/1908.10063) Bert applied to Financial Sentiment Analysis.
 Code: https://github.com/ProsusAI/finBERT]

</details>

### Non-Sesame Street Environment

** [Turing NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)
"Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP
tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and
research purposes. <|endoftext|>" - Summary generated by itself.
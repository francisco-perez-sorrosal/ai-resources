Pytorch conference:

INTRO CONF
----------

jax and julia languages

2020 state of ai report

Tensor Statements (Experimental)


Facebook MMF Multimodal library

MLPerf a machine learning performane benchnark suite with broad industry and academia



Microsoft DeepSpeed Dist training
Google Brain: ML for system
DiffTaichi: MIT

Usability: Try to balance again to performance.





State of pytorch
----------------

-More operators/quantized operators driven by numpy compatibility
-More language support
-More platforms

In 2020:
-Composability
-Distributed RPC
-XLA

To come:
-Multitensor optimizers
-Lazy modules
-torch.fx python to python transformations instead with C++
-Vulkan and Metal hw accel to mobile
-Benchmarking tools
-Metatensors

Contrib:
-rfcs: Proposals bigger than an issue
-dev newsletter
-nightly checkout tool


Complex numbers in pytorch
--------------------------
- Quantum mechanics and singal processing
-Research: Deep Complex networks, On complex valued convolutional networks, etc...
- Representation:
-- API easy to use based on a natural representation
-- Operations
-- Autograd

-new datatypes: torch.complex64/128
-core properties: angle, abs, real, imag, polar, complex based on numpy
-complex differencitation

Future:
-JIT and distribute computing

Pytorch marries Numpy
---------------------
-Compatible here means the same function can be called the same way to produce the same (or very similar) behaviour
-New functions added every day and plan to add linear algebra

High Level API for Autograd
---------------------------
torch.autograd.functional
-Takes a python function as an input
-Closer to functional API and mathematical definition

functional.jacobian(func, inputs,...)
functional.vjp(func, input,v,..) -- backprop
functional.jvp(...) -- forward mode
functinal.hessian(func, input,...) second order derivativeses
functinal.vhp(...) backward
functinal.hvp(...) forward

Pytorch RPC
-----------
DistributedDataParallel was the standard. The input was distributed but not works for large models as the model is centraliced

RPC in Pytorch: Flexible Low Level Tools
Features:
- Remote execution: Run user function or modules remotely
rpc_init,sync, async, return a remote reference (shared pointer), shutdown
- Remote reference
points to objects in a distributed env and can be passed as rpc arguments to avoid communicating real data
- Distributed autograd
Automatically stitch togethr local autograd to perform distributed differentiation across rpc boundaries

Use cases:
-Parameter server multiple trainers and serevers
-Distributed model parallel
-Pipeline parallelism

Pytorch DDP
-----------

DDP and C10D merges with RPC

Future adds:
Add zero (DeepSpeed) style training framework for large models
intra layer perallelism (Used by megatron model)
Torchscript support for c10d apis
Auto tuning for DDP
Hybrid Parallelism
Auto Parallelism


TorchText:
----------
-Raw text
Download, unzip, read
-Transforms
tokenize,vocabulary,sentencepiece
-Batch and sampler
DataLoader,Sampler
-Model
-Transformer,MultiheadAttention




Datasets
AG_NEWS, Sogounews

Tokenizer->vocab lookup->to tensor
Tokenizer->Vector Lookup

Multihead-attention container:
-Drop-in replacement
-torchscript support
-incremental decoding
-broadcast support
-research variants


Pyroch and Tensorboard
----------------------
pr_curve!!!

hyperpararmeter dashboard

Future:
Perf plugin
plotly
alert

Performance
-----------
Better AMP
tf32

Metal - Apple GPU inference
Vulkan - Android GPU inferene

Android and Pytorch
-------------------
Take advantage of GPUs/NPUs Neural Process Accelerators in mobile devices
Android Neural Networks API abstracts the different underlying vendor driver libs and chips
Pytorch supports NNAPI

Pytorch for Other Languages
---------------------------
- Libtorch (C++) Allows other programing languages binding to C++ have their own Torch-like library
- Serialized representation of models like torchscript and onnx. This allows to exchange trained models between
programming languages

Torch for R or Hasktorch are examples of a library implemented on Libtorch

The Gradient Review: Frameworks don't just enable machine learning resetarch, they enable and restrict what researchers can do


Pytorch for Graph Neural networks
---------------------------------

-Pythonic API
-For High Energy Physics
-Maybe for Quantum computing in the future?


Hyperparameter tuning
---------------------

hyperparameterss:
layers, units per layer, lr, etc.. determine the performance. can determine the success or failure of the models.
preprossingaugmentation, image format

 network trainerbatchs size image, optimizer chosen
 hwlayer : fp3 smixed precission


 Hyper-parameter optimization paper
 Suervay of ML expeprimettal methods at neurips2019 and iclr2020
    - 6% where using only HP optimizers

 Avoid Curse of Dimensionality
 An efficient approach for assesing hyperparameter importance

 Optuma came as a result.

 Best hyperparameters to tune:
 LR is the most hyer parameter
 Units in the firsr layer
 the optimizer
 dropout in the first layer

Hyperparameter evolution:

not tuning
manually play
grid search
optuna
optupta weveraged with hyperparameter tuning


DeepSpeed (Interesting)
---------
Training optimization library
Faster Transformer training
zero-offload: Democrazitzes big models
1-bit adam: 5x faster training

ONNX Runtime (ORT)
    optimized for pytorch
    integrating collections of optimization techiniques from Deepspeed


ML Perf
-------
GPU support in Pytorch.

NVIDIA trains BERT in 48.6s using 2048 A100 GPUs

Challenges:
Slow non-gpu work: Cuda graphs
model parallel utilities:
automated optimizations: Automated operator fusion (collaborate witht the JIT team at FB)

Dedicated pytorch team in NVIDIA


Going Forward
-------------
-Run in mobile devices, embedded hw like Rapsberry Pi, new devices
- ONNX Runtime and TVM

ONNX
-Need to expoert
-good perf out o f  the box
-files as means of exchange
should pytorch take ONNX?

TVM
-can take python JIT graph
-conversion implemented in Pythn
-typing one of the hard bits
can tune for new hw/optimization paterns

3ed option: hook into pytorch as torch vision ant trttorch

jit extension
allows to bind 3rd party libraries for benchmark or quick speedups on specifi ops
also fuse operation tugehter. trt requritest the jit grap in custom pass
you stay in pytorch
you cherry picks which things run externally

Reproducibility AI using Pytorch
-----------

MLFlow: Trochserve


Pytorch/XLA
-----------
TPUs

XLA compoiler based linear algrebra exec engine. targets cpu gpu and tpu

pytrch/xla is a layer eteen xla and pytorch

provides apis for out of the tree backend extensions.

Lazy tensor system . Build on top of the pytorch eager runtime.
stores the pytorch ops as intermidate representation
then pytroch xla to xla converting theIR node tho the HLOptimizer

TorchServe
----------

Pytorch
Need for custom code to deploy and predict
Need for custom infrastructure to scale and monitor



Use version for A/B testing
Ensemble support (commint soon) and batch inferencing
REST API, grpc API (coming soon)

Future:
- Improve memory and resource usage for scalability
- compliancy with serving APIs sun as KFServing (look at it)
- integrating with Captum
- Autoscalling for K8s


Differential Privacy
---------------------
Destroy a controlled amount of information

GDPR and used in the US census

Aggregates can be:
-count things
-a mean
-train a ML model

memorization violates privacy: has to do with losing memory

Differential privacy can be added to : data, training, model.zip, deployment
the gold standard is to add it to training: Opacus (limits memorization)

bit.ly/opacus-dev-day

Future of AI Tools
------------------

What was build in 2020:
-






Optuna




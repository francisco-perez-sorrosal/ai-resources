{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Read data set\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "# Inputs and labels\n",
    "with tf.name_scope(\"input/labels\"):\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Weights/bias for hidden layer\n",
    "with tf.name_scope(\"hidden_layer_1_weights\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, 10])*(1/tf.sqrt(784.0)))\n",
    "    b1 = tf.Variable(tf.truncated_normal([10]))\n",
    "    w1_hist = tf.summary.histogram(\"hidden_layer_1_W1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"hidden_layer_1_B1\", b1)\n",
    "\n",
    "# Weights/bias for output layer\n",
    "with tf.name_scope(\"output_layer_weights\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([10, 10])*(1/tf.sqrt(10.0)))\n",
    "    b2 = tf.Variable(tf.truncated_normal([10]))\n",
    "    w2_hist = tf.summary.histogram(\"output_layer_W2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"output_layer_B2\", b2)\n",
    "\n",
    "# Computational graph\n",
    "with tf.name_scope(\"hidden_1\"):\n",
    "    Z2 = tf.matmul(x, W1) + b1\n",
    "    z2_hist = tf.summary.histogram(\"hidden_layer_1_Z2\", Z2)\n",
    "\n",
    "    # Batch Normalization layer\n",
    "    bn_mean_2, bn_var_2 = tf.nn.moments(Z2, [0]) # Computes mean and var of Z2. Only one dimension is passed to axis\n",
    "    scale_2 = tf.Variable(tf.ones([10])) # we're gonna learn also this param. Init with 1's. Scale\n",
    "    beta_2 = tf.Variable(tf.zeros([10])) # we're gonna learn also this param. Init with 0's. Offset (or shift)\n",
    "    epsilon = 0.0001 # to avoid divide by zero\n",
    "    bn_2 = tf.nn.batch_normalization(Z2, bn_mean_2, bn_var_2, beta_2, scale_2, epsilon)\n",
    "\n",
    "    #A2 = tf.nn.relu(Z2) # without batch normalization\n",
    "    A2 = tf.nn.relu(bn_2)\n",
    "    a2_hist = tf.summary.histogram(\"hidden_layer_1_A2\", A2)\n",
    "\n",
    "# Computational graph\n",
    "with tf.name_scope(\"output\"):\n",
    "    y = tf.matmul(A2, W2) + b2\n",
    "    y_hist = tf.summary.histogram(\"output_y\", y)\n",
    "\n",
    "# Cost function\n",
    "with tf.name_scope(\"cost\"):\n",
    "    ce_cost = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    cross_entropy = tf.reduce_mean(ce_cost)\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    w1_grads, b1_grads = tf.gradients(cross_entropy, [W1, b1])\n",
    "    w2_grads, b2_grads = tf.gradients(cross_entropy, [W2, b2])\n",
    "    w1_grads_hist = tf.summary.histogram(\"w1_gradients\", w1_grads)\n",
    "    b1_grads_hist = tf.summary.histogram(\"b1_gradients\", b1_grads)\n",
    "    w2_grads_hist = tf.summary.histogram(\"w2_gradients\", w2_grads)\n",
    "    b2_grads_hist = tf.summary.histogram(\"b2_gradients\", b2_grads)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.01\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.5, staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Create session\n",
    "sess = tf.Session()\n",
    "tf.set_random_seed(1)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Test trained model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Tensorboard\n",
    "writer = tf.summary.FileWriter(\"./bn\", graph=tf.get_default_graph())\n",
    "cost_summary = tf.summary.scalar(\"cross-entropy\", cross_entropy)\n",
    "acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "params_summary = tf.summary.merge([w1_hist, b1_hist, w2_hist, b2_hist, z2_hist, a2_hist, y_hist])\n",
    "grad_summary = tf.summary.merge([w1_grads_hist, b1_grads_hist, w2_grads_hist, b2_grads_hist])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for calculations. Comment if you want to try the train below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "z2_ = sess.run(Z2, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "bn2_ = sess.run(bn_2, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(bn2_, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.var(bn2_, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(z2_, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.var(z2_, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.88863 0.1143\n",
      "1 2.96416 0.1209\n",
      "2 2.86708 0.1252\n",
      "3 2.69219 0.1286\n",
      "4 2.77342 0.1356\n",
      "5 2.60345 0.142\n",
      "6 2.74622 0.1498\n",
      "7 2.71651 0.1579\n",
      "8 2.63498 0.1677\n",
      "9 2.66297 0.1727\n",
      "10 2.60155 0.1782\n",
      "11 2.44917 0.1831\n",
      "12 2.53236 0.1894\n",
      "13 2.51441 0.1925\n",
      "14 2.49089 0.1966\n",
      "15 2.31554 0.1987\n",
      "16 2.41269 0.2013\n",
      "17 2.33383 0.2034\n",
      "18 2.55344 0.2055\n",
      "19 2.35927 0.2064\n",
      "20 2.27483 0.2085\n",
      "21 2.43961 0.2112\n",
      "22 2.50538 0.2137\n",
      "23 2.40121 0.216\n",
      "24 2.33549 0.2194\n",
      "25 2.29155 0.2234\n",
      "26 2.24471 0.227\n",
      "27 2.32553 0.2297\n",
      "28 2.25375 0.234\n",
      "29 2.31413 0.2374\n",
      "30 2.20564 0.2412\n",
      "31 2.00827 0.2441\n",
      "32 2.26211 0.2486\n",
      "33 2.06503 0.2521\n",
      "34 2.36415 0.2554\n",
      "35 2.04373 0.2574\n",
      "36 2.2507 0.2589\n",
      "37 2.44489 0.2606\n",
      "38 2.18389 0.2637\n",
      "39 2.14119 0.2682\n",
      "40 2.13096 0.2724\n",
      "41 2.12657 0.2747\n",
      "42 2.14146 0.2763\n",
      "43 2.43236 0.2788\n",
      "44 1.99696 0.2825\n",
      "45 2.09842 0.2854\n",
      "46 2.09897 0.2871\n",
      "47 2.08322 0.2905\n",
      "48 2.15861 0.2928\n",
      "49 1.90472 0.2945\n",
      "50 2.13452 0.2973\n",
      "51 1.98231 0.3003\n",
      "52 2.02409 0.3017\n",
      "53 1.96896 0.3035\n",
      "54 2.14343 0.3054\n",
      "55 2.0167 0.3069\n",
      "56 1.96151 0.3087\n",
      "57 2.17412 0.3111\n",
      "58 1.92642 0.3131\n",
      "59 2.01065 0.3156\n",
      "60 1.84031 0.3174\n",
      "61 1.97304 0.3179\n",
      "62 1.84941 0.3199\n",
      "63 2.05078 0.3218\n",
      "64 1.97744 0.3236\n",
      "65 2.07574 0.325\n",
      "66 1.95253 0.3269\n",
      "67 2.09693 0.3285\n",
      "68 2.07246 0.3299\n",
      "69 1.98779 0.3319\n",
      "70 1.98288 0.3325\n",
      "71 1.7801 0.3337\n",
      "72 1.94332 0.334\n",
      "73 1.99718 0.3355\n",
      "74 1.99001 0.3373\n",
      "75 2.0333 0.3397\n",
      "76 2.158 0.3417\n",
      "77 1.86285 0.3426\n",
      "78 2.12739 0.3445\n",
      "79 1.96123 0.3469\n",
      "80 2.11122 0.3481\n",
      "81 1.80804 0.35\n",
      "82 1.96986 0.3516\n",
      "83 2.08529 0.3525\n",
      "84 1.85858 0.3537\n",
      "85 1.90438 0.3555\n",
      "86 1.9979 0.3565\n",
      "87 1.93088 0.3575\n",
      "88 1.84088 0.36\n",
      "89 2.08182 0.3617\n",
      "90 1.92633 0.3629\n",
      "91 2.09103 0.3642\n",
      "92 1.83346 0.3645\n",
      "93 1.96249 0.3661\n",
      "94 1.94196 0.3683\n",
      "95 1.8717 0.3695\n",
      "96 2.1405 0.3716\n",
      "97 1.93855 0.3733\n",
      "98 1.90018 0.3757\n",
      "99 1.79176 0.3773\n",
      "100 1.89896 0.3784\n",
      "101 1.90846 0.3798\n",
      "102 1.73838 0.3823\n",
      "103 1.87258 0.384\n",
      "104 1.79924 0.3862\n",
      "105 1.76917 0.3869\n",
      "106 1.78316 0.3888\n",
      "107 1.92132 0.3911\n",
      "108 1.99968 0.3931\n",
      "109 1.69935 0.3933\n",
      "110 1.96132 0.3964\n",
      "111 1.84949 0.3973\n",
      "112 1.67445 0.3988\n",
      "113 1.80476 0.4008\n",
      "114 1.88171 0.4034\n",
      "115 1.75869 0.4058\n",
      "116 1.78617 0.407\n",
      "117 1.84679 0.4089\n",
      "118 1.70796 0.4113\n",
      "119 1.70949 0.4139\n",
      "120 1.74307 0.4163\n",
      "121 1.70068 0.4179\n",
      "122 1.84354 0.4202\n",
      "123 1.61897 0.4231\n",
      "124 1.70018 0.4241\n",
      "125 2.08888 0.4255\n",
      "126 1.79385 0.4268\n",
      "127 1.72586 0.4299\n",
      "128 1.74867 0.4307\n",
      "129 1.61628 0.4331\n",
      "130 1.64932 0.4365\n",
      "131 1.63671 0.4388\n",
      "132 1.74769 0.4416\n",
      "133 1.69244 0.4433\n",
      "134 1.72762 0.4439\n",
      "135 1.65044 0.4464\n",
      "136 1.66959 0.4484\n",
      "137 1.63761 0.4494\n",
      "138 1.59346 0.4507\n",
      "139 1.673 0.4539\n",
      "140 1.6851 0.4547\n",
      "141 1.75978 0.4579\n",
      "142 1.64039 0.4595\n",
      "143 1.80084 0.4616\n",
      "144 1.69684 0.4638\n",
      "145 1.77183 0.4644\n",
      "146 1.59989 0.4668\n",
      "147 1.58874 0.4695\n",
      "148 1.7923 0.4719\n",
      "149 1.79296 0.4734\n",
      "150 1.83237 0.4757\n",
      "151 1.71461 0.4795\n",
      "152 1.6146 0.4804\n",
      "153 1.51361 0.4807\n",
      "154 1.59312 0.482\n",
      "155 1.7014 0.4825\n",
      "156 1.50765 0.484\n",
      "157 1.65655 0.4847\n",
      "158 1.64452 0.4864\n",
      "159 1.75856 0.4861\n",
      "160 1.65458 0.4894\n",
      "161 1.61141 0.4926\n",
      "162 1.62854 0.4952\n",
      "163 1.72637 0.4963\n",
      "164 1.75099 0.4983\n",
      "165 1.8106 0.4993\n",
      "166 1.67201 0.501\n",
      "167 1.55124 0.5012\n",
      "168 1.6089 0.5048\n",
      "169 1.51322 0.5085\n",
      "170 1.69955 0.5101\n",
      "171 1.853 0.5104\n",
      "172 1.72315 0.5122\n",
      "173 1.55798 0.5136\n",
      "174 1.55441 0.5125\n",
      "175 1.89934 0.5129\n",
      "176 1.67302 0.5133\n",
      "177 1.64022 0.5131\n",
      "178 1.59266 0.5139\n",
      "179 1.47916 0.5152\n",
      "180 1.59223 0.5159\n",
      "181 1.60231 0.5174\n",
      "182 1.5275 0.5175\n",
      "183 1.55406 0.5186\n",
      "184 1.62692 0.5198\n",
      "185 1.55479 0.5207\n",
      "186 1.71066 0.5226\n",
      "187 1.65307 0.5236\n",
      "188 1.7415 0.524\n",
      "189 1.72721 0.5257\n",
      "190 1.5971 0.5258\n",
      "191 1.54294 0.528\n",
      "192 1.61741 0.5297\n",
      "193 1.46354 0.531\n",
      "194 1.54198 0.5315\n",
      "195 1.5689 0.5329\n",
      "196 1.52228 0.5344\n",
      "197 1.66842 0.5358\n",
      "198 1.55022 0.5359\n",
      "199 1.51144 0.5371\n",
      "200 1.44873 0.5381\n",
      "201 1.46227 0.5397\n",
      "202 1.57344 0.5409\n",
      "203 1.48501 0.543\n",
      "204 1.46744 0.545\n",
      "205 1.55739 0.5469\n",
      "206 1.57239 0.5496\n",
      "207 1.46855 0.5497\n",
      "208 1.41413 0.5499\n",
      "209 1.59124 0.5509\n",
      "210 1.45779 0.5506\n",
      "211 1.4537 0.5519\n",
      "212 1.50321 0.5525\n",
      "213 1.61569 0.5561\n",
      "214 1.59185 0.5594\n",
      "215 1.58503 0.5599\n",
      "216 1.68772 0.5612\n",
      "217 1.53183 0.5591\n",
      "218 1.57208 0.5621\n",
      "219 1.34035 0.5639\n",
      "220 1.45746 0.5647\n",
      "221 1.6342 0.5659\n",
      "222 1.56004 0.5673\n",
      "223 1.50507 0.569\n",
      "224 1.71212 0.5696\n",
      "225 1.40585 0.5713\n",
      "226 1.50964 0.5724\n",
      "227 1.53828 0.5727\n",
      "228 1.45141 0.5743\n",
      "229 1.38259 0.5744\n",
      "230 1.34607 0.5757\n",
      "231 1.47631 0.5796\n",
      "232 1.57844 0.5815\n",
      "233 1.65221 0.5838\n",
      "234 1.47761 0.5835\n",
      "235 1.32369 0.5845\n",
      "236 1.46552 0.5841\n",
      "237 1.35508 0.585\n",
      "238 1.3525 0.586\n",
      "239 1.45873 0.5861\n",
      "240 1.58643 0.5872\n",
      "241 1.62396 0.5875\n",
      "242 1.38307 0.5891\n",
      "243 1.48811 0.5883\n",
      "244 1.27652 0.5897\n",
      "245 1.43475 0.5902\n",
      "246 1.47067 0.5934\n",
      "247 1.63596 0.5952\n",
      "248 1.56365 0.5968\n",
      "249 1.50765 0.5973\n",
      "250 1.59296 0.6\n",
      "251 1.35643 0.601\n",
      "252 1.31456 0.6019\n",
      "253 1.37422 0.603\n",
      "254 1.41512 0.6037\n",
      "255 1.4522 0.605\n",
      "256 1.3749 0.606\n",
      "257 1.31184 0.6079\n",
      "258 1.51065 0.6101\n",
      "259 1.33425 0.6074\n",
      "260 1.5497 0.6093\n",
      "261 1.32078 0.6097\n",
      "262 1.41543 0.6112\n",
      "263 1.47585 0.6127\n",
      "264 1.57462 0.6145\n",
      "265 1.2561 0.6159\n",
      "266 1.51827 0.616\n",
      "267 1.50465 0.6164\n",
      "268 1.35254 0.6162\n",
      "269 1.41209 0.6158\n",
      "270 1.42701 0.6186\n",
      "271 1.51304 0.6201\n",
      "272 1.50528 0.6197\n",
      "273 1.5316 0.6218\n",
      "274 1.38349 0.6223\n",
      "275 1.53281 0.6236\n",
      "276 1.475 0.6247\n",
      "277 1.46614 0.6254\n",
      "278 1.22633 0.626\n",
      "279 1.3621 0.6262\n",
      "280 1.40609 0.6259\n",
      "281 1.09531 0.6265\n",
      "282 1.4413 0.6266\n",
      "283 1.32976 0.6264\n",
      "284 1.31357 0.6267\n",
      "285 1.30082 0.6281\n",
      "286 1.28818 0.6285\n",
      "287 1.3614 0.6293\n",
      "288 1.18938 0.6322\n",
      "289 1.20069 0.6331\n",
      "290 1.30953 0.6327\n",
      "291 1.27789 0.6351\n",
      "292 1.25366 0.635\n",
      "293 1.28146 0.6361\n",
      "294 1.44472 0.6368\n",
      "295 1.37358 0.638\n",
      "296 1.31984 0.6372\n",
      "297 1.26912 0.6378\n",
      "298 1.49056 0.6382\n",
      "299 1.23486 0.6395\n",
      "300 1.31987 0.6401\n",
      "301 1.38985 0.6412\n",
      "302 1.30507 0.6422\n",
      "303 1.24979 0.6412\n",
      "304 1.39389 0.6439\n",
      "305 1.15076 0.646\n",
      "306 1.29115 0.6465\n",
      "307 1.34241 0.6474\n",
      "308 1.16894 0.6488\n",
      "309 1.33209 0.6495\n",
      "310 1.41551 0.6503\n",
      "311 1.23449 0.6513\n",
      "312 1.23928 0.6524\n",
      "313 1.25489 0.6528\n",
      "314 1.40875 0.6546\n",
      "315 1.21137 0.6551\n",
      "316 1.24352 0.6562\n",
      "317 1.22668 0.657\n",
      "318 1.35881 0.6571\n",
      "319 1.23629 0.6565\n",
      "320 1.34848 0.6575\n",
      "321 1.286 0.6577\n",
      "322 1.21192 0.6593\n",
      "323 1.39556 0.6596\n",
      "324 1.52927 0.6608\n",
      "325 1.45682 0.6616\n",
      "326 1.27436 0.6623\n",
      "327 1.29146 0.6631\n",
      "328 1.37392 0.663\n",
      "329 1.19454 0.6645\n",
      "330 1.21993 0.6635\n",
      "331 1.11827 0.6645\n",
      "332 1.27666 0.6657\n",
      "333 1.34949 0.666\n",
      "334 1.15268 0.6661\n",
      "335 1.27185 0.667\n",
      "336 1.42427 0.6682\n",
      "337 1.17961 0.6691\n",
      "338 1.22253 0.6694\n",
      "339 1.14166 0.6698\n",
      "340 1.12196 0.6694\n",
      "341 1.19314 0.6699\n",
      "342 1.22183 0.6705\n",
      "343 1.36348 0.6713\n",
      "344 1.42302 0.6717\n",
      "345 1.206 0.6716\n",
      "346 1.21376 0.6717\n",
      "347 1.29145 0.6719\n",
      "348 1.21171 0.6709\n",
      "349 1.46123 0.6713\n",
      "350 1.16845 0.6718\n",
      "351 1.26605 0.674\n",
      "352 1.1643 0.6732\n",
      "353 1.07222 0.6732\n",
      "354 1.2807 0.6748\n",
      "355 1.2883 0.6749\n",
      "356 1.20486 0.6755\n",
      "357 1.27616 0.6764\n",
      "358 1.12266 0.676\n",
      "359 1.17572 0.6774\n",
      "360 1.25591 0.6784\n",
      "361 1.17958 0.6797\n",
      "362 1.32631 0.6806\n",
      "363 1.41099 0.6809\n",
      "364 1.2939 0.6807\n",
      "365 1.35609 0.6807\n",
      "366 1.14066 0.6795\n",
      "367 1.22097 0.6806\n",
      "368 1.09656 0.6812\n",
      "369 1.26297 0.6821\n",
      "370 1.06993 0.6822\n",
      "371 1.18622 0.6828\n",
      "372 1.29192 0.6826\n",
      "373 1.41089 0.6833\n",
      "374 1.23121 0.6841\n",
      "375 1.25857 0.6851\n",
      "376 1.30208 0.6858\n",
      "377 1.08153 0.6862\n",
      "378 1.15309 0.6876\n",
      "379 1.31662 0.6873\n",
      "380 1.24974 0.6889\n",
      "381 1.17882 0.689\n",
      "382 1.15394 0.6891\n",
      "383 1.06915 0.6891\n",
      "384 1.09384 0.69\n",
      "385 1.30159 0.691\n",
      "386 1.21892 0.6915\n",
      "387 1.14125 0.6915\n",
      "388 1.28131 0.6937\n",
      "389 1.15396 0.694\n",
      "390 1.152 0.6932\n",
      "391 1.14881 0.6916\n",
      "392 1.20586 0.693\n",
      "393 1.14273 0.6937\n",
      "394 1.15217 0.6938\n",
      "395 1.01007 0.694\n",
      "396 1.19745 0.6954\n",
      "397 1.25343 0.6959\n",
      "398 1.30053 0.6958\n",
      "399 1.37092 0.6977\n",
      "400 1.32827 0.6969\n",
      "401 1.21097 0.6975\n",
      "402 1.16768 0.6974\n",
      "403 1.0523 0.6984\n",
      "404 1.26679 0.6998\n",
      "405 1.14515 0.6994\n",
      "406 1.1332 0.6996\n",
      "407 1.13041 0.7\n",
      "408 1.29934 0.7005\n",
      "409 1.08282 0.7013\n",
      "410 1.15547 0.7006\n",
      "411 1.1584 0.7018\n",
      "412 1.14674 0.7022\n",
      "413 1.27663 0.7026\n",
      "414 1.10757 0.7031\n",
      "415 1.10269 0.7039\n",
      "416 1.08609 0.7032\n",
      "417 1.09941 0.7045\n",
      "418 1.11349 0.7048\n",
      "419 1.05928 0.7049\n",
      "420 1.1073 0.7056\n",
      "421 1.09562 0.7072\n",
      "422 1.25193 0.7075\n",
      "423 1.1556 0.708\n",
      "424 1.21453 0.7078\n",
      "425 1.17555 0.7087\n",
      "426 1.18251 0.7095\n",
      "427 1.10487 0.7093\n",
      "428 1.05416 0.7094\n",
      "429 1.32697 0.7088\n",
      "430 1.10868 0.7096\n",
      "431 1.00023 0.711\n",
      "432 1.09095 0.7107\n",
      "433 1.24593 0.7115\n",
      "434 0.997863 0.7131\n",
      "435 1.14374 0.7133\n",
      "436 1.08518 0.7129\n",
      "437 1.10361 0.713\n",
      "438 1.09895 0.7141\n",
      "439 1.2796 0.714\n",
      "440 1.33506 0.7145\n",
      "441 1.0949 0.7146\n",
      "442 1.13247 0.7145\n",
      "443 0.935142 0.7149\n",
      "444 1.24847 0.7165\n",
      "445 1.41368 0.7165\n",
      "446 1.31559 0.7164\n",
      "447 1.14821 0.7176\n",
      "448 1.18882 0.7168\n",
      "449 1.18791 0.7176\n",
      "450 1.09072 0.7189\n",
      "451 1.04922 0.7188\n",
      "452 1.12316 0.7202\n",
      "453 1.28273 0.7196\n",
      "454 1.18977 0.7205\n",
      "455 1.17533 0.722\n",
      "456 1.11464 0.7225\n",
      "457 1.23306 0.723\n",
      "458 1.05089 0.724\n",
      "459 0.992064 0.7241\n",
      "460 0.961106 0.7233\n",
      "461 1.1361 0.7231\n",
      "462 1.07693 0.7238\n",
      "463 1.17052 0.7251\n",
      "464 1.01284 0.7264\n",
      "465 0.938896 0.7255\n",
      "466 1.07667 0.7263\n",
      "467 1.11719 0.7271\n",
      "468 1.12244 0.7281\n",
      "469 1.02801 0.7284\n",
      "470 1.15752 0.7283\n",
      "471 1.28366 0.7283\n",
      "472 1.07958 0.7283\n",
      "473 1.01248 0.7281\n",
      "474 0.976795 0.7286\n",
      "475 0.965344 0.7292\n",
      "476 0.983319 0.7287\n",
      "477 1.17823 0.7299\n",
      "478 1.16591 0.7292\n",
      "479 1.17698 0.7301\n",
      "480 1.16258 0.7297\n",
      "481 0.996711 0.731\n",
      "482 1.04798 0.733\n",
      "483 0.885436 0.7324\n",
      "484 1.07934 0.7328\n",
      "485 1.10755 0.7321\n",
      "486 1.08857 0.7324\n",
      "487 1.04618 0.7335\n",
      "488 0.904662 0.7342\n",
      "489 1.19902 0.7342\n",
      "490 1.17075 0.7347\n",
      "491 0.982973 0.7351\n",
      "492 1.01994 0.7357\n",
      "493 0.955832 0.7354\n",
      "494 1.04217 0.7367\n",
      "495 1.06501 0.7364\n",
      "496 0.947905 0.7369\n",
      "497 0.97391 0.7384\n",
      "498 0.986355 0.739\n",
      "499 1.23323 0.7393\n"
     ]
    }
   ],
   "source": [
    "for step in range(500) :\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, cost, cross_entropy_summary, params_hist_summary, gradients_hist_summary  = sess.run([train_step, cross_entropy, cost_summary, params_summary, grad_summary], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    test_acc, test_acc_summary = sess.run([accuracy, acc_summary], feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "\n",
    "    writer.add_summary(cross_entropy_summary, step) \n",
    "    writer.add_summary(test_acc_summary, step) \n",
    "    writer.add_summary(params_hist_summary, step) \n",
    "    writer.add_summary(gradients_hist_summary, step) \n",
    "    print step, cost, test_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

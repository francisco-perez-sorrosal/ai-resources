{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, sys, tflearn\n",
    "\n",
    "# Query/Ad length\n",
    "query_len = 35\n",
    "ad_len = 140\n",
    "\n",
    "# char dictionary\n",
    "gap_fill_char = '\\xff'\n",
    "unknown_char = '\\xfe'\n",
    "ad_col_seperator_1 = '\\xfd'\n",
    "ad_col_seperator_2 = '\\xfc'\n",
    "\n",
    "char_set = set(string.printable) - set(string.ascii_uppercase)\n",
    "char_set.add(unknown_char); \n",
    "char_set.add(ad_col_seperator_1); \n",
    "char_set.add(ad_col_seperator_2)\n",
    "char_set_len = len(char_set)\n",
    "\n",
    "char_to_ix = {}; ix_to_char = {};\n",
    "for i,ch in enumerate(char_set):\n",
    "    char_to_ix[ch] = i\n",
    "    ix_to_char[i] = ch\n",
    "\n",
    "# Filler\n",
    "def fill_string(s, length, char='\\xff'):\n",
    "    length_of_string = len(s)\n",
    "    if length_of_string < length:\n",
    "        return s + (length - length_of_string)*char\n",
    "    else:\n",
    "        return s[:length]\n",
    "\n",
    "# Load editorial data set\n",
    "def load_editorial_data(dataset, nrows=None):\n",
    "\n",
    "    data_df = pd.read_csv(dataset, delimiter='\\t', nrows=nrows)\n",
    "    data_df.columns = ['phrase_canon_user_query', 'ad_title', 'ad_description', 'ad_url','click']\n",
    "    data_df['ad'] = data_df['ad_title'] + ad_col_seperator_1 + data_df['ad_description'] + ad_col_seperator_2 + data_df['ad_url']\n",
    "    data_df = data_df[['phrase_canon_user_query', 'ad', 'click']]\n",
    "    data_df['phrase_canon_user_query'] = data_df['phrase_canon_user_query'].str.slice(start=0,stop=query_len).astype(str)\n",
    "    data_df['ad'] = data_df['ad'].str.slice(start=0,stop=ad_len).astype(str)\n",
    "    query = data_df['phrase_canon_user_query'].apply(fill_string, length=query_len).values.tolist()\n",
    "    ad = data_df['ad'].apply(fill_string, length=ad_len).values.tolist()\n",
    "    labels = data_df['click'].values.astype(np.float32)\n",
    "    del data_df\n",
    "    return [ad, query, labels]\n",
    "\n",
    "# Vectorize query/ad text (1-hot encoding)\n",
    "def vectorize_text(in_list):\n",
    "\n",
    "    out_tensor = np.zeros((len(in_list), len(in_list[0]), char_set_len), dtype=np.float32)\n",
    "    for i, s in enumerate(in_list):\n",
    "        for t,char in enumerate(s):\n",
    "            if (char in char_set):\n",
    "                out_tensor[i, t, char_to_ix[char]] = 1\n",
    "\n",
    "            elif (char != gap_fill_char):\n",
    "                out_tensor[i, t, char_to_ix[unknown_char]] = 1\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "# Convolutional Block\n",
    "def conv_block(x, filter_size, number_of_filters, num_of_layers, trainable, restore, name=\"Conv1D\"):\n",
    "    net = x\n",
    "    for i in range(num_of_layers):\n",
    "        net = tflearn.layers.conv.conv_1d(net, nb_filter=number_of_filters, \n",
    "                filter_size=filter_size, strides=1, \n",
    "                padding='same', activation= 'linear', \n",
    "                weights_init = 'variance_scaling',\n",
    "                trainable=trainable, restore=restore, name=name)\n",
    "\n",
    "        net = tflearn.layers.normalization.batch_normalization(\n",
    "                net, trainable=trainable, restore=restore, \n",
    "                name=\"batch_normalization/\"+name)\n",
    "\n",
    "        net = tf.nn.relu(net, name=\"ReLU/\"+name)\n",
    "    return net\n",
    "\n",
    "\n",
    "# Model - Ads\n",
    "baseFactor = 64\n",
    "ad_place_holder = tf.placeholder(tf.float32, shape=[None, ad_len, char_set_len])\n",
    "ad_conv_1 = tflearn.layers.conv.conv_1d(ad_place_holder, nb_filter=baseFactor, \n",
    "        filter_size=3, strides=1, padding='same', \n",
    "        activation= 'linear', weights_init = 'variance_scaling',\n",
    "        trainable=True, restore=True, name=\"AdConv1\")\n",
    "\n",
    "ad_conv_2 = conv_block(ad_conv_1, filter_size=3, number_of_filters=baseFactor,\n",
    "        num_of_layers=2, trainable=True, restore=True, name=\"AdConv2\")\n",
    "ad_conv_3 = conv_block(ad_conv_2, filter_size=3, number_of_filters=baseFactor, \n",
    "        num_of_layers=2, trainable=True, restore=True, name=\"AdConv3\")\n",
    "ad_conv_3_max_pool = tflearn.layers.conv.max_pool_1d(ad_conv_3, \n",
    "        kernel_size=2, name=\"AdConv3MaxPool\")\n",
    "\n",
    "# Model - Queries\n",
    "query_place_holder = tf.placeholder(tf.float32, shape=[None, query_len, char_set_len])\n",
    "query_conv_1 = tflearn.layers.conv.conv_1d(query_place_holder, nb_filter=baseFactor, \n",
    "        filter_size=3, strides=1, \n",
    "        padding='same', activation= 'linear', weights_init = 'variance_scaling',\n",
    "        trainable=True, restore=True, name=\"QueryConv1\")\n",
    "\n",
    "query_conv_2 = conv_block(query_conv_1, filter_size=3, number_of_filters=baseFactor, \n",
    "        num_of_layers=2, trainable=True, restore=True, name=\"QueryConv2\")\n",
    "query_conv_3 = conv_block(query_conv_2, filter_size=3, number_of_filters=baseFactor, \n",
    "        num_of_layers=2, trainable=True, restore=True, name=\"QueryConv3\")\n",
    "\n",
    "# Ad/Query Cross Convolutional matrix\n",
    "seq_length_of_query = query_conv_3.get_shape()[1].value\n",
    "seq_length_of_ad = ad_conv_3_max_pool.get_shape()[1].value\n",
    "\n",
    "ad_entended_matrix = []\n",
    "for i in range(seq_length_of_query):\n",
    "    ad_entended_matrix.append(ad_conv_3_max_pool)\n",
    "ad_entended_matrix = tf.concat(ad_entended_matrix, axis=1)\n",
    "\n",
    "query_extended_matrix = []\n",
    "queryMatrixUnpacked = tf.unstack(query_conv_3, num=None, axis=1)\n",
    "for actualQueryRow in queryMatrixUnpacked:\n",
    "    for i in range(seq_length_of_ad):\n",
    "        query_extended_matrix.append(actualQueryRow)\n",
    "query_extended_matrix = tf.stack(query_extended_matrix, axis=1)\n",
    "query_ads = tf.concat([query_extended_matrix, ad_entended_matrix], axis=2)\n",
    "\n",
    "# Session run\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(\"./run_1\", graph=tf.get_default_graph())\n",
    "\n",
    "# Testing\n",
    "[ads, queries, labels] = load_editorial_data(\"small_train.txt\")\n",
    "ad_vectors =  vectorize_text(ads[:512]) \n",
    "# Input\n",
    "query_vectors =  vectorize_text(queries[:512]) \n",
    "labels_vector = labels[:512]\n",
    "\n",
    "ads_ = sess.run(ad_conv_3_max_pool, feed_dict={ad_place_holder:ad_vectors})\n",
    "queries_ = sess.run(query_conv_3, feed_dict={ad_place_holder:ad_vectors, query_place_holder:query_vectors})\n",
    "ad_matrix = sess.run(ad_entended_matrix, feed_dict={ad_place_holder:ad_vectors, query_place_holder:query_vectors})\n",
    "query_matrix = sess.run(query_extended_matrix, feed_dict={ad_place_holder:ad_vectors, query_place_holder:query_vectors})\n",
    "query_ads_ = sess.run(query_ads, feed_dict={ad_place_holder:ad_vectors, query_place_holder:query_vectors})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

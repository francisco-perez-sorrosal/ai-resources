{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.50518584],\n",
       "         [ 0.73519874],\n",
       "         [ 0.55573463],\n",
       "         [ 0.41568804],\n",
       "         [-0.28822255]],\n",
       "\n",
       "        [[-0.46627617],\n",
       "         [ 0.82505345],\n",
       "         [ 0.41570282],\n",
       "         [ 0.3787756 ],\n",
       "         [-0.94981098]],\n",
       "\n",
       "        [[-0.86273956],\n",
       "         [-0.6960845 ],\n",
       "         [ 0.86087441],\n",
       "         [ 0.31347966],\n",
       "         [-0.70282483]],\n",
       "\n",
       "        [[ 0.54513073],\n",
       "         [ 0.99571466],\n",
       "         [ 0.08626652],\n",
       "         [ 0.6693058 ],\n",
       "         [-0.70080662]],\n",
       "\n",
       "        [[-0.30533576],\n",
       "         [ 0.59425211],\n",
       "         [-0.73013449],\n",
       "         [ 0.56492805],\n",
       "         [-0.0988152 ]],\n",
       "\n",
       "        [[ 0.95599389],\n",
       "         [-0.89279747],\n",
       "         [-0.44778395],\n",
       "         [-0.29341841],\n",
       "         [-0.48117065]],\n",
       "\n",
       "        [[ 0.54513073],\n",
       "         [ 0.99571466],\n",
       "         [ 0.08626652],\n",
       "         [ 0.6693058 ],\n",
       "         [-0.70080662]]],\n",
       "\n",
       "\n",
       "       [[[-0.61933088],\n",
       "         [ 0.41693616],\n",
       "         [-0.42060399],\n",
       "         [ 0.62799668],\n",
       "         [ 0.62960315]],\n",
       "\n",
       "        [[-0.32618475],\n",
       "         [ 0.01747489],\n",
       "         [ 0.06450629],\n",
       "         [-0.9572382 ],\n",
       "         [-0.82381082]],\n",
       "\n",
       "        [[-0.41804576],\n",
       "         [-0.22154641],\n",
       "         [ 0.26282668],\n",
       "         [ 0.82127666],\n",
       "         [ 0.96237779]],\n",
       "\n",
       "        [[ 0.78811312],\n",
       "         [-0.59079242],\n",
       "         [-0.5485909 ],\n",
       "         [ 0.93735909],\n",
       "         [-0.40100217]],\n",
       "\n",
       "        [[-0.30533576],\n",
       "         [ 0.59425211],\n",
       "         [-0.73013449],\n",
       "         [ 0.56492805],\n",
       "         [-0.0988152 ]],\n",
       "\n",
       "        [[-0.32197475],\n",
       "         [-0.2799592 ],\n",
       "         [-0.9332788 ],\n",
       "         [-0.59125972],\n",
       "         [ 0.72142744]],\n",
       "\n",
       "        [[ 0.78811312],\n",
       "         [-0.59079242],\n",
       "         [-0.5485909 ],\n",
       "         [ 0.93735909],\n",
       "         [-0.40100217]]],\n",
       "\n",
       "\n",
       "       [[[ 0.16144252],\n",
       "         [ 0.6759944 ],\n",
       "         [ 0.3273139 ],\n",
       "         [-0.0929141 ],\n",
       "         [ 0.35625124]],\n",
       "\n",
       "        [[ 0.41530848],\n",
       "         [ 0.3720572 ],\n",
       "         [-0.75039601],\n",
       "         [ 0.32620716],\n",
       "         [ 0.167346  ]],\n",
       "\n",
       "        [[-0.35582829],\n",
       "         [ 0.96897054],\n",
       "         [-0.6783216 ],\n",
       "         [-0.17654657],\n",
       "         [-0.44879293]],\n",
       "\n",
       "        [[-0.69206977],\n",
       "         [ 0.40419507],\n",
       "         [-0.03381586],\n",
       "         [-0.31715989],\n",
       "         [-0.21509647]],\n",
       "\n",
       "        [[-0.30533576],\n",
       "         [ 0.59425211],\n",
       "         [-0.73013449],\n",
       "         [ 0.56492805],\n",
       "         [-0.0988152 ]],\n",
       "\n",
       "        [[-0.79579592],\n",
       "         [-0.51428699],\n",
       "         [-0.78405142],\n",
       "         [-0.45906925],\n",
       "         [-0.62541842]],\n",
       "\n",
       "        [[-0.69206977],\n",
       "         [ 0.40419507],\n",
       "         [-0.03381586],\n",
       "         [-0.31715989],\n",
       "         [-0.21509647]]],\n",
       "\n",
       "\n",
       "       [[[ 0.85071635],\n",
       "         [-0.20816302],\n",
       "         [-0.36474776],\n",
       "         [-0.4629705 ],\n",
       "         [ 0.24757409]],\n",
       "\n",
       "        [[ 0.03944516],\n",
       "         [-0.33618975],\n",
       "         [ 0.71163177],\n",
       "         [-0.48284125],\n",
       "         [ 0.4163909 ]],\n",
       "\n",
       "        [[-0.05097127],\n",
       "         [ 0.71011138],\n",
       "         [-0.41616964],\n",
       "         [ 0.24575305],\n",
       "         [ 0.03409934]],\n",
       "\n",
       "        [[ 0.20124626],\n",
       "         [ 0.54376745],\n",
       "         [-0.43056178],\n",
       "         [-0.30729508],\n",
       "         [ 0.71793199]],\n",
       "\n",
       "        [[-0.30533576],\n",
       "         [ 0.59425211],\n",
       "         [-0.73013449],\n",
       "         [ 0.56492805],\n",
       "         [-0.0988152 ]],\n",
       "\n",
       "        [[ 0.68539381],\n",
       "         [-0.52618146],\n",
       "         [-0.1989162 ],\n",
       "         [-0.13546824],\n",
       "         [ 0.30661774]],\n",
       "\n",
       "        [[ 0.20124626],\n",
       "         [ 0.54376745],\n",
       "         [-0.43056178],\n",
       "         [-0.30729508],\n",
       "         [ 0.71793199]]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sequence_length = 7\n",
    "vocab_size = 128\n",
    "embedding_size = 5\n",
    "\n",
    "# emb captures embeddings for the entire vocabulary\n",
    "# Usually obtained through word2vec training using some corpus (example: news data)\n",
    "# But for this toy example, we are randomly generating them - [128 x 5] matrix\n",
    "# We are also assuming we have only 128 words in this vocab set\n",
    "emb = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"emb\")\n",
    "\n",
    "# Place holder to hold batch of sentences\n",
    "# In this example, sentence is limited to max 7 words\n",
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "\n",
    "# Look up word embeddings from emb for each sentence (pay attention to matrix shape)\n",
    "emb_input = tf.nn.embedding_lookup(emb, input_x)\n",
    "\n",
    "# Add one more dimension at end - channel, so that we can use conv2d later\n",
    "# conv2d operator requires input to be in [batch, height, width, channel]\n",
    "# in our example we have only channel (in case of images, we may have 3 channels)\n",
    "# Convert data from [batch, height, width] => [batch, height, width, channel]\n",
    "# Remember - we are just adding one dimension to matrix, it can only have one channel\n",
    "emb_input_expanded = tf.expand_dims(emb_input, -1)\n",
    "\n",
    "# Create 2 filters of each with heights [2, 3, 4]\n",
    "# Filter with height 2 will cover 2 consecutive words each time\n",
    "# width of filter is the embedding dimension size \n",
    "filter_sizes = [2, 3, 4] \n",
    "num_filters = 2\n",
    "\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "        # Initialize each filter's weights\n",
    "        # It is 3-dimesional filter, but depth is only 1 channel\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "\n",
    "        # Apply Filter on input [batch, seq_length, embedding_size, 1]\n",
    "        # Move one word at time. In images\n",
    "        conv = tf.nn.conv2d(emb_input_expanded, \n",
    "            W, strides=[1, 1, 1, 1], \n",
    "            padding=\"VALID\", name=\"conv\")\n",
    "\n",
    "        # Apply relu - max(0, output) as activation functiona \n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "        # Apply max pooling on result activation map\n",
    "        pooled = tf.nn.max_pool( h, ksize=[1, sequence_length - filter_size + 1, 1, 1], \n",
    "            strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "# Create session and initialize weight matrices\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Write graph definition to a file, so that tensorboard can read it ...\n",
    "writer = tf.summary.FileWriter(\"./cnn_text\", graph=tf.get_default_graph())\n",
    "\n",
    "# Input setup, each row is one sentence, each column represents one word\n",
    "# Pick each words embeddings from previously trained embeddings\n",
    "# Batch contains 4 sentences, each sentence is one training sample (add labels later)\n",
    "batch_x = [\n",
    "    [1, 4, 6, 8, 20, 2, 8], \n",
    "    [11, 14, 16, 18, 20, 12, 18],\n",
    "    [21, 24, 26, 28, 20, 22, 28],\n",
    "    [31, 34, 36, 38, 20, 32, 38],\n",
    "]\n",
    "\n",
    "sess.run(emb_input_expanded, feed_dict={input_x:batch_x})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71179408,  1.50984275, -0.14741264, -0.77934337], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of shape [4] => just one dimension (simple array of 4 elements)\n",
    "a = tf.Variable(tf.random_normal([4]))\n",
    "sess.run(a.initializer)\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71179408],\n",
       "       [ 1.50984275],\n",
       "       [-0.14741264],\n",
       "       [-0.77934337]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's expand it by adding one dimension at end (indicated by -1)\n",
    "# Converts into [4, 1]   => 4 rows, each row with one element\n",
    "sess.run(tf.expand_dims(a, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.expand_dims(a, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e1 = tf.Variable([1,2,3,4])\n",
    "sess.run(e1.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sess.run(e1)\n",
    "print(a.shape)\n",
    "b = sess.run(tf.expand_dims(e1, -1))\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

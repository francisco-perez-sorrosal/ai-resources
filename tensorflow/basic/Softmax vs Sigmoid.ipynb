{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax vs Sigmoid\n",
    "\n",
    "What is the difference between these 2 functions? Experiment it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "s = tf.Session()\n",
    "\n",
    "example_labels = [0.0, 0.0, 1.0, 0.0]\n",
    "\n",
    "sm = s.run(tf.nn.softmax([1.0, 2.0, 3.0, 4.0]))\n",
    "sm_ce = s.run(tf.nn.softmax_cross_entropy_with_logits(logits=[1.0, 2.0, 3.0, 4.0], labels=example_labels))\n",
    "sig = s.run(tf.nn.sigmoid_cross_entropy_with_logits(logits=[1.0, 2.0, 3.0, 4.0], labels=example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between these 3 functions?\n",
    "\n",
    "1) `tf.nn.softmax` produces just the result of applying the softmax function to an input tensor. The softmax \"squishes\" the inputs so that sum(input) = 1; it's a way of normalizing. The outputs of softmax can be interpreted as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0320586   0.08714432  0.23688281  0.64391422]\n"
     ]
    }
   ],
   "source": [
    "print(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) `tf.nn.softmax_cross_entropy_with_logits` Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both. \n",
    "The cross entropy is computing after applying the softmax function (but it does it all together in a more mathematically careful way). It's similar to the result of:\n",
    "`sm = tf.nn.softmax(x)`\n",
    "and\n",
    "`ce = cross_entropy(sm)`\n",
    "\n",
    "See also [this](http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44019\n",
      "[ 0.0320586   0.08714432  0.23688281  0.64391422]\n",
      "1.44018975034\n",
      "1.44035237283\n"
     ]
    }
   ],
   "source": [
    "print(sm_ce)\n",
    "\n",
    "# Let's demonstrate that tf.nn.softmax_cross_entropy_with_logits is equivalent to sm = tf.nn.softmax(x) and ce = cross_entropy(sm)\n",
    "\n",
    "sm = tf.nn.softmax([1.0, 2.0, 3.0, 4.0])\n",
    "sm_vals = s.run(sm)\n",
    "print(sm_vals)\n",
    "# Simple way to calculate the cross entropy for the example above, as the only predicted label has index 2\n",
    "import math\n",
    "print(-(math.log(sm_vals[0]) * example_labels[0] +\n",
    "        math.log(sm_vals[1]) * example_labels[1] + \n",
    "        math.log(sm_vals[2]) * example_labels[2] + # Only this is taking into account as the other example_label indexes are 0\n",
    "        math.log(sm_vals[3]) * example_labels[3]))\n",
    "\n",
    "\n",
    "# The sklearn way... Requires a weird epsilon\n",
    "from sklearn.metrics import log_loss # this is the equivalent of cross entropy in sklearn\n",
    "ce = log_loss(example_labels, sm_vals, eps=0.845)\n",
    "print(ce)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) `tf.nn.sigmoid_cross_entropy_with_logits` Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.31326175  2.12692809  0.04858735  4.01814985]\n"
     ]
    }
   ],
   "source": [
    "print(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

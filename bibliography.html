<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Bibliography - AI Resources</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="history.html"><strong aria-hidden="true">2.</strong> Timeline/History</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">3.</strong> Applications</a></li><li class="chapter-item expanded "><a href="multidisciplinary_approach.html"><strong aria-hidden="true">4.</strong> A Multidisciplinary Approach</a></li><li class="chapter-item expanded "><a href="approaches.html"><strong aria-hidden="true">5.</strong> Approaches</a></li><li class="chapter-item expanded "><a href="classical_ml.html"><strong aria-hidden="true">6.</strong> &quot;Classical&quot; Machine Learning</a></li><li class="chapter-item expanded "><a href="topics.html"><strong aria-hidden="true">7.</strong> Advanced Machine and Deep Learning Topics</a></li><li class="chapter-item expanded "><a href="nlp.html"><strong aria-hidden="true">8.</strong> NLP</a></li><li class="chapter-item expanded "><a href="transformers.html"><strong aria-hidden="true">9.</strong> Transformers</a></li><li class="chapter-item expanded "><a href="genai.html"><strong aria-hidden="true">10.</strong> Generative AI</a></li><li class="chapter-item expanded "><a href="agents.html"><strong aria-hidden="true">11.</strong> Agents</a></li><li class="chapter-item expanded "><a href="agi.html"><strong aria-hidden="true">12.</strong> AGI</a></li><li class="chapter-item expanded "><a href="system_design.html"><strong aria-hidden="true">13.</strong> System Design</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">14.</strong> To Production</a></li><li class="chapter-item expanded "><a href="tools_and_frameworks.html"><strong aria-hidden="true">15.</strong> Tools and Frameworks</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">16.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="vocabulary.html"><strong aria-hidden="true">17.</strong> Vocabulary</a></li><li class="chapter-item expanded "><a href="people.html"><strong aria-hidden="true">18.</strong> People</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">19.</strong> Books and Resources</a></li><li class="chapter-item expanded affix "><a href="bibliography.html" class="active">Bibliography</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#bibliography" id="bibliography">Bibliography</a></h1>
<script type="text/javascript">
function defaultCopyTextToClipboard(text) {
    var textArea = document.createElement("textarea");
    textArea.value = text;

    // Avoid scrolling to bottom
    textArea.style.top = "0";
    textArea.style.left = "0";
    textArea.style.position = "fixed";

    document.body.appendChild(textArea);
    textArea.focus();
    textArea.select();

    try {
        var ok = document.execCommand('copy');
        var msg = ok ? 'was ok' : 'failed';
        console.log('Backing copy: Text copy was ' + msg);
    } catch (err) {
        console.error('Backing copy: Unable to copy text', err);
    }

    document.body.removeChild(textArea);
}

function copyToClipboard(text) {
    if (!navigator.clipboard) {
        defaultCopyTextToClipboard(text);
        return;
    }
    navigator.clipboard.writeText(text).then(function() {
        console.log('Text copied to clipboard');
    }, function(err) {
        console.error('Error copying text: ', err);
    });
}

</script>
<style></style>
<div class="bib_div">
<details data-key="abuelsaad_agent-e_2024" class=ref>
<summary class=citation>
<a id="abuelsaad_agent-e_2024">[abuelsaad_agent-e_2024]</a> - Abuelsaad, Tamer and Akkil, Deepak and Dey, Prasenjit and Jagmohan, Ashish and Vempaty, Aditya and Kokku, Ravi - <a href="http://arxiv.org/abs/2407.13032" target="_blank"><cite>Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite abuelsaad_agent-e_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract" id="summaryabstract">Summary/Abstract</a></h1>
<div>{AI} Agents are changing the way work gets done, both in consumer and enterprise domains. However, the design patterns and architectures to build highly capable agents or multi-agent systems are still developing, and the understanding of the implication of various design choices and algorithms is still evolving. In this paper, we present our work on building a novel web agent, Agent-E {\textbackslash}footnote\{Our code is available at {\textbackslash}url\{https://github.com/{EmergenceAI}/Agent-E\}\}. Agent-E introduces numerous architectural improvements over prior state-of-the-art web agents such as hierarchical architecture, flexible {DOM} distillation and denoising method, and the concept of {\textbackslash}textit\{change observation\} to guide the agent towards more accurate performance. We first present the results of an evaluation of Agent-E on {WebVoyager} benchmark dataset and show that Agent-E beats other {SOTA} text and multi-modal web agents on this benchmark in most categories by 10-30{\textbackslash}\%. We then synthesize our learnings from the development of Agent-E into general design principles for developing agentic systems. These include the use of domain-specific primitive skills, the importance of distillation and de-noising of environmental observations, the advantages of a hierarchical architecture, and the role of agentic self-improvement to enhance agent efficiency and efficacy as the agent gathers experience.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="agarwal_neural_2020" class=ref>
<summary class=citation>
<a id="agarwal_neural_2020">[agarwal_neural_2020]</a> - Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E. - <a href="http://arxiv.org/abs/2004.13912" target="_blank"><cite>Neural Additive Models: Interpretable Machine Learning with Neural Nets</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite agarwal_neural_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-1" id="summaryabstract-1">Summary/Abstract</a></h1>
<div>Deep neural networks ({DNNs}) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models ({NAMs}) which combine some of the expressivity of {DNNs} with the inherent intelligibility of generalized additive models. {NAMs} learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that {NAMs} are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="akama_elements_2015" class=ref>
<summary class=citation>
<a id="akama_elements_2015">[akama_elements_2015]</a> - Akama, Seiki - <a href="http://link.springer.com/10.1007/978-3-319-08284-4" target="_blank"><cite>Elements of Quantum Computing</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite akama_elements_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-2" id="summaryabstract-2">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="al-rfou_character-level_2018" class=ref>
<summary class=citation>
<a id="al-rfou_character-level_2018">[al-rfou_character-level_2018]</a> - Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion - <a href="http://arxiv.org/abs/1808.04444" target="_blank"><cite>Character-Level Language Modeling with Deeper Self-Attention</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite al-rfou_character-level_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-3" id="summaryabstract-3">Summary/Abstract</a></h1>
<div>{LSTMs} and other {RNN} variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms {RNN} variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="alyafeai_survey_2020" class=ref>
<summary class=citation>
<a id="alyafeai_survey_2020">[alyafeai_survey_2020]</a> - Alyafeai, Zaid and {AlShaibani}, Maged Saeed and Ahmad, Irfan - <a href="http://arxiv.org/abs/2007.04239" target="_blank"><cite>A Survey on Transfer Learning in Natural Language Processing</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite alyafeai_survey_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-4" id="summaryabstract-4">Summary/Abstract</a></h1>
<div>Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging {NLP} tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of {NLP}. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="arute_quantum_2019" class=ref>
<summary class=citation>
<a id="arute_quantum_2019">[arute_quantum_2019]</a> - Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G. S. L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandrà, Salvatore and {McClean}, Jarrod R. and {McEwen}, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M. - <a href="http://www.nature.com/articles/s41586-019-1666-5" target="_blank"><cite>Quantum supremacy using a programmable superconducting processor</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite arute_quantum_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-5" id="summaryabstract-5">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="austin_structured_2023" class=ref>
<summary class=citation>
<a id="austin_structured_2023">[austin_structured_2023]</a> - Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and Berg, Rianne van den - <a href="http://arxiv.org/abs/2107.03006" target="_blank"><cite>Structured Denoising Diffusion Models in Discrete State-Spaces</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite austin_structured_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-6" id="summaryabstract-6">Summary/Abstract</a></h1>
<div>Denoising diffusion probabilistic models ({DDPMs}) [19] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusionlike generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [20], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on {LM}1B. On the image dataset {CIFAR}-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space {DDPM} model.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="austin_structured_2023-1" class=ref>
<summary class=citation>
<a id="austin_structured_2023-1">[austin_structured_2023-1]</a> - Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and Berg, Rianne van den - <a href="http://arxiv.org/abs/2107.03006" target="_blank"><cite>Structured Denoising Diffusion Models in Discrete State-Spaces</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite austin_structured_2023-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-7" id="summaryabstract-7">Summary/Abstract</a></h1>
<div>Denoising diffusion probabilistic models ({DDPMs}) [19] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusionlike generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [20], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on {LM}1B. On the image dataset {CIFAR}-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space {DDPM} model.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ba_layer_2016" class=ref>
<summary class=citation>
<a id="ba_layer_2016">[ba_layer_2016]</a> - Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E. - <a href="http://arxiv.org/abs/1607.06450" target="_blank"><cite>Layer Normalization</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite ba_layer_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-8" id="summaryabstract-8">Summary/Abstract</a></h1>
<div>Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="baevski_data2vec_nodate" class=ref>
<summary class=citation>
<a id="baevski_data2vec_nodate">[baevski_data2vec_nodate]</a> - Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael - <cite>data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite baevski_data2vec_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-9" id="summaryabstract-9">Summary/Abstract</a></h1>
<div>While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, {NLP} or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="barrett_seven_2021" class=ref>
<summary class=citation>
<a id="barrett_seven_2021">[barrett_seven_2021]</a> - Barrett, Lisa - <cite>Seven and a Half Lessons About the Brain</cite>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite barrett_seven_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-10" id="summaryabstract-10">Summary/Abstract</a></h1>
<div>From the author of How Emotions Are Made, a myth-busting primer on the brain in the tradition of Seven Brief Lessons on Physics and Astrophysics for People in a Hurry Have you ever wondered why you have a brain? Let renowned neuroscientist Lisa Feldman Barrett demystify that big gray blob between your ears. In seven short essays (plus a bite-size story about how brains evolved), this slim, entertaining, and accessible collection reveals mind-expanding lessons from the front lines of neuroscience research. You’ll learn where brains came from, how they’re structured (and why it matters), and how yours works in tandem with other brains to create everything you experience. Along the way, you’ll also learn to dismiss popular myths such as the idea of a “lizard brain” and the alleged battle between thoughts and emotions—or between nature and nurture—to determine your behavior.   Sure to intrigue casual readers and scientific veterans alike, Seven and a Half Lessons About the Brain is full of surprises, humor, and important implications for human nature—a gift of a book that you will want to savor again and again.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="bengio_neural_2003" class=ref>
<summary class=citation>
<a id="bengio_neural_2003">[bengio_neural_2003]</a> - Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian - <cite>A neural probabilistic language model</cite>. - 2003. -
<button onclick="copyToClipboard('\{\{ #cite bengio_neural_2003 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-11" id="summaryabstract-11">Summary/Abstract</a></h1>
<div>A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="besold_neural-symbolic_2017" class=ref>
<summary class=citation>
<a id="besold_neural-symbolic_2017">[besold_neural-symbolic_2017]</a> - Besold, Tarek R. and Garcez, Artur d&#x27;Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson - <a href="http://arxiv.org/abs/1711.03902" target="_blank"><cite>Neural-Symbolic Learning and Reasoning: A Survey and Interpretation</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite besold_neural-symbolic_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-12" id="summaryabstract-12">Summary/Abstract</a></h1>
<div>The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="biesialska_continual_2020" class=ref>
<summary class=citation>
<a id="biesialska_continual_2020">[biesialska_continual_2020]</a> - Biesialska, Magdalena and Biesialska, Katarzyna and Costa-jussà, Marta R. - <a href="http://arxiv.org/abs/2012.09823" target="_blank"><cite>Continual Lifelong Learning in Natural Language Processing: A Survey</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite biesialska_continual_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-13" id="summaryabstract-13">Summary/Abstract</a></h1>
<div>Continual learning ({CL}) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, {CL} is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of {CL} through the lens of various {NLP} tasks. Our survey discusses major challenges in {CL} and current methods applied in neural network models. We also provide a critical review of the existing {CL} evaluation methods and datasets in {NLP}. Finally, we present our outlook on future research directions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="blank_quantum_2020" class=ref>
<summary class=citation>
<a id="blank_quantum_2020">[blank_quantum_2020]</a> - Blank, Carsten and Park, Daniel K. and Rhee, June-Koo Kevin and Petruccione, Francesco - <a href="https://www.nature.com/articles/s41534-020-0272-6" target="_blank"><cite>Quantum classifier with tailored quantum kernel</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite blank_quantum_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-14" id="summaryabstract-14">Summary/Abstract</a></h1>
<div>Kernel methods have a wide spectrum of applications in machine learning. Recently, a link between quantum computing and kernel theory has been formally established, opening up opportunities for quantum techniques to enhance various existing machine-learning methods. We present a distance-based quantum classifier whose kernel is based on the quantum state fidelity between training and test data. The quantum kernel can be tailored systematically with a quantum circuit to raise the kernel to an arbitrary power and to assign arbitrary weights to each training data. Given a specific input state, our protocol calculates the weighted power sum of fidelities of quantum data in quantum parallel via a swap-test circuit followed by two single-qubit measurements, requiring only a constant number of repetitions regardless of the number of data. We also show that our classifier is equivalent to measuring the expectation value of a Helstrom operator, from which the well-known optimal quantum state discrimination can be derived. We demonstrate the performance of our classifier via classical simulations with a realistic noise model and proof-of-principle experiments using the {IBM} quantum cloud platform.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="bommasani_opportunities_2021" class=ref>
<summary class=citation>
<a id="bommasani_opportunities_2021">[bommasani_opportunities_2021]</a> - Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Kohd, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy - <a href="http://arxiv.org/abs/2108.07258" target="_blank"><cite>On the Opportunities and Risks of Foundation Models</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite bommasani_opportunities_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-15" id="summaryabstract-15">Summary/Abstract</a></h1>
<div>{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="bowman_eight_2023" class=ref>
<summary class=citation>
<a id="bowman_eight_2023">[bowman_eight_2023]</a> - Bowman, Samuel R. - <a href="http://arxiv.org/abs/2304.00612" target="_blank"><cite>Eight Things to Know about Large Language Models</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite bowman_eight_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-16" id="summaryabstract-16">Summary/Abstract</a></h1>
<div>The widespread public deployment of large language models ({LLMs}) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. {LLMs} predictably get more capable with increasing investment, even without targeted innovation. 2. Many important {LLM} behaviors emerge unpredictably as a byproduct of increasing investment. 3. {LLMs} often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of {LLMs}. 5. Experts are not yet able to interpret the inner workings of {LLMs}. 6. Human performance on a task isn&#x27;t an upper bound on {LLM} performance. 7. {LLMs} need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with {LLMs} are often misleading.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="bowman_large_2015" class=ref>
<summary class=citation>
<a id="bowman_large_2015">[bowman_large_2015]</a> - Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D. - <a href="http://aclweb.org/anthology/D15-1075" target="_blank"><cite>A large annotated corpus for learning natural language inference</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite bowman_large_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-17" id="summaryabstract-17">Summary/Abstract</a></h1>
<div>Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classiﬁers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the ﬁrst time.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brittain_prioritized_nodate" class=ref>
<summary class=citation>
<a id="brittain_prioritized_nodate">[brittain_prioritized_nodate]</a> - Brittain, Marc and Bertram, Josh and Yang, Xuxi and Wei, Peng - <cite>Prioritized Sequence Experience Replay</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite brittain_prioritized_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-18" id="summaryabstract-18">Summary/Abstract</a></h1>
<div>Experience replay is widely used in deep reinforcement learning algorithms and allows agents to remember and learn from experiences from the past. In an effort to learn more efﬁciently, researchers proposed prioritized experience replay ({PER}) which samples important transitions more frequently. In this paper, we propose Prioritized Sequence Experience Replay ({PSER}) a framework for prioritizing sequences of experience in an attempt to both learn more efﬁciently and to obtain better performance. We compare the performance of {PER} and {PSER} sampling techniques in a tabular Q-learning environment and in {DQN} on the Atari 2600 benchmark. We prove theoretically that {PSER} is guaranteed to converge faster than {PER} and empirically show {PSER} substantially improves upon {PER}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="broughton_tensorflow_2020" class=ref>
<summary class=citation>
<a id="broughton_tensorflow_2020">[broughton_tensorflow_2020]</a> - Broughton, Michael and Verdon, Guillaume and {McCourt}, Trevor and Martinez, Antonio J. and Yoo, Jae Hyeon and Isakov, Sergei V. and Massey, Philip and Niu, Murphy Yuezhen and Halavati, Ramin and Peters, Evan and Leib, Martin and Skolik, Andrea and Streif, Michael and Von Dollen, David and {McClean}, Jarrod R. and Boixo, Sergio and Bacon, Dave and Ho, Alan K. and Neven, Hartmut and Mohseni, Masoud - <a href="http://arxiv.org/abs/2003.02989" target="_blank"><cite>{TensorFlow} Quantum: A Software Framework for Quantum Machine Learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite broughton_tensorflow_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-19" id="summaryabstract-19">Summary/Abstract</a></h1>
<div>We introduce {TensorFlow} Quantum ({TFQ}), an open source library for the rapid prototyping of hybrid quantum-classical models for classical or quantum data. This framework offers high-level abstractions for the design and training of both discriminative and generative quantum models under {TensorFlow} and supports high-performance quantum circuit simulators. We provide an overview of the software architecture and building blocks through several examples and review the theory of hybrid quantum-classical neural networks. We illustrate {TFQ} functionalities via several basic applications including supervised learning for quantum classification, quantum control, and quantum approximate optimization. Moreover, we demonstrate how one can apply {TFQ} to tackle advanced quantum learning tasks including meta-learning, Hamiltonian learning, and sampling thermal states. We hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brown_language_2020" class=ref>
<summary class=citation>
<a id="brown_language_2020">[brown_language_2020]</a> - Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario - <a href="http://arxiv.org/abs/2005.14165" target="_blank"><cite>Language Models are Few-Shot Learners</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite brown_language_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-20" id="summaryabstract-20">Summary/Abstract</a></h1>
<div>Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3&#x27;s few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brunner_entity_2020" class=ref>
<summary class=citation>
<a id="brunner_entity_2020">[brunner_entity_2020]</a> - Brunner, Ursin and Stockinger, Kurt - <a href="https://openproceedings.org/2020/conf/edbt/paper_205.pdf" target="_blank"><cite>Entity Matching with Transformer Architectures - A Step Forward in Data Integration</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite brunner_entity_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-21" id="summaryabstract-21">Summary/Abstract</a></h1>
<div>Transformer architectures have proven to be very effective and provide state-of-the-art results in many natural language tasks. The attention-based architecture in combination with pre-training on large amounts of text lead to the recent breakthrough and a variety of slightly different implementations. In this paper we analyze how well four of the most recent attention-based transformer architectures ({BERT}[6], {XLNet}[33], {RoBERTa}[17] and {DistilBERT} [23]) perform on the task of entity matching - a crucial part of data integration. Entity matching ({EM}) is the task of finding data instances that refer to the same real-world entity. It is a challenging task if the data instances consist of long textual data or if the data instances are dirty due to misplaced values.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brunner_identifiability_2020" class=ref>
<summary class=citation>
<a id="brunner_identifiability_2020">[brunner_identifiability_2020]</a> - Brunner, Gino and Liu, Yang and Pascual, Damián and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger - <a href="http://arxiv.org/abs/1908.04211" target="_blank"><cite>On Identifiability in Transformers</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite brunner_identifiability_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-22" id="summaryabstract-22">Summary/Abstract</a></h1>
<div>In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="bucilua_model_2006" class=ref>
<summary class=citation>
<a id="bucilua_model_2006">[bucilua_model_2006]</a> - Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru - <a href="https://doi.org/10.1145/1150402.1150464" target="_blank"><cite>Model compression</cite></a>. - 2006. -
<button onclick="copyToClipboard('\{\{ #cite bucilua_model_2006 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-23" id="summaryabstract-23">Summary/Abstract</a></h1>
<div>Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. {PDAs}), and where computational power is limited (e.g. hea-ring aids). We present a method for compressing large, complex ensembles into smaller, faster models, usually without significant loss in performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="cappelletti_polyadic_2020" class=ref>
<summary class=citation>
<a id="cappelletti_polyadic_2020">[cappelletti_polyadic_2020]</a> - Cappelletti, William and Erbanni, Rebecca and Keller, Joaquín - <a href="http://arxiv.org/abs/2007.14044" target="_blank"><cite>Polyadic Quantum Classifier</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite cappelletti_polyadic_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-24" id="summaryabstract-24">Summary/Abstract</a></h1>
<div>We introduce here a supervised quantum machine learning algorithm for multi-class classification on {NISQ} architectures. A parametric quantum circuit is trained to output a specific bit string corresponding to the class of the input datapoint. We train and test it on an {IBMq} 5-qubit quantum computer and the algorithm shows good accuracy –compared to a classical machine learning model– for ternary classification of the Iris dataset and an extension of the {XOR} problem. Furthermore, we evaluate with simulations how the algorithm fares for a binary and a quaternary classification on resp. a known binary dataset and a synthetic dataset.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="caruana_multitask_1997" class=ref>
<summary class=citation>
<a id="caruana_multitask_1997">[caruana_multitask_1997]</a> - Caruana, Rich - <a href="http://reports-archive.adm.cs.cmu.edu/anon/1997/CMU-CS-97-203.pdf" target="_blank"><cite>Multitask Learning</cite></a>. - 1997. -
<button onclick="copyToClipboard('\{\{ #cite caruana_multitask_1997 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-25" id="summaryabstract-25">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="chen_universal_2023" class=ref>
<summary class=citation>
<a id="chen_universal_2023">[chen_universal_2023]</a> - Chen, Xinyun and Aksitov, Renat and Alon, Uri and Ren, Jie and Xiao, Kefan and Yin, Pengcheng and Prakash, Sushant and Sutton, Charles and Wang, Xuezhi and Zhou, Denny - <a href="http://arxiv.org/abs/2311.17311" target="_blank"><cite>Universal Self-Consistency for Large Language Model Generation</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite chen_universal_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-26" id="summaryabstract-26">Summary/Abstract</a></h1>
<div>Self-consistency with chain-of-thought prompting ({CoT}) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models ({LLMs}). However, selfconsistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency ({USC}), which leverages {LLMs} themselves to select the most consistent answer among multiple candidates. We evaluate {USC} on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, {USC} effectively utilizes multiple samples and improves the performance. For mathematical reasoning, {USC} matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, {USC} also matches the execution-based voting performance on code generation.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="chen_teaching_2023" class=ref>
<summary class=citation>
<a id="chen_teaching_2023">[chen_teaching_2023]</a> - Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and Zhou, Denny - <a href="http://arxiv.org/abs/2304.05128" target="_blank"><cite>Teaching Large Language Models to Self-Debug</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite chen_teaching_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-27" id="summaryabstract-27">Summary/Abstract</a></h1>
<div>Large language models ({LLMs}) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose {SELF}-{DEBUGGING}, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that {SELF}-{DEBUGGING} can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. {SELF}-{DEBUGGING} achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-{SQL} generation, {TransCoder} for C++to-Python translation, and {MBPP} for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, {SELF}-{DEBUGGING} with code explanation consistently improves the baseline by 2 − 3\%, and improves the prediction accuracy on problems of the hardest level by 9\%. On {TransCoder} and {MBPP} where unit tests are available, {SELF}-{DEBUGGING} improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, {SELF}-{DEBUGGING} notably improves sample efficiency, and can match or outperform baseline models that generate more than 10× candidate programs.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="cheng_long_2016" class=ref>
<summary class=citation>
<a id="cheng_long_2016">[cheng_long_2016]</a> - Cheng, Jianpeng and Dong, Li and Lapata, Mirella - <a href="http://arxiv.org/abs/1601.06733" target="_blank"><cite>Long Short-Term Memory-Networks for Machine Reading</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite cheng_long_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-28" id="summaryabstract-28">Summary/Abstract</a></h1>
<div>In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="child_generating_2019" class=ref>
<summary class=citation>
<a id="child_generating_2019">[child_generating_2019]</a> - Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya - <a href="http://arxiv.org/abs/1904.10509" target="_blank"><cite>Generating Long Sequences with Sparse Transformers</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite child_generating_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-29" id="summaryabstract-29">Summary/Abstract</a></h1>
<div>Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n {\textbackslash}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, {CIFAR}-10, and {ImageNet}-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="clark_whatever_2013" class=ref>
<summary class=citation>
<a id="clark_whatever_2013">[clark_whatever_2013]</a> - Clark, Andy - <a href="https://www.cambridge.org/core/product/identifier/S0140525X12000477/type/journal_article" target="_blank"><cite>Whatever next? Predictive brains, situated agents, and the future of cognitive science</cite></a>. - 2013. -
<button onclick="copyToClipboard('\{\{ #cite clark_whatever_2013 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-30" id="summaryabstract-30">Summary/Abstract</a></h1>
<div>Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a uniﬁed science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="clark_electra_2019" class=ref>
<summary class=citation>
<a id="clark_electra_2019">[clark_electra_2019]</a> - Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D. - <a href="https://openreview.net/forum?id&#x3D;r1xMH1BtvB" target="_blank"><cite>{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite clark_electra_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-31" id="summaryabstract-31">Summary/Abstract</a></h1>
<div>Masked language modeling ({MLM}) pre-training methods such as {BERT} corrupt the input by replacing some tokens with [{MASK}] and then train a model to reconstruct the original tokens. While they produce...</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="clark_what_2019" class=ref>
<summary class=citation>
<a id="clark_what_2019">[clark_what_2019]</a> - Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D. - <a href="http://arxiv.org/abs/1906.04341" target="_blank"><cite>What Does {BERT} Look At? An Analysis of {BERT}&#x27;s Attention</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite clark_what_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-32" id="summaryabstract-32">Summary/Abstract</a></h1>
<div>Large pre-trained neural networks such as {BERT} have had great recent success in {NLP}, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to {BERT}. {BERT}&#x27;s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in {BERT}&#x27;s attention.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="dai_transformer-xl:_2019" class=ref>
<summary class=citation>
<a id="dai_transformer-xl:_2019">[dai_transformer-xl:_2019]</a> - Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan - <a href="http://arxiv.org/abs/1901.02860" target="_blank"><cite>Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite dai_transformer-xl:_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-33" id="summaryabstract-33">Summary/Abstract</a></h1>
<div>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-{XL} that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-{XL} learns dependency that is 80\% longer than {RNNs} and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on {WikiText}-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on {WikiText}-103, Transformer-{XL} manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and {PyTorch}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="dathathri_plug_2020" class=ref>
<summary class=citation>
<a id="dathathri_plug_2020">[dathathri_plug_2020]</a> - Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne - <a href="http://arxiv.org/abs/1912.02164" target="_blank"><cite>Plug and Play Language Models: A Simple Approach to Controlled Text Generation</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite dathathri_plug_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-34" id="summaryabstract-34">Summary/Abstract</a></h1>
<div>Large transformer-based language models ({LMs}) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model ({PPLM}) for controllable language generation, which combines a pretrained {LM} with one or more simple attribute classifiers that guide text generation without any further training of the {LM}. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the {LM}. Sampling entails a forward and backward pass in which gradients from the attribute model push the {LM}&#x27;s hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. {PPLMs} are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="de_lange_continual_2020" class=ref>
<summary class=citation>
<a id="de_lange_continual_2020">[de_lange_continual_2020]</a> - De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne - <a href="http://arxiv.org/abs/1909.08383" target="_blank"><cite>A continual learning survey: Defying forgetting in classification tasks</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite de_lange_continual_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-35" id="summaryabstract-35">Summary/Abstract</a></h1>
<div>Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced {iNaturalist} and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="devlin_bert_2019" class=ref>
<summary class=citation>
<a id="devlin_bert_2019">[devlin_bert_2019]</a> - Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina - <a href="http://arxiv.org/abs/1810.04805" target="_blank"><cite>{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite devlin_bert_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-36" id="summaryabstract-36">Summary/Abstract</a></h1>
<div>We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="doersch_tutorial_2016" class=ref>
<summary class=citation>
<a id="doersch_tutorial_2016">[doersch_tutorial_2016]</a> - Doersch, Carl - <a href="http://arxiv.org/abs/1606.05908" target="_blank"><cite>Tutorial on Variational Autoencoders</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite doersch_tutorial_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-37" id="summaryabstract-37">Summary/Abstract</a></h1>
<div>In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="domingos_master_2015" class=ref>
<summary class=citation>
<a id="domingos_master_2015">[domingos_master_2015]</a> - Domingos, Pedro - <cite>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</cite>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite domingos_master_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-38" id="summaryabstract-38">Summary/Abstract</a></h1>
<div>A thought-provoking and wide-ranging exploration of machine learning and the race to build computer intelligences as flexible as our {ownIn} the world&#x27;s top research labs and universities, the race is on to invent the ultimate learning algorithm: one capable of discovering any knowledge from data, and doing anything we want, before we even ask. In The Master Algorithm, Pedro Domingos lifts the veil to give us a peek inside the learning machines that power Google, Amazon, and your smartphone. He assembles a blueprint for the future universal learner--the Master Algorithm--and discusses what it will mean for business, science, and society. If data-ism is today&#x27;s philosophy, this book is its bible.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="dubey_llama_2024" class=ref>
<summary class=citation>
<a id="dubey_llama_2024">[dubey_llama_2024]</a> - Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and {McConnell}, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and {AlBadawy}, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and {McPhie}, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and {DeVito}, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei - <a href="http://arxiv.org/abs/2407.21783" target="_blank"><cite>The Llama 3 Herd of Models</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite dubey_llama_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-39" id="summaryabstract-39">Summary/Abstract</a></h1>
<div>Modern artificial intelligence ({AI}) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as {GPT}-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="edge_local_2024" class=ref>
<summary class=citation>
<a id="edge_local_2024">[edge_local_2024]</a> - Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan - <a href="http://arxiv.org/abs/2404.16130" target="_blank"><cite>From Local to Global: A Graph {RAG} Approach to Query-Focused Summarization</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite edge_local_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-40" id="summaryabstract-40">Summary/Abstract</a></h1>
<div>The use of retrieval-augmented generation ({RAG}) to retrieve relevant information from an external knowledge source enables large language models ({LLMs}) to answer questions over private and/or previously unseen document collections. However, {RAG} fails on global questions directed at an entire text corpus, such as What are the main themes in the dataset?, since this is inherently a query-focused summarization ({QFS}) task, rather than an explicit retrieval task. Prior {QFS} methods, meanwhile, fail to scale to the quantities of text indexed by typical {RAG} systems. To combine the strengths of these contrasting methods, we propose a Graph {RAG} approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an {LLM} to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph {RAG} leads to substantial improvements over a na{\textbackslash}ive {RAG} baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph {RAG} approaches is forthcoming at https://aka.ms/graphrag.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="elias_what_2024" class=ref>
<summary class=citation>
<a id="elias_what_2024">[elias_what_2024]</a> - Elias, Greggory - <a href="https://skimai.com/what-is-autogen-our-full-guide-to-the-autogen-multi-agent-platform/" target="_blank"><cite>What is {AutoGen}? Our Full Guide to the Autogen Multi-Agent Platform</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite elias_what_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-41" id="summaryabstract-41">Summary/Abstract</a></h1>
<div>Discover the transformative potential of {AI} agents and multi-agent systems in business operations. Learn about Microsoft {AutoGen}, a framework that leverages {AI} agents and large language models to enhance flexibility, scalability, and problem-solving capabilities. Explore key features, real-world applications, and the unique advantages of {AutoGen} in {AI} development.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="fatemi_talk_2023" class=ref>
<summary class=citation>
<a id="fatemi_talk_2023">[fatemi_talk_2023]</a> - Fatemi, Bahare and Halcrow, Jonathan and Perozzi, Bryan - <a href="https://arxiv.org/abs/2310.04560v1" target="_blank"><cite>Talk like a Graph: Encoding Graphs for Large Language Models</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite fatemi_talk_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-42" id="summaryabstract-42">Summary/Abstract</a></h1>
<div>Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models ({LLMs}) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by {LLMs}. We show that {LLM} performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside {LLMs} by 4.8\% to 61.8\%, depending on the task.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="fedorenko_language_2024" class=ref>
<summary class=citation>
<a id="fedorenko_language_2024">[fedorenko_language_2024]</a> - Fedorenko, Evelina and Piantadosi, Steven T. and Gibson, Edward A. F. - <a href="https://www.nature.com/articles/s41586-024-07522-w" target="_blank"><cite>Language is primarily a tool for communication rather than thought</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite fedorenko_language_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-43" id="summaryabstract-43">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="fei-fei_one-shot_2006" class=ref>
<summary class=citation>
<a id="fei-fei_one-shot_2006">[fei-fei_one-shot_2006]</a> - Fei-Fei, Li and Fergus, R. and Perona, P. - <cite>One-shot learning of object categories</cite>. - 2006. -
<button onclick="copyToClipboard('\{\{ #cite fei-fei_one-shot_2006 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-44" id="summaryabstract-44">Summary/Abstract</a></h1>
<div>Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood ({ML}) and maximum a posteriori ({MAP}) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="fink_object_2004" class=ref>
<summary class=citation>
<a id="fink_object_2004">[fink_object_2004]</a> - Fink, Michael - <a href="https://proceedings.neurips.cc/paper/2004/hash/ef1e491a766ce3127556063d49bc2f98-Abstract.html" target="_blank"><cite>Object Classification from a Single Example Utilizing Class Relevance Metrics</cite></a>. - 2004. -
<button onclick="copyToClipboard('\{\{ #cite fink_object_2004 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-45" id="summaryabstract-45">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="friston_free-energy_2010" class=ref>
<summary class=citation>
<a id="friston_free-energy_2010">[friston_free-energy_2010]</a> - Friston, Karl - <a href="https://www.nature.com/articles/nrn2787" target="_blank"><cite>The free-energy principle: a unified brain theory?</cite></a>. - 2010. -
<button onclick="copyToClipboard('\{\{ #cite friston_free-energy_2010 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-46" id="summaryabstract-46">Summary/Abstract</a></h1>
<div>A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories — optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="friston_free_2006" class=ref>
<summary class=citation>
<a id="friston_free_2006">[friston_free_2006]</a> - Friston, Karl and Kilner, James and Harrison, Lee - <a href="https://linkinghub.elsevier.com/retrieve/pii/S092842570600060X" target="_blank"><cite>A free energy principle for the brain</cite></a>. - 2006. -
<button onclick="copyToClipboard('\{\{ #cite friston_free_2006 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-47" id="summaryabstract-47">Summary/Abstract</a></h1>
<div>By formulating Helmholtz’s ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="garcez_neural-symbolic_2019" class=ref>
<summary class=citation>
<a id="garcez_neural-symbolic_2019">[garcez_neural-symbolic_2019]</a> - Garcez, Artur d&#x27;Avila and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N. - <a href="http://arxiv.org/abs/1905.06088" target="_blank"><cite>Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite garcez_neural-symbolic_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-48" id="summaryabstract-48">Summary/Abstract</a></h1>
<div>Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of {AI} have been raised by influential thinkers. In spite of the recent impact of {AI}, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable {AI} systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable {AI} systems.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="gasparetto_survey_2022" class=ref>
<summary class=citation>
<a id="gasparetto_survey_2022">[gasparetto_survey_2022]</a> - Gasparetto, Andrea and Marcuzzo, Matteo and Zangari, Alessandro and Albarelli, Andrea - <a href="https://www.mdpi.com/2078-2489/13/2/83" target="_blank"><cite>A Survey on Text Classification Algorithms: From Text to Predictions</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite gasparetto_survey_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-49" id="summaryabstract-49">Summary/Abstract</a></h1>
<div>In recent years, the exponential growth of digital documents has been met by rapid progress in text classification techniques. Newly proposed machine learning algorithms leverage the latest advancements in deep learning methods, allowing for the automatic extraction of expressive features. The swift development of these methods has led to a plethora of strategies to encode natural language into machine-interpretable data. The latest language modelling algorithms are used in conjunction with ad hoc preprocessing procedures, of which the description is often omitted in favour of a more detailed explanation of the classification step. This paper offers a concise review of recent text classification models, with emphasis on the flow of data, from raw text to output labels. We highlight the differences between earlier methods and more recent, deep learning-based methods in both their functioning and in how they transform input data. To give a better perspective on the text classification landscape, we provide an overview of datasets for the English language, as well as supplying instructions for the synthesis of two new multilabel datasets, which we found to be particularly scarce in this setting. Finally, we provide an outline of new experimental results and discuss the open research challenges posed by deep learning-based language models.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="geiger_scaling_2020" class=ref>
<summary class=citation>
<a id="geiger_scaling_2020">[geiger_scaling_2020]</a> - Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d&#x27;Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu - <a href="http://arxiv.org/abs/1901.01608" target="_blank"><cite>Scaling description of generalization with number of parameters in deep learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite geiger_scaling_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-50" id="summaryabstract-50">Summary/Abstract</a></h1>
<div>Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N{\textasciicircum}\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \${\textbackslash}{\textbar}f\_\{N\}-{\textbackslash}bar\{f\}\_\{N\}{\textbackslash}{\textbar}{\textbackslash}sim N{\textasciicircum}\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \${\textbackslash}bar\{f\}\_\{N\}\$. These affect the generalization error \${\textbackslash}epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \${\textbackslash}epsilon\_\{{\textbackslash}infty\}\$ in a power-law fashion \${\textbackslash}sim N{\textasciicircum}\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N&#x3D;N{\textasciicircum}\{*\}\$. At this threshold, we argue that \${\textbackslash}{\textbar}f\_\{N\}{\textbackslash}{\textbar}\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N{\textasciicircum}\{*\}\$. Our results are confirmed by extensive empirical observations on the {MNIST} and {CIFAR} image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N{\textasciicircum}\{*\}\$, and averaging their outputs.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="goldberg_word2vec_2014" class=ref>
<summary class=citation>
<a id="goldberg_word2vec_2014">[goldberg_word2vec_2014]</a> - Goldberg, Yoav and Levy, Omer - <a href="http://arxiv.org/abs/1402.3722" target="_blank"><cite>word2vec Explained: deriving Mikolov et al.&#x27;s negative-sampling word-embedding method</cite></a>. - 2014. -
<button onclick="copyToClipboard('\{\{ #cite goldberg_word2vec_2014 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-51" id="summaryabstract-51">Summary/Abstract</a></h1>
<div>The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in Distributed Representations of Words and Phrases and their Compositionality by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="gong_hierarchical_2020" class=ref>
<summary class=citation>
<a id="gong_hierarchical_2020">[gong_hierarchical_2020]</a> - Gong, Jibing and Liu, Mingsheng and Ma, Hongyuan and Teng, Zhiyong and Teng, Qi and Zhang, Hekai and Du, Linfeng and Chen, Shuai and Bhuiyan, Md and Li, Jianhua - <cite>Hierarchical Graph Transformer Based Deep Learning Model for Large-Scale Multi-Label Text Classification</cite>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite gong_hierarchical_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-52" id="summaryabstract-52">Summary/Abstract</a></h1>
<div>Traditional methods of multi-label text classification, particularly deep learning, have achieved remarkable results. However, most of these methods use word2vec technology to represent sequential text information, while ignoring the logic and internal hierarchy of the text itself. Although these approaches can learn the hypothetical hierarchy and logic of the text, it is unexplained. In addition, the traditional approach treats labels as independent individuals and ignores the relationships between them, which not only does not reflect reality but also causes significant loss of semantic information. In this paper, we propose a novel Hierarchical Graph Transformer based deep learning model for large-scale multi-label text classification. We first model the text into a graph structure that can embody the different semantics of the text and the connections between them. We then use a multi-layer transformer structure with a multi-head attention mechanism at the word, sentence, and graph levels to fully capture the features of the text and observe the importance of the separate parts. Finally, we use the hierarchical relationship of the labels to generate the representation of the labels, and design a weighted loss function based on the semantic distances of the labels. Extensive experiments conducted on three benchmark datasets demonstrated that the proposed model can realistically capture the hierarchy and logic of text and improve performance compared with the state-of-the-art methods.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="goodfellow_generative_2014" class=ref>
<summary class=citation>
<a id="goodfellow_generative_2014">[goodfellow_generative_2014]</a> - Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua - <a href="http://arxiv.org/abs/1406.2661" target="_blank"><cite>Generative Adversarial Networks</cite></a>. - 2014. -
<button onclick="copyToClipboard('\{\{ #cite goodfellow_generative_2014 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-53" id="summaryabstract-53">Summary/Abstract</a></h1>
<div>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="goodfellow_deep_2016" class=ref>
<summary class=citation>
<a id="goodfellow_deep_2016">[goodfellow_deep_2016]</a> - Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron - <cite>Deep Learning</cite>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite goodfellow_deep_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-54" id="summaryabstract-54">Summary/Abstract</a></h1>
<div>An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="gupta_effective_2020" class=ref>
<summary class=citation>
<a id="gupta_effective_2020">[gupta_effective_2020]</a> - Gupta, Aakriti and Thadani, Kapil and O&#x27;Hare, Neil - <a href="https://www.aclweb.org/anthology/2020.coling-main.92" target="_blank"><cite>Effective Few-Shot Classification with Transfer Learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite gupta_effective_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-55" id="summaryabstract-55">Summary/Abstract</a></h1>
<div>Few-shot learning addresses the the problem of learning based on a small amount of training data. Although more well-studied in the domain of computer vision, recent work has adapted the Amazon Review Sentiment Classification ({ARSC}) text dataset for use in the few-shot setting. In this work, we use the {ARSC} dataset to study a simple application of transfer learning approaches to few-shot classification. We train a single binary classifier to learn all few-shot classes jointly by prefixing class identifiers to the input text. Given the text and class, the model then makes a binary prediction for that text/class pair. Our results show that this simple approach can outperform most published results on this dataset. Surprisingly, we also show that including domain information as part of the task definition only leads to a modest improvement in model accuracy, and zero-shot classification, without further fine-tuning on few-shot domains, performs equivalently to few-shot classification. These results suggest that the classes in the {ARSC} few-shot task, which are defined by the intersection of domain and rating, are actually very similar to each other, and that a more suitable dataset is needed for the study of few-shot text classification.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="guu_realm_2020" class=ref>
<summary class=citation>
<a id="guu_realm_2020">[guu_realm_2020]</a> - Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei - <a href="https://arxiv.org/abs/2002.08909v1" target="_blank"><cite>{REALM}: Retrieval-Augmented Language Model Pre-Training</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite guu_realm_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-56" id="summaryabstract-56">Summary/Abstract</a></h1>
<div>Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for {NLP} tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training ({REALM}) by fine-tuning on the challenging task of Open-domain Question Answering (Open-{QA}). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-{QA} benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="han_parameter-efficient_2024" class=ref>
<summary class=citation>
<a id="han_parameter-efficient_2024">[han_parameter-efficient_2024]</a> - Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian - <a href="http://arxiv.org/abs/2403.14608" target="_blank"><cite>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite han_parameter-efficient_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-57" id="summaryabstract-57">Summary/Abstract</a></h1>
<div>Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="hinton_distilling_2015" class=ref>
<summary class=citation>
<a id="hinton_distilling_2015">[hinton_distilling_2015]</a> - Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff - <a href="http://arxiv.org/abs/1503.02531" target="_blank"><cite>Distilling the Knowledge in a Neural Network</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite hinton_distilling_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-58" id="summaryabstract-58">Summary/Abstract</a></h1>
<div>A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="huang_large_2024" class=ref>
<summary class=citation>
<a id="huang_large_2024">[huang_large_2024]</a> - Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny - <a href="http://arxiv.org/abs/2310.01798" target="_blank"><cite>Large Language Models Cannot Self-Correct Reasoning Yet</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite huang_large_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-59" id="summaryabstract-59">Summary/Abstract</a></h1>
<div>Large Language Models ({LLMs}) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within {LLMs}, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an {LLM} attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that {LLMs} struggle to selfcorrect their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="huszar_quadratic_2017" class=ref>
<summary class=citation>
<a id="huszar_quadratic_2017">[huszar_quadratic_2017]</a> - Huszár, Ferenc - <a href="http://arxiv.org/abs/1712.03847" target="_blank"><cite>On Quadratic Penalties in Elastic Weight Consolidation</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite huszar_quadratic_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-60" id="summaryabstract-60">Summary/Abstract</a></h1>
<div>Elastic weight consolidation ({EWC}, Kirkpatrick et al, 2017) is a novel algorithm designed to safeguard against catastrophic forgetting in neural networks. {EWC} can be seen as an approximation to Laplace propagation (Eskin et al, 2004), and this view is consistent with the motivation given by Kirkpatrick et al (2017). In this note, I present an extended derivation that covers the case when there are more than two tasks. I show that the quadratic penalties in {EWC} are inconsistent with this derivation and might lead to double-counting data from earlier tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ioffe_batch_2015" class=ref>
<summary class=citation>
<a id="ioffe_batch_2015">[ioffe_batch_2015]</a> - Ioffe, Sergey and Szegedy, Christian - <a href="http://arxiv.org/abs/1502.03167" target="_blank"><cite>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite ioffe_batch_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-61" id="summaryabstract-61">Summary/Abstract</a></h1>
<div>Training Deep Neural Networks is complicated by the fact that the distribution of each layer&#x27;s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="jaderberg_quantum_2021" class=ref>
<summary class=citation>
<a id="jaderberg_quantum_2021">[jaderberg_quantum_2021]</a> - Jaderberg, Ben and Anderson, Lewis W. and Xie, Weidi and Albanie, Samuel and Kiffner, Martin and Jaksch, Dieter - <a href="http://arxiv.org/abs/2103.14653" target="_blank"><cite>Quantum Self-Supervised Learning</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite jaderberg_quantum_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-62" id="summaryabstract-62">Summary/Abstract</a></h1>
<div>The popularisation of neural networks has seen incredible advances in pattern recognition, driven by the supervised learning of human annotations. However, this approach is unsustainable in relation to the dramatically increasing size of real-world datasets. This has led to a resurgence in self-supervised learning, a paradigm whereby the model generates its own supervisory signal from the data. Here we propose a hybrid quantum-classical neural network architecture for contrastive self-supervised learning and test its effectiveness in proof-of-principle experiments. Interestingly, we observe a numerical advantage for the learning of visual representations using small-scale quantum neural networks over equivalently structured classical networks, even when the quantum circuits are sampled with only 100 shots. Furthermore, we apply our best quantum model to classify unseen images on the ibmq\_paris quantum computer and find that current noisy devices can already achieve equal accuracy to the equivalent classical model on downstream tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="jaegle_perceiver_2021" class=ref>
<summary class=citation>
<a id="jaegle_perceiver_2021">[jaegle_perceiver_2021]</a> - Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao - <a href="http://arxiv.org/abs/2103.03206" target="_blank"><cite>Perceiver: General Perception with Iterative Attention</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite jaegle_perceiver_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-63" id="summaryabstract-63">Summary/Abstract</a></h1>
<div>Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like {ConvNets}. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to {ResNet}-50 and {ViT} on {ImageNet} without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in {AudioSet}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kaplan_scaling_2020" class=ref>
<summary class=citation>
<a id="kaplan_scaling_2020">[kaplan_scaling_2020]</a> - Kaplan, Jared and {McCandlish}, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario - <a href="http://arxiv.org/abs/2001.08361" target="_blank"><cite>Scaling Laws for Neural Language Models</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite kaplan_scaling_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-64" id="summaryabstract-64">Summary/Abstract</a></h1>
<div>We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="karpathy_large-scale_nodate" class=ref>
<summary class=citation>
<a id="karpathy_large-scale_nodate">[karpathy_large-scale_nodate]</a> - Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li - <cite>Large-scale Video Classiﬁcation with Convolutional Neural Networks</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite karpathy_large-scale_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-65" id="summaryabstract-65">Summary/Abstract</a></h1>
<div>Convolutional Neural Networks ({CNNs}) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of {CNNs} on largescale video classiﬁcation using a new dataset of 1 million {YouTube} videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a {CNN} in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display signiﬁcant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the {UCF}101 Action Recognition dataset and observe signiﬁcant performance improvements compared to the {UCF}-101 baseline model (63.3\% up from 43.9\%).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="karpathy_visualizing_2015" class=ref>
<summary class=citation>
<a id="karpathy_visualizing_2015">[karpathy_visualizing_2015]</a> - Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li - <a href="http://arxiv.org/abs/1506.02078" target="_blank"><cite>Visualizing and Understanding Recurrent Networks</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite karpathy_visualizing_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-66" id="summaryabstract-66">Summary/Abstract</a></h1>
<div>Recurrent Neural Networks ({RNNs}), and specifically a variant with Long Short-Term Memory ({LSTM}), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while {LSTMs} provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the {LSTM} improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="keitakurita_building_2019" class=ref>
<summary class=citation>
<a id="keitakurita_building_2019">[keitakurita_building_2019]</a> - keitakurita, Author - <a href="https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/" target="_blank"><cite>Building the Transformer {XL} from Scratch</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite keitakurita_building_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-67" id="summaryabstract-67">Summary/Abstract</a></h1>
<div>With the release of {XLNet}, the Transformer {XL} is the new cool kid on the block. Although the Transformer {XL} is simple in concept, actually understanding the details is harder than might meet the ey…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="keitakurita_intuitive_2018" class=ref>
<summary class=citation>
<a id="keitakurita_intuitive_2018">[keitakurita_intuitive_2018]</a> - keitakurita, Author - <a href="http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/" target="_blank"><cite>An Intuitive Explanation of Why Batch Normalization Really Works (Normalization in Deep Learning Part 1)</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite keitakurita_intuitive_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-68" id="summaryabstract-68">Summary/Abstract</a></h1>
<div>Batch normalization is one of the reasons why deep learning has made such outstanding progress in recent years. Batch normalization enables the use of higher learning rates, greatly accelerating th…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kemker_fearnet_2018" class=ref>
<summary class=citation>
<a id="kemker_fearnet_2018">[kemker_fearnet_2018]</a> - Kemker, Ronald and Kanan, Christopher - <a href="https://openreview.net/forum?id&#x3D;SJ1Xmf-Rb" target="_blank"><cite>{FearNet}: Brain-Inspired Model for Incremental Learning</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite kemker_fearnet_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-69" id="summaryabstract-69">Summary/Abstract</a></h1>
<div>{FearNet} is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="keskar_ctrl_nodate" class=ref>
<summary class=citation>
<a id="keskar_ctrl_nodate">[keskar_ctrl_nodate]</a> - Keskar, Nitish Shirish and {McCann}, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard - <cite>{CTRL}: A {CONDITIONAL} {TRANSFORMER} {LANGUAGE} {MODEL} {FOR} {CONTROLLABLE} {GENERATION}</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite keskar_ctrl_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-70" id="summaryabstract-70">Summary/Abstract</a></h1>
<div>Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release {CTRL}, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-speciﬁc behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow {CTRL} to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of {CTRL} at https://github.com/salesforce/ctrl.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="khattab_colbert_2020" class=ref>
<summary class=citation>
<a id="khattab_colbert_2020">[khattab_colbert_2020]</a> - Khattab, Omar and Zaharia, Matei - <a href="http://arxiv.org/abs/2004.12832" target="_blank"><cite>{ColBERT}: Efficient and Effective Passage Search via Contextualized Late Interaction over {BERT}</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite khattab_colbert_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-71" id="summaryabstract-71">Summary/Abstract</a></h1>
<div>Recent progress in Natural Language Understanding ({NLU}) is driving fast-paced advances in Information Retrieval ({IR}), largely owed to fine-tuning deep language models ({LMs}) for document ranking. While remarkably effective, the ranking models based on these {LMs} increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present {ColBERT}, a novel ranking model that adapts deep {LMs} (in particular, {BERT}) for efficient retrieval. {ColBERT} introduces a late interaction architecture that independently encodes the query and the document using {BERT} and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, {ColBERT} can leverage the expressiveness of deep {LMs} while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, {ColBERT}&#x27;s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate {ColBERT} using two recent passage search datasets. Results show that {ColBERT}&#x27;s effectiveness is competitive with existing {BERT}-based models (and outperforms every non-{BERT} baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer {FLOPs} per query.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kim_llm_2024" class=ref>
<summary class=citation>
<a id="kim_llm_2024">[kim_llm_2024]</a> - Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael W. and Keutzer, Kurt and Gholami, Amir - <a href="http://arxiv.org/abs/2312.04511" target="_blank"><cite>An {LLM} Compiler for Parallel Function Calling</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite kim_llm_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-72" id="summaryabstract-72">Summary/Abstract</a></h1>
<div>The reasoning capabilities of the recent {LLMs} enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed {LLMs} to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce {LLMCompiler}, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, {LLMCompiler} enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. {LLMCompiler} automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked {LLMCompiler} on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to {\textasciitilde}9\% compared to {ReAct}. Our code is available at https://github.com/{SqueezeAILab}/{LLMCompiler}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kirkpatrick_overcoming_2017" class=ref>
<summary class=citation>
<a id="kirkpatrick_overcoming_2017">[kirkpatrick_overcoming_2017]</a> - Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia - <a href="http://arxiv.org/abs/1612.00796" target="_blank"><cite>Overcoming catastrophic forgetting in neural networks</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite kirkpatrick_overcoming_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-73" id="summaryabstract-73">Summary/Abstract</a></h1>
<div>The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the {MNIST} hand written digit dataset and by learning several Atari 2600 games sequentially.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kirkpatrick_optimization_1983" class=ref>
<summary class=citation>
<a id="kirkpatrick_optimization_1983">[kirkpatrick_optimization_1983]</a> - Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P. - <a href="https://science.sciencemag.org/content/220/4598/671" target="_blank"><cite>Optimization by Simulated Annealing</cite></a>. - 1983. -
<button onclick="copyToClipboard('\{\{ #cite kirkpatrick_optimization_1983 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-74" id="summaryabstract-74">Summary/Abstract</a></h1>
<div>There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kitaev_reformer_2020" class=ref>
<summary class=citation>
<a id="kitaev_reformer_2020">[kitaev_reformer_2020]</a> - Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm - <a href="http://arxiv.org/abs/2001.04451" target="_blank"><cite>Reformer: The Efficient Transformer</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite kitaev_reformer_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-75" id="summaryabstract-75">Summary/Abstract</a></h1>
<div>Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L{\textasciicircum}2\$) to O(\$L{\textbackslash}log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="klein_opennmt_2017" class=ref>
<summary class=citation>
<a id="klein_opennmt_2017">[klein_opennmt_2017]</a> - Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander - <a href="http://aclweb.org/anthology/P17-4012" target="_blank"><cite>{OpenNMT}: Open-Source Toolkit for Neural Machine Translation</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite klein_opennmt_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-76" id="summaryabstract-76">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kraska_case_2018" class=ref>
<summary class=citation>
<a id="kraska_case_2018">[kraska_case_2018]</a> - Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis - <a href="http://arxiv.org/abs/1712.01208" target="_blank"><cite>The Case for Learned Index Structures</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite kraska_case_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-77" id="summaryabstract-77">Summary/Abstract</a></h1>
<div>Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a {BitMap}-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70\% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kusupati_matryoshka_2024" class=ref>
<summary class=citation>
<a id="kusupati_matryoshka_2024">[kusupati_matryoshka_2024]</a> - Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali - <a href="http://arxiv.org/abs/2205.13147" target="_blank"><cite>Matryoshka Representation Learning</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite kusupati_matryoshka_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-78" id="summaryabstract-78">Summary/Abstract</a></h1>
<div>Learned representations are a central component in modern {ML} systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning ({MRL}) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. {MRL} minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. {MRL} learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14× smaller embedding size for {ImageNet}-1K classification at the same level of accuracy; (b) up to 14× real-world speed-ups for large-scale retrieval on {ImageNet}-1K and 4K; and (c) up to 2\% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that {MRL} extends seamlessly to web-scale datasets ({ImageNet}, {JFT}) across various modalities – vision ({ViT}, {ResNet}), vision + language ({ALIGN}) and language ({BERT}). {MRL} code and pretrained models are open-sourced at https://github.com/{RAIVNLab}/{MRL}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lake_building_2016" class=ref>
<summary class=citation>
<a id="lake_building_2016">[lake_building_2016]</a> - Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J. - <a href="http://arxiv.org/abs/1604.00289" target="_blank"><cite>Building Machines That Learn and Think Like People</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite lake_building_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-79" id="summaryabstract-79">Summary/Abstract</a></h1>
<div>Recent progress in artificial intelligence ({AI}) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lample_cross-lingual_2019" class=ref>
<summary class=citation>
<a id="lample_cross-lingual_2019">[lample_cross-lingual_2019]</a> - Lample, Guillaume and Conneau, Alexis - <a href="http://arxiv.org/abs/1901.07291" target="_blank"><cite>Cross-lingual Language Model Pretraining</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite lample_cross-lingual_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-80" id="summaryabstract-80">Summary/Abstract</a></h1>
<div>Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models ({XLMs}): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On {XNLI}, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 {BLEU} on {WMT}&#x27;16 German-English, improving the previous state of the art by more than 9 {BLEU}. On supervised machine translation, we obtain a new state of the art of 38.5 {BLEU} on {WMT}&#x27;16 Romanian-English, outperforming the previous best approach by more than 4 {BLEU}. Our code and pretrained models will be made publicly available.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lecun_path_nodate" class=ref>
<summary class=citation>
<a id="lecun_path_nodate">[lecun_path_nodate]</a> - {LeCun}, Yann - <cite>A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite lecun_path_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-81" id="summaryabstract-81">Summary/Abstract</a></h1>
<div>How could machines learn as eﬃciently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as conﬁgurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lee-thorp_fnet_2021" class=ref>
<summary class=citation>
<a id="lee-thorp_fnet_2021">[lee-thorp_fnet_2021]</a> - Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago - <a href="http://arxiv.org/abs/2105.03824" target="_blank"><cite>{FNet}: Mixing Tokens with Fourier Transforms</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite lee-thorp_fnet_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-82" id="summaryabstract-82">Summary/Abstract</a></h1>
<div>We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that mix input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of {BERT} on the {GLUE} benchmark, but pre-trains and runs up to seven times faster on {GPUs} and twice as fast on {TPUs}. The resulting model, which we name {FNet}, scales very efficiently to long inputs, matching the accuracy of the most accurate efficient Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on {GPUs} and relatively shorter sequence lengths on {TPUs}. Finally, {FNet} has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small {FNet} models outperform Transformer counterparts.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lehnert_beyond_2024" class=ref>
<summary class=citation>
<a id="lehnert_beyond_2024">[lehnert_beyond_2024]</a> - Lehnert, Lucas and Sukhbaatar, Sainbayar and Mcvay, Paul and Rabbat, Michael and Tian, Yuandong - <a href="http://arxiv.org/abs/2402.14083" target="_blank"><cite>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite lehnert_beyond_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-83" id="summaryabstract-83">Summary/Abstract</a></h1>
<div>While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7\% of the time, while using up to 26.8\% fewer search steps than standard \$A{\textasciicircum}*\$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of \$A{\textasciicircum}*\$. This model is then fine-tuned via expert iterations to perform fewer search steps than \$A{\textasciicircum}*\$ search while still generating an optimal plan. In our training method, \$A{\textasciicircum}*\$&#x27;s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10\${\textbackslash}times\$ smaller model size and a 10\${\textbackslash}times\$ smaller training dataset. We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lewis_retrieval-augmented_2020" class=ref>
<summary class=citation>
<a id="lewis_retrieval-augmented_2020">[lewis_retrieval-augmented_2020]</a> - Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe - <a href="https://arxiv.org/abs/2005.11401v4" target="_blank"><cite>Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite lewis_retrieval-augmented_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-84" id="summaryabstract-84">Summary/Abstract</a></h1>
<div>Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="li_learning_2017" class=ref>
<summary class=citation>
<a id="li_learning_2017">[li_learning_2017]</a> - Li, Zhizhong and Hoiem, Derek - <a href="http://arxiv.org/abs/1606.09282" target="_blank"><cite>Learning without Forgetting</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite li_learning_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-85" id="summaryabstract-85">Summary/Abstract</a></h1>
<div>When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network ({CNN}), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="li_deep_2020" class=ref>
<summary class=citation>
<a id="li_deep_2020">[li_deep_2020]</a> - Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, {AnHai} and Tan, Wang-Chiew - <a href="http://arxiv.org/abs/2004.00584" target="_blank"><cite>Deep Entity Matching with Pre-Trained Language Models</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite li_deep_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-86" id="summaryabstract-86">Summary/Abstract</a></h1>
<div>We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast {EM} as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as {BERT}, {DistilBERT}, or {RoBERTa} pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art ({SOTA}), by up to 29\% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto&#x27;s matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for {EM}. Finally, Ditto adapts a {SOTA} technique on data augmentation for text to {EM} to augment the training data with (difficult) examples. This way, Ditto is forced to learn harder to improve the model&#x27;s matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8\%. Perhaps more surprisingly, we establish that Ditto can achieve the previous {SOTA} results with at most half the number of labeled data. Finally, we demonstrate Ditto&#x27;s effectiveness on a real-world large-scale {EM} task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5\%.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="li_deep_2020-1" class=ref>
<summary class=citation>
<a id="li_deep_2020-1">[li_deep_2020-1]</a> - Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, {AnHai} and Tan, Wang-Chiew - <a href="http://arxiv.org/abs/2004.00584" target="_blank"><cite>Deep Entity Matching with Pre-Trained Language Models</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite li_deep_2020-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-87" id="summaryabstract-87">Summary/Abstract</a></h1>
<div>We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast {EM} as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as {BERT}, {DistilBERT}, or {RoBERTa} pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art ({SOTA}), by up to 29\% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto&#x27;s matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for {EM}. Finally, Ditto adapts a {SOTA} technique on data augmentation for text to {EM} to augment the training data with (difficult) examples. This way, Ditto is forced to learn harder to improve the model&#x27;s matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8\%. Perhaps more surprisingly, we establish that Ditto can achieve the previous {SOTA} results with at most half the number of labeled data. Finally, we demonstrate Ditto&#x27;s effectiveness on a real-world large-scale {EM} task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5\%.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="li_survey_2022" class=ref>
<summary class=citation>
<a id="li_survey_2022">[li_survey_2022]</a> - Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang - <a href="https://doi.org/10.1145/3495162" target="_blank"><cite>A Survey on Text Classification: From Traditional to Deep Learning</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite li_survey_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-88" id="summaryabstract-88">Summary/Abstract</a></h1>
<div>Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="li_chain_2024" class=ref>
<summary class=citation>
<a id="li_chain_2024">[li_chain_2024]</a> - Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu - <a href="http://arxiv.org/abs/2402.12875" target="_blank"><cite>Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite li_chain_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-89" id="summaryabstract-89">Summary/Abstract</a></h1>
<div>Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought ({CoT}), is a highly effective method to improve the accuracy of large language models ({LLMs}) on arithmetics and symbolic reasoning tasks. However, the mechanism behind {CoT} remains unclear. This work provides a theoretical understanding of the power of {CoT} for decoder-only transformers through the lens of expressiveness. Conceptually, {CoT} empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length n, previous works have shown that constantdepth transformers with finite precision poly(n) embedding size can only solve problems in {TC}0 without {CoT}. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in {AC}0, a proper subset of {TC}0. However, with T steps of {CoT}, constant-depth transformers using constant-bit precision and O(log n) embedding size can solve any problem solvable by boolean circuits of size T . Empirically, enabling {CoT} dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_roberta:_2019" class=ref>
<summary class=citation>
<a id="liu_roberta:_2019">[liu_roberta:_2019]</a> - Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin - <a href="http://arxiv.org/abs/1907.11692" target="_blank"><cite>{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite liu_roberta:_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-90" id="summaryabstract-90">Summary/Abstract</a></h1>
<div>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_multi-task_2019" class=ref>
<summary class=citation>
<a id="liu_multi-task_2019">[liu_multi-task_2019]</a> - Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng - <a href="https://www.aclweb.org/anthology/P19-1441" target="_blank"><cite>Multi-Task Deep Neural Networks for Natural Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite liu_multi-task_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-91" id="summaryabstract-91">Summary/Abstract</a></h1>
<div>In this paper, we present a Multi-Task Deep Neural Network ({MT}-{DNN}) for learning representations across multiple natural language understanding ({NLU}) tasks. {MT}-{DNN} not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. {MT}-{DNN} extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as {BERT} (Devlin et al., 2018). {MT}-{DNN} obtains new state-of-the-art results on ten {NLU} tasks, including {SNLI}, {SciTail}, and eight out of nine {GLUE} tasks, pushing the {GLUE} benchmark to 82.7\% (2.2\% absolute improvement) as of February 25, 2019 on the latest {GLUE} test set. We also demonstrate using the {SNLI} and {SciTail} datasets that the representations learned by {MT}-{DNN} allow domain adaptation with substantially fewer in-domain labels than the pre-trained {BERT} representations. Our code and pre-trained models will be made publicly available.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_seeing_nodate" class=ref>
<summary class=citation>
<a id="liu_seeing_nodate">[liu_seeing_nodate]</a> - Liu, Ziming and Gan, Eric and Tegmark, Max - <cite>{SEEING} {IS} {BELIEVING}: {BRAIN}-{INSPIRED} {MODULAR} {TRAINING} {FOR} {MECHANISTIC} {INTERPRETABILITY}</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite liu_seeing_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-92" id="summaryabstract-92">Summary/Abstract</a></h1>
<div>We introduce Brain-Inspired Modular Training ({BIMT}), a method for making neural networks more modular and interpretable. Inspired by brains, {BIMT} embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that {BIMT} discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_ring_2023" class=ref>
<summary class=citation>
<a id="liu_ring_2023">[liu_ring_2023]</a> - Liu, Hao and Zaharia, Matei and Abbeel, Pieter - <a href="http://arxiv.org/abs/2310.01889" target="_blank"><cite>Ring Attention with Blockwise Transformers for Near-Infinite Context</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite liu_ring_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-93" id="summaryabstract-93">Summary/Abstract</a></h1>
<div>Transformers have emerged as the architecture of choice for many state-of-the-art {AI} models, showcasing exceptional performance across a wide range of {AI} applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_aligning_2023" class=ref>
<summary class=citation>
<a id="liu_aligning_2023">[liu_aligning_2023]</a> - Liu, Wenhao and Wang, Xiaohua and Wu, Muling and Li, Tianlong and Lv, Changze and Ling, Zixuan and Zhu, Jianhao and Zhang, Cenyuan and Zheng, Xiaoqing and Huang, Xuanjing - <a href="http://arxiv.org/abs/2312.15997" target="_blank"><cite>Aligning Large Language Models with Human Preferences through Representation Engineering</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite liu_aligning_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-94" id="summaryabstract-94">Summary/Abstract</a></h1>
<div>Aligning large language models ({LLMs}) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback ({RLHF}) to fine-tune {LLMs} based on human labels assessing the relative quality of model responses. Nevertheless, {RLHF} is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering ({RepE}), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an {LLM}, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback ({RAHF}), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of {RAHF} in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). {RAHF}&#x27;s versatility in accommodating diverse human preferences shows its potential for advancing {LLM} performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_aligning_2023-1" class=ref>
<summary class=citation>
<a id="liu_aligning_2023-1">[liu_aligning_2023-1]</a> - Liu, Wenhao and Wang, Xiaohua and Wu, Muling and Li, Tianlong and Lv, Changze and Ling, Zixuan and Zhu, Jianhao and Zhang, Cenyuan and Zheng, Xiaoqing and Huang, Xuanjing - <a href="http://arxiv.org/abs/2312.15997" target="_blank"><cite>Aligning Large Language Models with Human Preferences through Representation Engineering</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite liu_aligning_2023-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-95" id="summaryabstract-95">Summary/Abstract</a></h1>
<div>Aligning large language models ({LLMs}) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback ({RLHF}) to fine-tune {LLMs} based on human labels assessing the relative quality of model responses. Nevertheless, {RLHF} is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering ({RepE}), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an {LLM}, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback ({RAHF}), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of {RAHF} in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). {RAHF}&#x27;s versatility in accommodating diverse human preferences shows its potential for advancing {LLM} performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lloyd_quantum_2020" class=ref>
<summary class=citation>
<a id="lloyd_quantum_2020">[lloyd_quantum_2020]</a> - Lloyd, Seth and Schuld, Maria and Ijaz, Aroosa and Izaac, Josh and Killoran, Nathan - <a href="http://arxiv.org/abs/2001.03622" target="_blank"><cite>Quantum embeddings for machine learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite lloyd_quantum_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-96" id="summaryabstract-96">Summary/Abstract</a></h1>
<div>Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lopez-paz_gradient_2017" class=ref>
<summary class=citation>
<a id="lopez-paz_gradient_2017">[lopez-paz_gradient_2017]</a> - Lopez-Paz, David and Ranzato, Marc&#x27;Aurelio - <a href="http://arxiv.org/abs/1706.08840" target="_blank"><cite>Gradient Episodic Memory for Continual Learning</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite lopez-paz_gradient_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-97" id="summaryabstract-97">Summary/Abstract</a></h1>
<div>One major obstacle towards {AI} is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory ({GEM}) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the {MNIST} and {CIFAR}-100 datasets demonstrate the strong performance of {GEM} when compared to the state-of-the-art.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lou_discrete_2024" class=ref>
<summary class=citation>
<a id="lou_discrete_2024">[lou_discrete_2024]</a> - Lou, Aaron and Meng, Chenlin and Ermon, Stefano - <a href="http://arxiv.org/abs/2310.16834" target="_blank"><cite>Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite lou_discrete_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-98" id="summaryabstract-98">Summary/Abstract</a></h1>
<div>Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models ({SEDD}) on standard language modeling tasks. For comparable model sizes, {SEDD} beats existing language diffusion paradigms (reducing perplexity by 25-75\%) and is competitive with autoregressive models, in particular outperforming {GPT}-2. Furthermore, compared to autoregressive mdoels, {SEDD} generates faithful text without requiring distribution annealing techniques like temperature scaling (around 68× better generative perplexity than un-annealed {GPT}-2), can trade compute and quality (similar quality with 32× fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lundberg_unified_2017" class=ref>
<summary class=citation>
<a id="lundberg_unified_2017">[lundberg_unified_2017]</a> - Lundberg, Scott and Lee, Su-In - <a href="http://arxiv.org/abs/1705.07874" target="_blank"><cite>A Unified Approach to Interpreting Model Predictions</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite lundberg_unified_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-99" id="summaryabstract-99">Summary/Abstract</a></h1>
<div>Understanding why a model makes a certain prediction can be as crucial as the prediction&#x27;s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, {SHAP} ({SHapley} Additive {exPlanations}). {SHAP} assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="luo_have_2021" class=ref>
<summary class=citation>
<a id="luo_have_2021">[luo_have_2021]</a> - Luo, Ziyang - <a href="http://arxiv.org/abs/2102.07926" target="_blank"><cite>Have Attention Heads in {BERT} Learned Constituency Grammar?</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite luo_have_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-100" id="summaryabstract-100">Summary/Abstract</a></h1>
<div>With the success of pre-trained language models in recent years, more and more researchers focus on opening the black box of these models. Following this interest, we carry out a qualitative and quantitative analysis of constituency grammar in attention heads of {BERT} and {RoBERTa}. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist heads that can induce some grammar types much better than baselines, suggesting that some heads act as a proxy for constituency grammar. We also analyze how attention heads&#x27; constituency grammar inducing ({CGI}) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity ({SMS}) tasks and natural language inference ({NLI}) tasks. Our results suggest that {SMS} tasks decrease the average {CGI} ability of upper layers, while {NLI} tasks increase it. Lastly, we investigate the connections between {CGI} ability and natural language understanding ability on {QQP} and {MNLI} tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ma_sciagent_2024" class=ref>
<summary class=citation>
<a id="ma_sciagent_2024">[ma_sciagent_2024]</a> - Ma, Yubo and Gou, Zhibin and Hao, Junheng and Xu, Ruochen and Wang, Shuohang and Pan, Liangming and Yang, Yujiu and Cao, Yixin and Sun, Aixin and Awadalla, Hany and Chen, Weizhu - <a href="http://arxiv.org/abs/2402.11451" target="_blank"><cite>{SciAgent}: Tool-augmented Language Models for Scientific Reasoning</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite ma_sciagent_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-101" id="summaryabstract-101">Summary/Abstract</a></h1>
<div>Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models ({LLMs}). To make this task more practical and solvable for {LLMs}, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements {LLMs} with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named {MathFunc} which encompasses over 30,000 samples and roughly 6,000 tools. Building on {MathFunc}, we develop {SciAgent} to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, {SciToolBench}, spanning five scientific domains to evaluate {LLMs}&#x27; abilities with tool assistance. Extensive experiments on {SciToolBench} confirm the effectiveness of {SciAgent}. Notably, {SciAgent}-Mistral-7B surpasses other {LLMs} with the same size by more than 13\% in absolute accuracy. Furthermore, {SciAgent}-{DeepMath}-7B shows much superior performance than {ChatGPT}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="marcus_next_2020" class=ref>
<summary class=citation>
<a id="marcus_next_2020">[marcus_next_2020]</a> - Marcus, Gary - <a href="http://arxiv.org/abs/2002.06177" target="_blank"><cite>The Next Decade in {AI}: Four Steps Towards Robust Artificial Intelligence</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite marcus_next_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-102" id="summaryabstract-102">Summary/Abstract</a></h1>
<div>Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust {AI} than is currently possible.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="masse_alleviating_2018" class=ref>
<summary class=citation>
<a id="masse_alleviating_2018">[masse_alleviating_2018]</a> - Masse, Nicolas Y. and Grant, Gregory D. and Freedman, David J. - <a href="https://www.pnas.org/content/115/44/E10467" target="_blank"><cite>Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite masse_alleviating_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-103" id="summaryabstract-103">Summary/Abstract</a></h1>
<div>Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks ({ANNs}) on new tasks typically causes them to forget previously learned tasks. This phenomenon is the result of “catastrophic forgetting,” in which training an {ANN} disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of {ANNs} that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows {ANNs} to maintain high performance across large numbers of sequentially presented tasks, particularly when combined with weight stabilization. We show that this method works for both feedforward and recurrent network architectures, trained using either supervised or reinforcement-based learning. This suggests that using multiple, complementary methods, akin to what is believed to occur in the brain, can be a highly effective strategy to support continual learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mccloskey_catastrophic_1989" class=ref>
<summary class=citation>
<a id="mccloskey_catastrophic_1989">[mccloskey_catastrophic_1989]</a> - {McCloskey}, Michael and Cohen, Neal J. - <a href="https://www.sciencedirect.com/science/article/pii/S0079742108605368" target="_blank"><cite>Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</cite></a>. - 1989. -
<button onclick="copyToClipboard('\{\{ #cite mccloskey_catastrophic_1989 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-104" id="summaryabstract-104">Summary/Abstract</a></h1>
<div>Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="metz_genius_2021" class=ref>
<summary class=citation>
<a id="metz_genius_2021">[metz_genius_2021]</a> - Metz, Cade - <cite>Genius Makers: The Mavericks Who Brought {AI} to Google, Facebook, and the World</cite>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite metz_genius_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-105" id="summaryabstract-105">Summary/Abstract</a></h1>
<div>This colorful page-turner puts artificial intelligence into a human perspective. Through the lives of Geoff Hinton and other major players, Metz explains this transformative technology and makes the quest thrilling.—Walter Isaacson, author of The Code {BreakerRecipient} of starred reviews in both Kirkus and Library {JournalTHE} {UNTOLD} {TECH} {STORY} {OF} {OUR} {TIME}   What does it mean to be smart? To be human? What do we really want from life and the intelligence we have, or might create?   With deep and exclusive reporting, across hundreds of interviews, New York Times Silicon Valley journalist Cade Metz brings you into the rooms where these questions are being answered. Where an extraordinarily powerful new artificial intelligence has been built into our biggest companies, our social discourse, and our daily lives, with few of us even noticing.     Long dismissed as a technology of the distant future, artificial intelligence was a project consigned to the fringes of the scientific community. Then two researchers changed everything. One was a sixty-four-year-old computer science professor who didn’t drive and didn’t fly because he could no longer sit down—but still made his way across North America for the moment that would define a new age of technology. The other was a thirty-six-year-old neuroscientist and chess prodigy who laid claim to being the greatest game player of all time before vowing to build a machine that could do anything the human brain could do.   They took two very different paths to that lofty goal, and they disagreed on how quickly it would arrive. But both were soon drawn into the heart of the tech industry. Their ideas drove a new kind of arms race, spanning Google, Microsoft, Facebook, and {OpenAI}, a new lab founded by Silicon Valley kingpin Elon Musk. But some believed that China would beat them all to the finish line.   Genius Makers dramatically presents the fierce conflict between national interests, shareholder value, the pursuit of scientific knowledge, and the very human concerns about privacy, security, bias, and prejudice. Like a great Victorian novel, this world of eccentric, brilliant, often unimaginably yet suddenly wealthy characters draws you into the most profound moral questions we can ask. And like a great mystery, it presents the story and facts that lead to a core, vital question:   How far will we let it go?</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mikolov_efficient_2013" class=ref>
<summary class=citation>
<a id="mikolov_efficient_2013">[mikolov_efficient_2013]</a> - Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey - <a href="http://arxiv.org/abs/1301.3781" target="_blank"><cite>Efficient Estimation of Word Representations in Vector Space</cite></a>. - 2013. -
<button onclick="copyToClipboard('\{\{ #cite mikolov_efficient_2013 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-106" id="summaryabstract-106">Summary/Abstract</a></h1>
<div>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mikolov_distributed_nodate" class=ref>
<summary class=citation>
<a id="mikolov_distributed_nodate">[mikolov_distributed_nodate]</a> - Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff - <cite>Distributed Representations of Words and Phrases and their Compositionality</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite mikolov_distributed_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-107" id="summaryabstract-107">Summary/Abstract</a></h1>
<div>The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="miller_explanation_2018" class=ref>
<summary class=citation>
<a id="miller_explanation_2018">[miller_explanation_2018]</a> - Miller, Tim - <a href="http://arxiv.org/abs/1706.07269" target="_blank"><cite>Explanation in Artificial Intelligence: Insights from the Social Sciences</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite miller_explanation_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-108" id="summaryabstract-108">Summary/Abstract</a></h1>
<div>There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers&#x27; intuition of what constitutes a &#x60;good&#x27; explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minaee_deep_2021" class=ref>
<summary class=citation>
<a id="minaee_deep_2021">[minaee_deep_2021]</a> - Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng - <a href="http://arxiv.org/abs/2004.03705" target="_blank"><cite>Deep Learning Based Text Classification: A Comprehensive Review</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite minaee_deep_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-109" id="summaryabstract-109">Summary/Abstract</a></h1>
<div>Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minsky_framework_1974" class=ref>
<summary class=citation>
<a id="minsky_framework_1974">[minsky_framework_1974]</a> - Minsky, Marvin - <a href="https://web.media.mit.edu/~minsky/papers/Frames/frames.html" target="_blank"><cite>A Framework for Representing Knowledge</cite></a>. - 1974. -
<button onclick="copyToClipboard('\{\{ #cite minsky_framework_1974 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-110" id="summaryabstract-110">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minsky_k-lines_1979" class=ref>
<summary class=citation>
<a id="minsky_k-lines_1979">[minsky_k-lines_1979]</a> - Minsky, Marvin - <a href="https://dspace.mit.edu/handle/1721.1/5739" target="_blank"><cite>K-Lines: A Theory of Memory</cite></a>. - 1979. -
<button onclick="copyToClipboard('\{\{ #cite minsky_k-lines_1979 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-111" id="summaryabstract-111">Summary/Abstract</a></h1>
<div>Most theories of memory suggest that when  we learn or memorize something, some  representation of that something is  constructed, stored and later retrieved. This  raises questions like: How is information  represented? How is it stored? How is it  retrieved? Then, how is it use? This paper  tries to deal with all these at once. When you  get an idea and want to remember it, you  create a K-line for it. When later activated, the  K-line induces a partial mental state  resembling the one that created it. A partial  mental state is a subset of those mental  agencies operating at one moment. This view  leads to many ideas about the development,  structure and physiology of Memory, and  about how to implement frame-like  representations in a distributed processor.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minsky_perceptrons_1987" class=ref>
<summary class=citation>
<a id="minsky_perceptrons_1987">[minsky_perceptrons_1987]</a> - Minsky, Marvin and Papert, Seymour A. - <cite>Perceptrons: An Introduction to Computational Geometry, Expanded Edition</cite>. - 1987. -
<button onclick="copyToClipboard('\{\{ #cite minsky_perceptrons_1987 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-112" id="summaryabstract-112">Summary/Abstract</a></h1>
<div>Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades. It marked a historical turn in artificial intelligence, and it is required reading for anyone who wants to understand the connectionist counterrevolution that is going on today. Artificial-intelligence research, which for a time concentrated on the programming of ton Neumann computers, is swinging back to the idea that intelligence might emerge from the activity of networks of neuronlike entities. Minsky and Papert&#x27;s book was the first example of a mathematical analysis carried far enough to show the exact limitations of a class of computing machines that could seriously be considered as models of the brain. Now the new developments in mathematical tools, the recent interest of physicists in the theory of disordered matter, the new insights into and psychological models of how the brain works, and the evolution of fast computers that can simulate networks of automata have given Perceptrons new importance.Witnessing the swing of the intellectual pendulum, Minsky and Papert have added a new chapter in which they discuss the current state of parallel computers, review developments since the appearance of the 1972 edition, and identify new research directions related to connectionism. They note a central theoretical challenge facing connectionism: the challenge to reach a deeper understanding of how objects or agents with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called society theories of mind. Marvin L. Minsky is Donner Professor of Science in M.I.T.&#x27;s Electrical Engineering and Computer Science Department. Seymour A. Papert is Professor of Media Technology at M.I.T. .</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mnih_human-level_2015" class=ref>
<summary class=citation>
<a id="mnih_human-level_2015">[mnih_human-level_2015]</a> - Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis - <a href="http://www.nature.com/articles/nature14236" target="_blank"><cite>Human-level control through deep reinforcement learning</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite mnih_human-level_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-113" id="summaryabstract-113">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="molnar_interpretable_nodate" class=ref>
<summary class=citation>
<a id="molnar_interpretable_nodate">[molnar_interpretable_nodate]</a> - Molnar, Christoph - <a href="https://christophm.github.io/interpretable-ml-book/" target="_blank"><cite>Interpretable Machine Learning</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite molnar_interpretable_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-114" id="summaryabstract-114">Summary/Abstract</a></h1>
<div>Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mudgal_deep_2018" class=ref>
<summary class=citation>
<a id="mudgal_deep_2018">[mudgal_deep_2018]</a> - Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, {AnHai} and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay - <a href="https://dl.acm.org/doi/10.1145/3183713.3196926" target="_blank"><cite>Deep Learning for Entity Matching: A Design Space Exploration</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite mudgal_deep_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-115" id="summaryabstract-115">Summary/Abstract</a></h1>
<div>Entity matching ({EM}) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning ({DL}) to {EM}, to understand {DL}’s benefits and limitations. We review many {DL} solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of {DL} solutions for {EM}, as embodied by four solutions with varying representational power: {SIF}, {RNN}, Attention, and Hybrid. Next, we investigate the types of {EM} problems for which {DL} can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four {DL} solutions with Magellan, a state-of-the-art learning-based {EM} solution. The results show that {DL} does not outperform current solutions on structured {EM}, but it can significantly outperform them on textual and dirty {EM}. For practitioners, this suggests that they should seriously consider using {DL} for textual and dirty {EM} problems. Finally, we analyze {DL}’s performance and discuss future research directions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="nakkiran_deep_2019" class=ref>
<summary class=citation>
<a id="nakkiran_deep_2019">[nakkiran_deep_2019]</a> - Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya - <a href="http://arxiv.org/abs/1912.02292" target="_blank"><cite>Deep Double Descent: Where Bigger Models and More Data Hurt</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite nakkiran_deep_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-116" id="summaryabstract-116">Summary/Abstract</a></h1>
<div>We show that a variety of modern deep learning tasks exhibit a double-descent phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="nasr_scalable_2023" class=ref>
<summary class=citation>
<a id="nasr_scalable_2023">[nasr_scalable_2023]</a> - Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Tramèr, Florian and Lee, Katherine - <a href="http://arxiv.org/abs/2311.17035" target="_blank"><cite>Scalable Extraction of Training Data from (Production) Language Models</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite nasr_scalable_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-117" id="summaryabstract-117">Summary/Abstract</a></h1>
<div>This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or {GPT}-Neo, semi-open models like {LLaMA} or Falcon, and closed models like {ChatGPT}. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned {ChatGPT}, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ouyang_training_2022" class=ref>
<summary class=citation>
<a id="ouyang_training_2022">[ouyang_training_2022]</a> - Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan - <a href="http://arxiv.org/abs/2203.02155" target="_blank"><cite>Training language models to follow instructions with human feedback</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite ouyang_training_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-118" id="summaryabstract-118">Summary/Abstract</a></h1>
<div>Making language models bigger does not inherently make them better at following a user&#x27;s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="pan_survey_2010" class=ref>
<summary class=citation>
<a id="pan_survey_2010">[pan_survey_2010]</a> - Pan, Sinno Jialin and Yang, Qiang - <a href="http://ieeexplore.ieee.org/document/5288526/" target="_blank"><cite>A Survey on Transfer Learning</cite></a>. - 2010. -
<button onclick="copyToClipboard('\{\{ #cite pan_survey_2010 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-119" id="summaryabstract-119">Summary/Abstract</a></h1>
<div>A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="pan_very_2024" class=ref>
<summary class=citation>
<a id="pan_very_2024">[pan_very_2024]</a> - Pan, Xuchen and Gao, Dawei and Xie, Yuexiang and Wei, Zhewei and Li, Yaliang and Ding, Bolin and Wen, Ji-Rong and Zhou, Jingren - <a href="http://arxiv.org/abs/2407.17789" target="_blank"><cite>Very Large-Scale Multi-Agent Simulation in {AgentScope}</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite pan_very_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-120" id="summaryabstract-120">Summary/Abstract</a></h1>
<div>Recent advances in large language models ({LLMs}) have opened new avenues for applying multi-agent systems in very large-scale simulations. However, there remain several challenges when conducting multi-agent simulations with existing platforms, such as limited scalability and low efficiency, unsatisfied agent diversity, and effort-intensive management processes. To address these challenges, we develop several new features and components for {AgentScope}, a user-friendly multi-agent platform, enhancing its convenience and flexibility for supporting very large-scale multi-agent simulations. Specifically, we propose an actor-based distributed mechanism as the underlying technological infrastructure towards great scalability and high efficiency, and provide flexible environment support for simulating various real-world scenarios, which enables parallel execution of multiple agents, centralized workflow orchestration, and both inter-agent and agent-environment interactions among agents. Moreover, we integrate an easy-to-use configurable tool and an automatic background generation pipeline in {AgentScope}, simplifying the process of creating agents with diverse yet detailed background settings. Last but not least, we provide a web-based interface for conveniently monitoring and managing a large number of agents that might deploy across multiple devices. We conduct a comprehensive simulation to demonstrate the effectiveness of the proposed enhancements in {AgentScope}, and provide detailed observations and discussions to highlight the great potential of applying multi-agent systems in large-scale simulations. The source code is released on {GitHub}1 to inspire further research and development in large-scale multi-agent simulations.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="parisi_continual_2019" class=ref>
<summary class=citation>
<a id="parisi_continual_2019">[parisi_continual_2019]</a> - Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan - <a href="http://arxiv.org/abs/1802.07569" target="_blank"><cite>Continual Lifelong Learning with Neural Networks: A Review</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite parisi_continual_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-121" id="summaryabstract-121">Summary/Abstract</a></h1>
<div>Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="parmar_logicbench_2024" class=ref>
<summary class=citation>
<a id="parmar_logicbench_2024">[parmar_logicbench_2024]</a> - Parmar, Mihir and Patel, Nisarg and Varshney, Neeraj and Nakamura, Mutsumi and Luo, Man and Mashetty, Santosh and Mitra, Arindam and Baral, Chitta - <a href="https://aclanthology.org/2024.acl-long.739" target="_blank"><cite>{LogicBench}: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite parmar_logicbench_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-122" id="summaryabstract-122">Summary/Abstract</a></h1>
<div>Recently developed large language models ({LLMs}) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really “reason” over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to &#x60;logical reasoning&#x27; has remained underexplored. Existing work investigating this reasoning ability of {LLMs} has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of {LLMs} on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce {LogicBench}, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of {LLMs} such as {GPT}-4, {ChatGPT}, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing {LLMs} do not fare well on {LogicBench}; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes tend to prioritize parametric knowledge over contextual information and overlook the correct reasoning chain. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of {LLMs}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="pearl_book_2020" class=ref>
<summary class=citation>
<a id="pearl_book_2020">[pearl_book_2020]</a> - Pearl, Judea and Mackenzie, Dana - <cite>The Book of Why: The New Science of Cause and Effect</cite>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite pearl_book_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-123" id="summaryabstract-123">Summary/Abstract</a></h1>
<div>A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence Correlation is not causation. This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl&#x27;s work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="peng_hierarchical_2019" class=ref>
<summary class=citation>
<a id="peng_hierarchical_2019">[peng_hierarchical_2019]</a> - Peng, Hao and Li, Jianxin and Gong, Qiran and Wang, Senzhang and He, Lifang and Li, Bo and Wang, Lihong and Yu, Philip S. - <a href="http://arxiv.org/abs/1906.04898" target="_blank"><cite>Hierarchical Taxonomy-Aware and Attentional Graph Capsule {RCNNs} for Large-Scale Multi-Label Text Classification</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite peng_hierarchical_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-124" id="summaryabstract-124">Summary/Abstract</a></h1>
<div>{CNNs}, {RNNs}, {GCNs}, and {CapsNets} have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent {CNNs} framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent {CNNs} for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="perozzi_let_2024" class=ref>
<summary class=citation>
<a id="perozzi_let_2024">[perozzi_let_2024]</a> - Perozzi, Bryan and Fatemi, Bahare and Zelle, Dustin and Tsitsulin, Anton and Kazemi, Mehran and Al-Rfou, Rami and Halcrow, Jonathan - <a href="https://arxiv.org/abs/2402.05862v1" target="_blank"><cite>Let Your Graph Do the Talking: Encoding Structured Data for {LLMs}</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite perozzi_let_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-125" id="summaryabstract-125">Summary/Abstract</a></h1>
<div>How can we best encode structured data into sequential form for use in large language models ({LLMs})? In this work, we introduce a parameter-efficient method to explicitly represent structured data for {LLMs}. Our method, {GraphToken}, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73\% points - on node, edge and, graph-level tasks from the {GraphQA} benchmark.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="peters_deep_2018" class=ref>
<summary class=citation>
<a id="peters_deep_2018">[peters_deep_2018]</a> - Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke - <a href="http://arxiv.org/abs/1802.05365" target="_blank"><cite>Deep contextualized word representations</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite peters_deep_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-126" id="summaryabstract-126">Summary/Abstract</a></h1>
<div>We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="polyzotis_slice_2019" class=ref>
<summary class=citation>
<a id="polyzotis_slice_2019">[polyzotis_slice_2019]</a> - Polyzotis, Neoklis and Whang, Steven and Kraska, Tim Klas and Chung, Yeounoh - <a href="https://arxiv.org/pdf/1807.06068.pdf" target="_blank"><cite>Slice Finder: Automated Data Slicing for Model Validation</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite polyzotis_slice_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-127" id="summaryabstract-127">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="potts_graphrag_2024" class=ref>
<summary class=citation>
<a id="potts_graphrag_2024">[potts_graphrag_2024]</a> - Potts, Brenda - <a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/" target="_blank"><cite>{GraphRAG}: A new approach for discovery using complex information</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite potts_graphrag_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-128" id="summaryabstract-128">Summary/Abstract</a></h1>
<div>Microsoft is transforming retrieval-augmented generation with {GraphRAG}, using {LLM}-generated knowledge graphs to significantly improve Q\&amp;A when analyzing complex information and consistently outperforming baseline {RAG}. Get the details.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="primpeli_profiling_2020" class=ref>
<summary class=citation>
<a id="primpeli_profiling_2020">[primpeli_profiling_2020]</a> - Primpeli, Anna and Bizer, Christian - <a href="https://dl.acm.org/doi/10.1145/3340531.3412781" target="_blank"><cite>Profiling Entity Matching Benchmark Tasks</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite primpeli_profiling_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-129" id="summaryabstract-129">Summary/Abstract</a></h1>
<div>Entity matching is a central task in data integration which has been researched for decades. Over this time, a wide range of benchmark tasks for evaluating entity matching methods has been developed. This resource paper systematically complements, profiles, and compares 21 entity matching benchmark tasks. In order to better understand the specific challenges associated with different tasks, we define a set of profiling dimensions which capture central aspects of the matching tasks. Using these dimensions, we create groups of benchmark tasks having similar characteristics. Afterwards, we assess the difficulty of the tasks in each group by computing baseline evaluation results using standard feature engineering together with two common classification methods. In order to enable the exact reproducibility of evaluation results, matching tasks need to contain exactly defined sets of matching and non-matching record pairs, as well as a fixed development and test split. As this is not the case for some widely-used benchmark tasks, we complement these tasks with fixed sets of non-matching pairs, as well as fixed splits, and provide the resulting development and test sets for public download. By profiling and complementing the benchmark tasks, we support researchers to select challenging as well as diverse tasks and to compare matching systems on clearly defined grounds.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="prukalpa_future_2022" class=ref>
<summary class=citation>
<a id="prukalpa_future_2022">[prukalpa_future_2022]</a> - Prukalpa - <a href="https://towardsdatascience.com/the-future-of-the-modern-data-stack-in-2022-4f4c91bb778f" target="_blank"><cite>The Future of the Modern Data Stack in 2022</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite prukalpa_future_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-130" id="summaryabstract-130">Summary/Abstract</a></h1>
<div>Featuring the 6 big ideas you should know from 2021</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="radford_language_nodate" class=ref>
<summary class=citation>
<a id="radford_language_nodate">[radford_language_nodate]</a> - Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya - <cite>Language Models are Unsupervised Multitask Learners</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite radford_language_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-131" id="summaryabstract-131">Summary/Abstract</a></h1>
<div>Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="radford_improving_nodate" class=ref>
<summary class=citation>
<a id="radford_improving_nodate">[radford_improving_nodate]</a> - Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya - <cite>Improving Language Understanding by Generative Pre-Training</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite radford_improving_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-132" id="summaryabstract-132">Summary/Abstract</a></h1>
<div>Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering ({RACE}), and 1.5\% on textual entailment ({MultiNLI}).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="rafailov_direct_2024" class=ref>
<summary class=citation>
<a id="rafailov_direct_2024">[rafailov_direct_2024]</a> - Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea - <a href="http://arxiv.org/abs/2305.18290" target="_blank"><cite>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite rafailov_direct_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-133" id="summaryabstract-133">Summary/Abstract</a></h1>
<div>While large-scale unsupervised language models ({LMs}) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised {LM} to align with these preferences, often with reinforcement learning from human feedback ({RLHF}). However, {RLHF} is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised {LM} using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in {RLHF} that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard {RLHF} problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization ({DPO}), is stable, performant, and computationally lightweight, eliminating the need for sampling from the {LM} during fine-tuning or performing significant hyperparameter tuning. Our experiments show that {DPO} can fine-tune {LMs} to align with human preferences as well as or better than existing methods. Notably, fine-tuning with {DPO} exceeds {PPO}-based {RLHF} in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="raffel_exploring_2020" class=ref>
<summary class=citation>
<a id="raffel_exploring_2020">[raffel_exploring_2020]</a> - Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J. - <a href="http://arxiv.org/abs/1910.10683" target="_blank"><cite>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite raffel_exploring_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-134" id="summaryabstract-134">Summary/Abstract</a></h1>
<div>Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new &#x60;&#x60;Colossal Clean Crawled Corpus&#x27;&#x27;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="rajbhandari_deepspeed-moe_2022" class=ref>
<summary class=citation>
<a id="rajbhandari_deepspeed-moe_2022">[rajbhandari_deepspeed-moe_2022]</a> - Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong - <a href="http://arxiv.org/abs/2201.05596" target="_blank"><cite>{DeepSpeed}-{MoE}: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation {AI} Scale</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite rajbhandari_deepspeed-moe_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-135" id="summaryabstract-135">Summary/Abstract</a></h1>
<div>As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts ({MoE}) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast {MoE} model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present {DeepSpeed}-{MoE}, an end-to-end {MoE} training and inference solution as part of the {DeepSpeed} library, including novel {MoE} architecture designs and model compression techniques that reduce {MoE} model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing {MoE} inference solutions. {DeepSpeed}-{MoE} offers an unprecedented scale and efficiency to serve massive {MoE} models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse {MoE} models, where training and deploying higher-quality models with fewer resources becomes more widely possible.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="rajpurkar_squad_2016" class=ref>
<summary class=citation>
<a id="rajpurkar_squad_2016">[rajpurkar_squad_2016]</a> - Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy - <a href="http://arxiv.org/abs/1606.05250" target="_blank"><cite>{SQuAD}: 100,000+ Questions for Machine Comprehension of Text</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite rajpurkar_squad_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-136" id="summaryabstract-136">Summary/Abstract</a></h1>
<div>We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="raschka_build_2024" class=ref>
<summary class=citation>
<a id="raschka_build_2024">[raschka_build_2024]</a> - Raschka, Sebastian - <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" target="_blank"><cite>Build A Large Language Model (From Scratch)</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite raschka_build_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-137" id="summaryabstract-137">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="real_automl-zero_2020" class=ref>
<summary class=citation>
<a id="real_automl-zero_2020">[real_automl-zero_2020]</a> - Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V. - <a href="http://arxiv.org/abs/2003.03384" target="_blank"><cite>{AutoML}-Zero: Evolving Machine Learning Algorithms From Scratch</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite real_automl-zero_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-138" id="summaryabstract-138">Summary/Abstract</a></h1>
<div>Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as {AutoML}, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that {AutoML} can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. {CIFAR}-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="rebuffel_controlling_2021" class=ref>
<summary class=citation>
<a id="rebuffel_controlling_2021">[rebuffel_controlling_2021]</a> - Rebuffel, Clément and Roberti, Marco and Soulier, Laure and Scoutheeten, Geoffrey and Cancelliere, Rossella and Gallinari, Patrick - <a href="http://arxiv.org/abs/2102.02810" target="_blank"><cite>Controlling Hallucinations at Word Level in Data-to-Text Generation</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite rebuffel_controlling_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-139" id="summaryabstract-139">Summary/Abstract</a></h1>
<div>Data-to-Text Generation ({DTG}) is a subfield of Natural Language Generation aiming at transcribing structured data in natural language descriptions. The field has been recently boosted by the use of neural-based generators which exhibit on one side great syntactic skills without the need of hand-crafted pipelines; on the other side, the quality of the generated text reflects the quality of the training data, which in realistic settings only offer imperfectly aligned structure-text pairs. Consequently, state-of-art neural models include misleading statements - usually called hallucinations - in their outputs. The control of this phenomenon is today a major challenge for {DTG}, and is the problem addressed in the paper. Previous work deal with this issue at the instance level: using an alignment score for each table-reference pair. In contrast, we propose a finer-grained approach, arguing that hallucinations should rather be treated at the word level. Specifically, we propose a Multi-Branch Decoder which is able to leverage word-level labels to learn the relevant parts of each training instance. These labels are obtained following a simple and efficient scoring procedure based on co-occurrence analysis and dependency parsing. Extensive evaluations, via automated metrics and human judgment on the standard {WikiBio} benchmark, show the accuracy of our alignment labels and the effectiveness of the proposed Multi-Branch Decoder. Our model is able to reduce and control hallucinations, while keeping fluency and coherence in generated texts. Further experiments on a degraded version of {ToTTo} show that our model could be successfully used on very noisy settings.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ribeiro_beyond_2020" class=ref>
<summary class=citation>
<a id="ribeiro_beyond_2020">[ribeiro_beyond_2020]</a> - Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer - <a href="http://arxiv.org/abs/2005.04118" target="_blank"><cite>Beyond Accuracy: Behavioral Testing of {NLP} models with {CheckList}</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite ribeiro_beyond_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-140" id="summaryabstract-140">Summary/Abstract</a></h1>
<div>Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of {NLP} models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce {CheckList}, a task-agnostic methodology for testing {NLP} models. {CheckList} includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of {CheckList} with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, {NLP} practitioners with {CheckList} created twice as many tests, and found almost three times as many bugs as users without it.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ribeiro_beyond_2020-1" class=ref>
<summary class=citation>
<a id="ribeiro_beyond_2020-1">[ribeiro_beyond_2020-1]</a> - Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer - <a href="https://www.aclweb.org/anthology/2020.acl-main.442" target="_blank"><cite>Beyond Accuracy: Behavioral Testing of {NLP} Models with {CheckList}</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite ribeiro_beyond_2020-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-141" id="summaryabstract-141">Summary/Abstract</a></h1>
<div>Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of {NLP} models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce {CheckList}, a task-agnostic methodology for testing {NLP} models. {CheckList} includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of {CheckList} with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, {NLP} practitioners with {CheckList} created twice as many tests, and found almost three times as many bugs as users without it.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="rios_few-shot_2018" class=ref>
<summary class=citation>
<a id="rios_few-shot_2018">[rios_few-shot_2018]</a> - Rios, Anthony and Kavuluru, Ramakanth - <a href="https://www.aclweb.org/anthology/D18-1352" target="_blank"><cite>Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite rios_few-shot_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-142" id="summaryabstract-142">Summary/Abstract</a></h1>
<div>Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: {MIMIC} {II} and {MIMIC} {III}. For few-shot labels we achieve improvements of 6.2\% and 4.8\% in R_at_10 for {MIMIC} {II} and {MIMIC} {III}, respectively, over prior efforts; the corresponding R_at_10 improvements for zero-shot labels are 17.3\% and 19\%.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ruder_transfer_2019" class=ref>
<summary class=citation>
<a id="ruder_transfer_2019">[ruder_transfer_2019]</a> - Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas - <a href="https://www.aclweb.org/anthology/N19-5004" target="_blank"><cite>Transfer Learning in Natural Language Processing</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite ruder_transfer_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-143" id="summaryabstract-143">Summary/Abstract</a></h1>
<div>The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing ({NLP}) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of {NLP} tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and {ImageNet} pretraining in computer vision, and indicate that these methods will likely become a common tool in the {NLP} landscape as well as an important research direction. We will present an overview of modern transfer learning methods in {NLP}, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream {NLP} tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ruffy_state_2019" class=ref>
<summary class=citation>
<a id="ruffy_state_2019">[ruffy_state_2019]</a> - Ruffy, Fabian and Chahal, Karanbir - <a href="https://arxiv.org/abs/1912.10850v1" target="_blank"><cite>The State of Knowledge Distillation for Classification</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite ruffy_state_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-144" id="summaryabstract-144">Summary/Abstract</a></h1>
<div>We survey various knowledge distillation ({KD}) strategies for simple
classification tasks and implement a set of techniques that claim
state-of-the-art accuracy. Our experiments using standardized model
architectures, fixed compute budgets, and consistent training schedules
indicate that many of these distillation results are hard to reproduce. This is
especially apparent with methods using some form of feature distillation.
Further examination reveals a lack of generalizability where these techniques
may only succeed for specific architectures and training settings. We observe
that appropriately tuned classical distillation in combination with a data
augmentation training scheme gives an orthogonal improvement over other
techniques. We validate this approach and open-source our code.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="salimans_weight_2016" class=ref>
<summary class=citation>
<a id="salimans_weight_2016">[salimans_weight_2016]</a> - Salimans, Tim and Kingma, Diederik P. - <a href="http://arxiv.org/abs/1602.07868" target="_blank"><cite>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite salimans_weight_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-145" id="summaryabstract-145">Summary/Abstract</a></h1>
<div>We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as {LSTMs} and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sanh_distilbert_2020" class=ref>
<summary class=citation>
<a id="sanh_distilbert_2020">[sanh_distilbert_2020]</a> - Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas - <a href="http://arxiv.org/abs/1910.01108" target="_blank"><cite>{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite sanh_distilbert_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-146" id="summaryabstract-146">Summary/Abstract</a></h1>
<div>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schaul_prioritized_2015" class=ref>
<summary class=citation>
<a id="schaul_prioritized_2015">[schaul_prioritized_2015]</a> - Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David - <a href="https://arxiv.org/abs/1511.05952v4" target="_blank"><cite>Prioritized Experience Replay</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite schaul_prioritized_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-147" id="summaryabstract-147">Summary/Abstract</a></h1>
<div>Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks ({DQN}), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. {DQN} with prioritized
experience replay achieves a new state-of-the-art, outperforming {DQN} with
uniform replay on 41 out of 49 games.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schmidhuber_deep_2015" class=ref>
<summary class=citation>
<a id="schmidhuber_deep_2015">[schmidhuber_deep_2015]</a> - Schmidhuber, Juergen - <a href="http://arxiv.org/abs/1404.7828" target="_blank"><cite>Deep Learning in Neural Networks: An Overview</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite schmidhuber_deep_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-148" id="summaryabstract-148">Summary/Abstract</a></h1>
<div>In recent years, deep artiﬁcial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \&amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schrimpf_neural_2021" class=ref>
<summary class=citation>
<a id="schrimpf_neural_2021">[schrimpf_neural_2021]</a> - Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina - <a href="https://pnas.org/doi/full/10.1073/pnas.2105646118" target="_blank"><cite>The neural architecture of language: Integrative modeling converges on predictive processing</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite schrimpf_neural_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-149" id="summaryabstract-149">Summary/Abstract</a></h1>
<div>Significance
            Language is a quintessentially human ability. Research has long probed the functional architecture of language in the mind and brain using diverse neuroimaging, behavioral, and computational modeling approaches. However, adequate neurally-mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report a first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurements—providing computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the brain.
          , 
            The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional {MRI} and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schulman_proximal_2017" class=ref>
<summary class=citation>
<a id="schulman_proximal_2017">[schulman_proximal_2017]</a> - Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg - <a href="http://arxiv.org/abs/1707.06347" target="_blank"><cite>Proximal Policy Optimization Algorithms</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite schulman_proximal_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-150" id="summaryabstract-150">Summary/Abstract</a></h1>
<div>We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schwarz_progress_2018" class=ref>
<summary class=citation>
<a id="schwarz_progress_2018">[schwarz_progress_2018]</a> - Schwarz, Jonathan and Luketina, Jelena and Czarnecki, Wojciech M. and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia - <a href="http://arxiv.org/abs/1805.06370" target="_blank"><cite>Progress \&amp; Compress: A scalable framework for continual learning</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite schwarz_progress_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-151" id="summaryabstract-151">Summary/Abstract</a></h1>
<div>We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress \&amp; compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sculley_hidden_2015" class=ref>
<summary class=citation>
<a id="sculley_hidden_2015">[sculley_hidden_2015]</a> - Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan - <a href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf" target="_blank"><cite>Hidden Technical Debt in Machine Learning Systems</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite sculley_hidden_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-152" id="summaryabstract-152">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sculley_machine_2014" class=ref>
<summary class=citation>
<a id="sculley_machine_2014">[sculley_machine_2014]</a> - Sculley, D. and Holt, Gary and Golovin, D. and Davydov, Eugene and Phillips, Todd and Ebner, D. and Chaudhary, Vinay and Young, M. - <a href="/paper/Machine-Learning%3A-The-High-Interest-Credit-Card-of-Sculley-Holt/51891710e30da33c4ced4ae7daee1593e0cb5cc4" target="_blank"><cite>Machine Learning: The High Interest Credit Card of Technical Debt</cite></a>. - 2014. -
<button onclick="copyToClipboard('\{\{ #cite sculley_machine_2014 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-153" id="summaryabstract-153">Summary/Abstract</a></h1>
<div>Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening {APIs}, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of “glue code” or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sculley_machine_2014-1" class=ref>
<summary class=citation>
<a id="sculley_machine_2014-1">[sculley_machine_2014-1]</a> - Sculley, D. and Holt, Gary and Golovin, D. and Davydov, Eugene and Phillips, Todd and Ebner, D. and Chaudhary, Vinay and Young, M. - <a href="/paper/Machine-Learning%3A-The-High-Interest-Credit-Card-of-Sculley-Holt/51891710e30da33c4ced4ae7daee1593e0cb5cc4" target="_blank"><cite>Machine Learning: The High Interest Credit Card of Technical Debt</cite></a>. - 2014. -
<button onclick="copyToClipboard('\{\{ #cite sculley_machine_2014-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-154" id="summaryabstract-154">Summary/Abstract</a></h1>
<div>Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening {APIs}, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of “glue code” or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sculley_machine_2014-2" class=ref>
<summary class=citation>
<a id="sculley_machine_2014-2">[sculley_machine_2014-2]</a> - Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael - <cite>Machine Learning: The High Interest Credit Card of Technical Debt</cite>. - 2014. -
<button onclick="copyToClipboard('\{\{ #cite sculley_machine_2014-2 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-155" id="summaryabstract-155">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sejnowski_deep_2018" class=ref>
<summary class=citation>
<a id="sejnowski_deep_2018">[sejnowski_deep_2018]</a> - Sejnowski, Terrence J. - <cite>The Deep Learning Revolution</cite>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite sejnowski_deep_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-156" id="summaryabstract-156">Summary/Abstract</a></h1>
<div>How deep learning—from Google Translate to driverless cars to personal cognitive assistants—is changing our lives and transforming every sector of the economy.The deep learning revolution has brought us driverless cars, the greatly improved Google Translate, fluent conversations with Siri and Alexa, and enormous profits from automated trading on the New York Stock Exchange. Deep learning networks can play poker better than professional poker players and defeat a world champion at Go. In this book, Terry Sejnowski explains how deep learning went from being an arcane academic field to a disruptive technology in the information economy.Sejnowski played an important role in the founding of deep learning, as one of a small group of researchers in the 1980s who challenged the prevailing logic-and-symbol based version of {AI}. The new version of {AI} Sejnowski and others developed, which became deep learning, is fueled instead by data. Deep networks learn from data in the same way that babies experience the world, starting with fresh eyes and gradually acquiring the skills needed to navigate novel environments. Learning algorithms extract information from raw data; information can be used to create knowledge; knowledge underlies understanding; understanding leads to wisdom. Someday a driverless car will know the road better than you do and drive with more skill; a deep learning network will diagnose your illness; a personal cognitive assistant will augment your puny human brain. It took nature many millions of years to evolve human intelligence; {AI} is on a trajectory measured in decades. Sejnowski prepares us for a deep learning future.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shaw_self-attention_2018" class=ref>
<summary class=citation>
<a id="shaw_self-attention_2018">[shaw_self-attention_2018]</a> - Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish - <a href="http://arxiv.org/abs/1803.02155" target="_blank"><cite>Self-Attention with Relative Position Representations</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite shaw_self-attention_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-157" id="summaryabstract-157">Summary/Abstract</a></h1>
<div>Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the {WMT} 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 {BLEU} and 0.3 {BLEU} over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shin_continual_2017" class=ref>
<summary class=citation>
<a id="shin_continual_2017">[shin_continual_2017]</a> - Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon - <a href="http://arxiv.org/abs/1705.08690" target="_blank"><cite>Continual Learning with Deep Generative Replay</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite shin_continual_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-158" id="summaryabstract-158">Summary/Abstract</a></h1>
<div>Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (generator) and a task solving model (solver). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shinn_reflexion_2023" class=ref>
<summary class=citation>
<a id="shinn_reflexion_2023">[shinn_reflexion_2023]</a> - Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu - <a href="http://arxiv.org/abs/2303.11366" target="_blank"><cite>Reflexion: Language Agents with Verbal Reinforcement Learning</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite shinn_reflexion_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-159" id="summaryabstract-159">Summary/Abstract</a></h1>
<div>Recent advancements in decision-making large language model ({LLM}) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model ﬁne-tuning, external model ﬁne-tuning, or policy optimization over a deﬁned state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-deﬁned state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, speciﬁcally the ability to learn from mistakes. Self-reﬂection allows humans to efﬁciently solve novel problems through a process of trial and error. Building on recent research, we propose Reﬂexion, an approach that endows an agent with dynamic memory and self-reﬂection capabilities to enhance its existing reasoning trace and task-speciﬁc action choice abilities. To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment. To assess our approach, we evaluate the agent’s ability to complete decision-making tasks in {AlfWorld} environments and knowledge-intensive, search-based question-and-answer tasks in {HotPotQA} environments. We observe success rates of 97\% and 51\%, respectively, and provide a discussion on the emergent property of self-reﬂection.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shoeybi_megatron-lm:_2019" class=ref>
<summary class=citation>
<a id="shoeybi_megatron-lm:_2019">[shoeybi_megatron-lm:_2019]</a> - Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and {LeGresley}, Patrick and Casper, Jared and Catanzaro, Bryan - <a href="https://arxiv.org/abs/1909.08053v3" target="_blank"><cite>Megatron-{LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite shoeybi_megatron-lm:_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-160" id="summaryabstract-160">Summary/Abstract</a></h1>
<div>Recent work in unsupervised language modeling demonstrates that training
large neural language models advances the state of the art in Natural Language
Processing applications. However, for very large models, memory constraints
limit the size of models that can be practically trained. Model parallelism
allows us to train larger models, because the parameters can be split across
multiple processors. In this work, we implement a simple, efficient intra-layer
model parallel approach that enables training state of the art transformer
language models with billions of parameters. Our approach does not require a
new compiler or library changes, is orthogonal and complimentary to pipeline
model parallelism, and can be fully implemented with the insertion of a few
communication operations in native {PyTorch}. We illustrate this approach by
converging an 8.3 billion parameter transformer language model using 512 {GPUs},
making it the largest transformer model ever trained at 24x times the size of
{BERT} and 5.6x times the size of {GPT}-2. We sustain up to 15.1 {PetaFLOPs} per
second across the entire application with 76\% scaling efficiency, compared to a
strong single processor baseline that sustains 39 {TeraFLOPs} per second, which
is 30\% of peak {FLOPs}. The model is trained on 174GB of text, requiring 12
{ZettaFLOPs} over 9.2 days to converge. Transferring this language model achieves
state of the art ({SOTA}) results on the {WikiText}103 (10.8 compared to {SOTA}
perplexity of 16.4) and {LAMBADA} (66.5\% compared to {SOTA} accuracy of 63.2\%)
datasets. We release training and evaluation code, as well as the weights of
our smaller portable model, for reproducibility.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sukhbaatar_adaptive_2019" class=ref>
<summary class=citation>
<a id="sukhbaatar_adaptive_2019">[sukhbaatar_adaptive_2019]</a> - Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand - <a href="http://arxiv.org/abs/1905.07799" target="_blank"><cite>Adaptive Attention Span in Transformers</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite sukhbaatar_adaptive_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-161" id="summaryabstract-161">Summary/Abstract</a></h1>
<div>We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sumers_cognitive_2024" class=ref>
<summary class=citation>
<a id="sumers_cognitive_2024">[sumers_cognitive_2024]</a> - Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L. - <a href="http://arxiv.org/abs/2309.02427" target="_blank"><cite>Cognitive Architectures for Language Agents</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite sumers_cognitive_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-162" id="summaryabstract-162">Summary/Abstract</a></h1>
<div>Recent efforts have augmented large language models ({LLMs}) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents ({CoALA}). {CoALA} describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decisionmaking process to choose actions. We use {CoALA} to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, {CoALA} contextualizes today’s language agents within the broader history of {AI} and outlines a path towards language-based general intelligence.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sun_how_2020" class=ref>
<summary class=citation>
<a id="sun_how_2020">[sun_how_2020]</a> - Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing - <a href="http://arxiv.org/abs/1905.05583" target="_blank"><cite>How to Fine-Tune {BERT} for Text Classification?</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite sun_how_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-163" id="summaryabstract-163">Summary/Abstract</a></h1>
<div>Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, {BERT} (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of {BERT} on text classification task and provide a general solution for {BERT} fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sundareswaran_survey_2020" class=ref>
<summary class=citation>
<a id="sundareswaran_survey_2020">[sundareswaran_survey_2020]</a> - Sundareswaran, Veena and Shankari, T and Sowmiya, Senthil and Varsha, Mundhra - <cite>A {SURVEY} {ON} {TOOLS} {USED} {FOR} {MACHINE} {LEARNING}</cite>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite sundareswaran_survey_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-164" id="summaryabstract-164">Summary/Abstract</a></h1>
<div>In this paper, a brief introduction to Machine Learning and its Tools are studied. In the recent developments, most of the Machine learning tools are more advanced and efficient. The various tools learn the machine by using a training set, which predicts the output correctly and efficiently. Machine Learning is applied in different applications such as Agriculture, Data Quality, Information Retrieval, Financial Market Analysis etc.., In this paper, we have discussed few tools like Scikit learn, Pytorch, Tensor flow, Amazon Machine Learning, {KNIME}, Rapid Miner, Keras, and Shogun with its features and its advantages.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sutton_reinforcement_2018" class=ref>
<summary class=citation>
<a id="sutton_reinforcement_2018">[sutton_reinforcement_2018]</a> - Sutton, Richard S. and Barto, Andrew G. - <cite>Reinforcement Learning, second edition: An Introduction</cite>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite sutton_reinforcement_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-165" id="summaryabstract-165">Summary/Abstract</a></h1>
<div>The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field&#x27;s key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including {UCB}, Expected Sarsa, and Double Learning. Part {II} extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part {III} has new chapters on reinforcement learning&#x27;s relationships to psychology and neuroscience, as well as an updated case-studies chapter including {AlphaGo} and {AlphaGo} Zero, Atari game playing, and {IBM} Watson&#x27;s wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sutton_reinforcement_1998" class=ref>
<summary class=citation>
<a id="sutton_reinforcement_1998">[sutton_reinforcement_1998]</a> - Sutton, Richard S. and Barto, Andrew G. - <cite>Reinforcement learning: an introduction</cite>. - 1998. -
<button onclick="copyToClipboard('\{\{ #cite sutton_reinforcement_1998 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-166" id="summaryabstract-166">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="synced_google_2019" class=ref>
<summary class=citation>
<a id="synced_google_2019">[synced_google_2019]</a> - Synced - <a href="https://medium.com/syncedreview/google-t5-explores-the-limits-of-transfer-learning-a87afbf2615b" target="_blank"><cite>Google T5 Explores the Limits of Transfer Learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite synced_google_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-167" id="summaryabstract-167">Summary/Abstract</a></h1>
<div>A Google research team recently published the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tan_survey_2018" class=ref>
<summary class=citation>
<a id="tan_survey_2018">[tan_survey_2018]</a> - Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang - <a href="http://arxiv.org/abs/1808.01974" target="_blank"><cite>A Survey on Deep Transfer Learning</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite tan_survey_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-168" id="summaryabstract-168">Summary/Abstract</a></h1>
<div>As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tavakoli_prioritizing_2019" class=ref>
<summary class=citation>
<a id="tavakoli_prioritizing_2019">[tavakoli_prioritizing_2019]</a> - Tavakoli, Arash and Levdik, Vitaly and Islam, Riashat and Kormushev, Petar - <a href="http://arxiv.org/abs/1811.11298" target="_blank"><cite>Prioritizing Starting States for Reinforcement Learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite tavakoli_prioritizing_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-169" id="summaryabstract-169">Summary/Abstract</a></h1>
<div>Online, off-policy reinforcement learning algorithms are able to use an experience memory to remember and replay past experiences. In prior work, this approach was used to stabilize training by breaking the temporal correlations of the updates and avoiding the rapid forgetting of possibly rare experiences. In this work, we propose a conceptually simple framework that uses an experience memory to help exploration by prioritizing the starting states from which the agent starts acting in the environment, importantly, in a fashion that is also compatible with on-policy algorithms. Given the capacity to restart the agent in states corresponding to its past observations, we achieve this objective by (i) enabling the agent to restart in states belonging to significant past experiences (e.g., nearby goals), and (ii) promoting faster coverage of the state space through starting from a more diverse set of states. While, using a good priority measure to identify significant past transitions, we expect case (i) to more considerably help exploration in certain domains (e.g., sparse reward tasks), we hypothesize that case (ii) will generally be beneficial, even without any prioritization. We show empirically that our approach improves learning performance for both off-policy and on-policy deep reinforcement learning methods, with most notable gains in highly sparse reward tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tj_idles_2024" class=ref>
<summary class=citation>
<a id="tj_idles_2024">[tj_idles_2024]</a> - {TJ} - <a href="https://www.youtube.com/watch?v&#x3D;POrZqZSNzes" target="_blank"><cite>{IDLES} Live The Warfield, San Francisco {CA} 2024-05-11 [Full Show]</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite tj_idles_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-170" id="summaryabstract-170">Summary/Abstract</a></h1>
<div>{IDLES} - Live
2024-05-11
The Warfield
San Francisco, {CA}
<p>Setlist:</p>
<p>{IDEA} 01
Colossus
Gift Horse
Mr. Motivator
Mother
Car Crash
I'm Scum
1049 Gotho
The Wheel
Jungle
War
Wizz
Benzocaine
Grounds
Gratitude
Divide and Conquer
{POP} {POP} {POP}
Samaritans
Crawl!
The Beachland Ballroom
Never Fight a Man With a Perm
Dancer
Danny Nedelko
Rottweiler</p>
<p>Support {IDLES}!! And Free Palestine!</p>
<p>Official: https://www.idlesband.com
{IG}:   / idlesband  
{FB}:   / idlesband  
Bandcamp: https://idlesband.bandcamp.com/merch
Spotify: https://open.spotify.com/artist/75maf...</div></p>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tsuda_modeling_2020" class=ref>
<summary class=citation>
<a id="tsuda_modeling_2020">[tsuda_modeling_2020]</a> - Tsuda, Ben and Tye, Kay M. and Siegelmann, Hava T. and Sejnowski, Terrence J. - <a href="https://www.pnas.org/content/117/47/29872" target="_blank"><cite>A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite tsuda_modeling_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-171" id="summaryabstract-171">Summary/Abstract</a></h1>
<div>The prefrontal cortex encodes and stores numerous, often disparate, schemas and flexibly switches between them. Recent research on artificial neural networks trained by reinforcement learning has made it possible to model fundamental processes underlying schema encoding and storage. Yet how the brain is able to create new schemas while preserving and utilizing old schemas remains unclear. Here we propose a simple neural network framework that incorporates hierarchical gating to model the prefrontal cortex’s ability to flexibly encode and use multiple disparate schemas. We show how gating naturally leads to transfer learning and robust memory savings. We then show how neuropsychological impairments observed in patients with prefrontal damage are mimicked by lesions of our network. Our architecture, which we call {DynaMoE}, provides a fundamental framework for how the prefrontal cortex may handle the abundance of schemas necessary to navigate the real world.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tunstall_efficient_2022" class=ref>
<summary class=citation>
<a id="tunstall_efficient_2022">[tunstall_efficient_2022]</a> - Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren - <a href="http://arxiv.org/abs/2209.11055" target="_blank"><cite>Efficient Few-Shot Learning Without Prompts</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite tunstall_efficient_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-172" id="summaryabstract-172">Summary/Abstract</a></h1>
<div>Recent few-shot methods, such as parameter-efficient fine-tuning ({PEFT}) and pattern exploiting training ({PET}), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose {SetFit} (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers ({ST}). {SetFit} works by first fine-tuning a pretrained {ST} on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that {SetFit} obtains comparable results with {PEFT} and {PET} techniques, while being an order of magnitude faster to train. We also show that {SetFit} can be applied in multilingual settings by simply switching the {ST} body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tunstall_efficient_2022-1" class=ref>
<summary class=citation>
<a id="tunstall_efficient_2022-1">[tunstall_efficient_2022-1]</a> - Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren - <a href="http://arxiv.org/abs/2209.11055" target="_blank"><cite>Efficient Few-Shot Learning Without Prompts</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite tunstall_efficient_2022-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-173" id="summaryabstract-173">Summary/Abstract</a></h1>
<div>Recent few-shot methods, such as parameter-efficient fine-tuning ({PEFT}) and pattern exploiting training ({PET}), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose {SetFit} (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers ({ST}). {SetFit} works by first fine-tuning a pretrained {ST} on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that {SetFit} obtains comparable results with {PEFT} and {PET} techniques, while being an order of magnitude faster to train. We also show that {SetFit} can be applied in multilingual settings by simply switching the {ST} body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tversky_mind_2019" class=ref>
<summary class=citation>
<a id="tversky_mind_2019">[tversky_mind_2019]</a> - Tversky, Barbara - <cite>Mind in Motion: How Action Shapes Thought</cite>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite tversky_mind_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-174" id="summaryabstract-174">Summary/Abstract</a></h1>
<div>An eminent psychologist offers a major new theory of human cognition: movement, not language, is the foundation of {thoughtWhen} we try to think about how we think, we can&#x27;t help but think of words. Indeed, some have called language the stuff of thought. But pictures are remembered far better than words, and describing faces, scenes, and events defies words. Anytime you take a shortcut or play chess or basketball or rearrange your furniture in your mind, you&#x27;ve done something remarkable: abstract thinking without words. In Mind in Motion, psychologist Barbara Tversky shows that spatial cognition isn&#x27;t just a peripheral aspect of thought, but its very foundation, enabling us to draw meaning from our bodies and their actions in the world. Our actions in real space get turned into mental actions on thought, often spouting spontaneously from our bodies as gestures. Spatial thinking underlies creating and using maps, assembling furniture, devising football strategies, designing airports, understanding the flow of people, traffic, water, and ideas. Spatial thinking even underlies the structure and meaning of language: why we say we push ideas forward or tear them apart, why we&#x27;re feeling up or have grown far apart. Like Thinking, Fast and Slow before it, Mind in Motion gives us a new way to think about how--and where--thinking takes place.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_three_2019" class=ref>
<summary class=citation>
<a id="van_de_ven_three_2019">[van_de_ven_three_2019]</a> - van de Ven, Gido M. and Tolias, Andreas S. - <a href="http://arxiv.org/abs/1904.07734" target="_blank"><cite>Three scenarios for continual learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_three_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-175" id="summaryabstract-175">Summary/Abstract</a></h1>
<div>Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and–in case it is not–whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted {MNIST} task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_generative_2019" class=ref>
<summary class=citation>
<a id="van_de_ven_generative_2019">[van_de_ven_generative_2019]</a> - van de Ven, Gido M. and Tolias, Andreas S. - <a href="http://arxiv.org/abs/1809.10635" target="_blank"><cite>Generative replay with feedback connections as a general strategy for continual learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_generative_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-176" id="summaryabstract-176">Summary/Abstract</a></h1>
<div>A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted {MNIST} task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as soft targets) achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_brain-inspired_2020" class=ref>
<summary class=citation>
<a id="van_de_ven_brain-inspired_2020">[van_de_ven_brain-inspired_2020]</a> - van de Ven, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S. - <a href="https://www.nature.com/articles/s41467-020-17866-2" target="_blank"><cite>Brain-inspired replay for continual learning with artificial neural networks</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_brain-inspired_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-177" id="summaryabstract-177">Summary/Abstract</a></h1>
<div>Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as ‘generative replay’, which can successfully – and surprisingly efficiently – prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network’s own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on {CIFAR}-100) without storing data, and it provides a novel model for replay in the brain.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_hasselt_deep_2015" class=ref>
<summary class=citation>
<a id="van_hasselt_deep_2015">[van_hasselt_deep_2015]</a> - van Hasselt, Hado and Guez, Arthur and Silver, David - <a href="http://arxiv.org/abs/1509.06461" target="_blank"><cite>Deep Reinforcement Learning with Double Q-learning</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite van_hasselt_deep_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-178" id="summaryabstract-178">Summary/Abstract</a></h1>
<div>The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="vaswani_attention_2017" class=ref>
<summary class=citation>
<a id="vaswani_attention_2017">[vaswani_attention_2017]</a> - Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia - <a href="http://arxiv.org/abs/1706.03762" target="_blank"><cite>Attention Is All You Need</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite vaswani_attention_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-179" id="summaryabstract-179">Summary/Abstract</a></h1>
<div>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="venkatesan_strategy_2017" class=ref>
<summary class=citation>
<a id="venkatesan_strategy_2017">[venkatesan_strategy_2017]</a> - Venkatesan, Ragav and Venkateswara, Hemanth and Panchanathan, Sethuraman and Li, Baoxin - <a href="http://arxiv.org/abs/1705.00744" target="_blank"><cite>A Strategy for an Uncompromising Incremental Learner</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite venkatesan_strategy_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-180" id="summaryabstract-180">Summary/Abstract</a></h1>
<div>Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these hacks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique, phantom sampling.We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets and through our strategy, we demonstrate that strict incremental learning could be achieved. We further put our strategy to test on challenging cases, including cross-domain increments and incrementing on a novel label space. We also propose a trivial extension to unbounded-continual learning and identify potential for future development.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_chain--thought_2024" class=ref>
<summary class=citation>
<a id="wang_chain--thought_2024">[wang_chain--thought_2024]</a> - Wang, Xuezhi and Zhou, Denny - <a href="http://arxiv.org/abs/2402.10200" target="_blank"><cite>Chain-of-Thought Reasoning Without Prompting</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite wang_chain--thought_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-181" id="summaryabstract-181">Summary/Abstract</a></h1>
<div>In enhancing the reasoning capabilities of large language models ({LLMs}), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought ({CoT}) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can {LLMs} reason effectively without prompting? Our findings reveal that, intriguingly, {CoT} reasoning paths can be elicited from pre-trained {LLMs} by simply altering the {\textbackslash}textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that {CoT} paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the {LLMs}&#x27; {\textbackslash}textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a {CoT} in the decoding path correlates with a higher confidence in the model&#x27;s decoded answer. This confidence metric effectively differentiates between {CoT} and non-{CoT} paths. Extensive empirical studies on various reasoning benchmarks show that the proposed {CoT}-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_dueling_2016" class=ref>
<summary class=citation>
<a id="wang_dueling_2016">[wang_dueling_2016]</a> - Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando - <a href="http://arxiv.org/abs/1511.06581" target="_blank"><cite>Dueling Network Architectures for Deep Reinforcement Learning</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite wang_dueling_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-182" id="summaryabstract-182">Summary/Abstract</a></h1>
<div>In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, {LSTMs}, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our {RL} agent to outperform the state-of-the-art on the Atari 2600 domain.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_glue_2019" class=ref>
<summary class=citation>
<a id="wang_glue_2019">[wang_glue_2019]</a> - Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R. - <a href="http://arxiv.org/abs/1804.07461" target="_blank"><cite>{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite wang_glue_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-183" id="summaryabstract-183">Summary/Abstract</a></h1>
<div>For natural language understanding ({NLU}) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark ({GLUE}), a tool for evaluating and analyzing the performance of models across a diverse range of existing {NLU} tasks. {GLUE} is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of {NLU} models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust {NLU} systems.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_generalizing_2020" class=ref>
<summary class=citation>
<a id="wang_generalizing_2020">[wang_generalizing_2020]</a> - Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M. - <a href="http://arxiv.org/abs/1904.05046" target="_blank"><cite>Generalizing from a Few Examples: A Survey on Few-Shot Learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wang_generalizing_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-184" id="summaryabstract-184">Summary/Abstract</a></h1>
<div>Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning ({FSL}) is proposed to tackle this problem. Using prior knowledge, {FSL} can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand {FSL}. Starting from a formal definition of {FSL}, we distinguish {FSL} from several relevant machine learning problems. We then point out that the core issue in {FSL} is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize {FSL} methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the {FSL} problem setups, techniques, applications and theories, are also proposed to provide insights for future research.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_cordel_2020" class=ref>
<summary class=citation>
<a id="wang_cordel_2020">[wang_cordel_2020]</a> - Wang, Zhengyang and Sisman, Bunyamin and Wei, Hao and Dong, Xin Luna and Ji, Shuiwang - <a href="https://ieeexplore.ieee.org/document/9338287/" target="_blank"><cite>{CorDEL}: A Contrastive Deep Learning Approach for Entity Linkage</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wang_cordel_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-185" id="summaryabstract-185">Summary/Abstract</a></h1>
<div>Entity linkage ({EL}) is a critical problem in data cleaning and integration. In the past several decades, {EL} has typically been done by rule-based systems or traditional machine learning models with hand-curated features, both of which heavily depend on manual human inputs. With the ever-increasing growth of new data, deep learning ({DL}) based approaches have been proposed to alleviate the high cost of {EL} associated with the traditional models. Existing exploration of {DL} models for {EL} strictly follows the well-known twin-network architecture. However, we argue that the twin-network architecture is sub-optimal to {EL}, leading to inherent drawbacks of existing models. In order to address the drawbacks, we propose a novel and generic contrastive {DL} framework for {EL}. The proposed framework is able to capture both syntactic and semantic matching signals and pays attention to subtle but critical differences. Based on the framework, we develop a contrastive {DL} approach for {EL}, {CORDEL}, with a simple yet powerful variant called {CORDEL}-Sum. We evaluate {CORDEL} with extensive experiments conducted on both public benchmark datasets and a real-world dataset. {CORDEL} outperforms previous state-of-the-art models by 5.2\% on public benchmark datasets. Moreover, {CORDEL} yields a 29.4\% improvement over the current best {DL} model on the real-world dataset, while reducing the number of training parameters by 96.8\%.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_machamp_2021" class=ref>
<summary class=citation>
<a id="wang_machamp_2021">[wang_machamp_2021]</a> - Wang, Jin and Li, Yuliang and Hirota, Wataru - <a href="http://arxiv.org/abs/2106.08455" target="_blank"><cite>Machamp: A Generalized Entity Matching Benchmark</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite wang_machamp_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-186" id="summaryabstract-186">Summary/Abstract</a></h1>
<div>Entity Matching ({EM}) refers to the problem of determining whether two different data representations refer to the same real-world entity. It has been a long-standing interest of the data management community and many efforts have been paid in creating benchmark tasks as well as in developing advanced matching techniques. However, existing benchmark tasks for {EM} are limited to the case where the two data collections of entities are structured tables with the same schema. Meanwhile, the data collections for matching could be structured, semi-structured, or unstructured in real-world scenarios of data science. In this paper, we come up with a new research problem -- Generalized Entity Matching to satisfy this requirement and create a benchmark Machamp for it. Machamp consists of seven tasks having diverse characteristics and thus provides good coverage of use cases in real applications. We summarize existing {EM} benchmark tasks for structured tables and conduct a series of processing and cleaning efforts to transform them into matching tasks between tables with different structures. Based on that, we further conduct comprehensive profiling of the proposed benchmark tasks and evaluate popular entity matching approaches on them. With the help of Machamp, it is the first time that researchers can evaluate {EM} techniques between data collections with different structures.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_machamp_2021-1" class=ref>
<summary class=citation>
<a id="wang_machamp_2021-1">[wang_machamp_2021-1]</a> - Wang, Jin and Li, Yuliang and Hirota, Wataru - <a href="http://arxiv.org/abs/2106.08455" target="_blank"><cite>Machamp: A Generalized Entity Matching Benchmark</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite wang_machamp_2021-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-187" id="summaryabstract-187">Summary/Abstract</a></h1>
<div>Entity Matching ({EM}) refers to the problem of determining whether two different data representations refer to the same real-world entity. It has been a long-standing interest of the data management community and many efforts have been paid in creating benchmark tasks as well as in developing advanced matching techniques. However, existing benchmark tasks for {EM} are limited to the case where the two data collections of entities are structured tables with the same schema. Meanwhile, the data collections for matching could be structured, semi-structured, or unstructured in real-world scenarios of data science. In this paper, we come up with a new research problem -- Generalized Entity Matching to satisfy this requirement and create a benchmark Machamp for it. Machamp consists of seven tasks having diverse characteristics and thus provides good coverage of use cases in real applications. We summarize existing {EM} benchmark tasks for structured tables and conduct a series of processing and cleaning efforts to transform them into matching tasks between tables with different structures. Based on that, we further conduct comprehensive profiling of the proposed benchmark tasks and evaluate popular entity matching approaches on them. With the help of Machamp, it is the first time that researchers can evaluate {EM} techniques between data collections with different structures.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_linformer_2020" class=ref>
<summary class=citation>
<a id="wang_linformer_2020">[wang_linformer_2020]</a> - Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao - <a href="http://arxiv.org/abs/2006.04768" target="_blank"><cite>Linformer: Self-Attention with Linear Complexity</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wang_linformer_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-188" id="summaryabstract-188">Summary/Abstract</a></h1>
<div>Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_self-consistency_2023" class=ref>
<summary class=citation>
<a id="wang_self-consistency_2023">[wang_self-consistency_2023]</a> - Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny - <a href="http://arxiv.org/abs/2203.11171" target="_blank"><cite>Self-Consistency Improves Chain of Thought Reasoning in Language Models</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite wang_self-consistency_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-189" id="summaryabstract-189">Summary/Abstract</a></h1>
<div>Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It ﬁrst samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including {GSM}8K (+17.9\%), {SVAMP} (+11.0\%), {AQuA} (+12.2\%), {StrategyQA} (+6.4\%) and {ARC}-challenge (+3.9\%).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wei_nezha_2019" class=ref>
<summary class=citation>
<a id="wei_nezha_2019">[wei_nezha_2019]</a> - Wei, Junqiu and Ren, Xiaozhe and Li, Xiaoguang and Huang, Wenyong and Liao, Yi and Wang, Yasheng and Lin, Jiashu and Jiang, Xin and Chen, Xiao and Liu, Qun - <a href="http://arxiv.org/abs/1909.00204" target="_blank"><cite>{NEZHA}: Neural Contextualized Representation for Chinese Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite wei_nezha_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-190" id="summaryabstract-190">Summary/Abstract</a></h1>
<div>The pre-trained language models have achieved great successes in various natural language understanding ({NLU}) tasks due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora. In this technical report, we present our practice of pre-training language models named {NEZHA} ({NEural} {contextualiZed} representation for {CHinese} {lAnguage} understanding) on Chinese corpora and finetuning for the Chinese {NLU} tasks. The current version of {NEZHA} is based on {BERT} with a collection of proven improvements, which include Functional Relative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy, Mixed Precision Training and the {LAMB} Optimizer in training the models. The experimental results show that {NEZHA} achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including named entity recognition (People&#x27;s Daily {NER}), sentence matching ({LCQMC}), Chinese sentiment classification ({ChnSenti}) and natural language inference ({XNLI}).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="weizenbaum_elizacomputer_1966" class=ref>
<summary class=citation>
<a id="weizenbaum_elizacomputer_1966">[weizenbaum_elizacomputer_1966]</a> - Weizenbaum, Joseph - <a href="https://dl.acm.org/doi/10.1145/365153.365168" target="_blank"><cite>{ELIZA}—a computer program for the study of natural language communication between man and machine</cite></a>. - 1966. -
<button onclick="copyToClipboard('\{\{ #cite weizenbaum_elizacomputer_1966 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-191" id="summaryabstract-191">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wolfram_class_2020" class=ref>
<summary class=citation>
<a id="wolfram_class_2020">[wolfram_class_2020]</a> - Wolfram, Stephen - <a href="http://arxiv.org/abs/2004.08210" target="_blank"><cite>A Class of Models with the Potential to Represent Fundamental Physics</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wolfram_class_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-192" id="summaryabstract-192">Summary/Abstract</a></h1>
<div>A class of models intended to be as minimal and structureless as possible is introduced. Even in cases with simple rules, rich and complex behavior is found to emerge, and striking correspondences to some important core known features of fundamental physics are seen, suggesting the possibility that the models may provide a new approach to finding a fundamental theory of physics.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wooldridge_brief_2021" class=ref>
<summary class=citation>
<a id="wooldridge_brief_2021">[wooldridge_brief_2021]</a> - Wooldridge, Michael - <cite>A Brief History of Artificial Intelligence: What It Is, Where We Are, and Where We Are Going</cite>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite wooldridge_brief_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-193" id="summaryabstract-193">Summary/Abstract</a></h1>
<div>From Oxford&#x27;s leading {AI} researcher comes a fun and accessible tour through the history and future of one of the most cutting edge and misunderstood field in science: Artificial {IntelligenceThe} somewhat ill-defined long-term aim of {AI} is to build machines that are conscious, self-aware, and sentient; machines capable of the kind of intelligent autonomous action that currently only people are capable of. As an {AI} researcher with 25 years of experience, professor Mike Wooldridge has learned to be obsessively cautious about such claims, while still promoting an intense optimism about the future of the field. There have been genuine scientific breakthroughs that have made {AI} systems possible in the past decade that the founders of the field would have hailed as miraculous. Driverless cars and automated translation tools are just two examples of {AI} technologies that have become a practical, everyday reality in the past few years, and which will have a huge impact on our world.While the dream of conscious machines remains, Professor Wooldridge believes, a distant prospect, the floodgates for {AI} have opened. Wooldridge&#x27;s A Brief History of Artificial Intelligence is an exciting romp through the history of this groundbreaking field--a one-stop-shop for {AI}&#x27;s past, present, and world-changing future.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wu_group_2018" class=ref>
<summary class=citation>
<a id="wu_group_2018">[wu_group_2018]</a> - Wu, Yuxin and He, Kaiming - <a href="http://arxiv.org/abs/1803.08494" target="_blank"><cite>Group Normalization</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite wu_group_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-194" id="summaryabstract-194">Summary/Abstract</a></h1>
<div>Batch Normalization ({BN}) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- {BN}&#x27;s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits {BN}&#x27;s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization ({GN}) as a simple alternative to {BN}. {GN} divides the channels into groups and computes within each group the mean and variance for normalization. {GN}&#x27;s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On {ResNet}-50 trained in {ImageNet}, {GN} has 10.6\% lower error than its {BN} counterpart when using a batch size of 2; when using typical batch sizes, {GN} is comparably good with {BN} and outperforms other normalization variants. Moreover, {GN} can be naturally transferred from pre-training to fine-tuning. {GN} can outperform its {BN}-based counterparts for object detection and segmentation in {COCO}, and for video classification in Kinetics, showing that {GN} can effectively replace the powerful {BN} in a variety of tasks. {GN} can be easily implemented by a few lines of code in modern libraries.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wu_incremental_2018" class=ref>
<summary class=citation>
<a id="wu_incremental_2018">[wu_incremental_2018]</a> - Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Zhang, Zhengyou and Fu, Yun - <a href="http://arxiv.org/abs/1802.00853" target="_blank"><cite>Incremental Classifier Learning with Generative Adversarial Networks</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite wu_incremental_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-195" id="summaryabstract-195">Summary/Abstract</a></h1>
<div>In this paper, we address the incremental classifier learning problem, which suffers from catastrophic forgetting. The main reason for catastrophic forgetting is that the past data are not available during learning. Typical approaches keep some exemplars for the past classes and use distillation regularization to retain the classification capability on the past classes and balance the past and new classes. However, there are four main problems with these approaches. First, the loss function is not efficient for classification. Second, there is unbalance problem between the past and new classes. Third, the size of pre-decided exemplars is usually limited and they might not be distinguishable from unseen new classes. Forth, the exemplars may not be allowed to be kept for a long time due to privacy regulations. To address these problems, we propose (a) a new loss function to combine the cross-entropy loss and distillation loss, (b) a simple way to estimate and remove the unbalance between the old and new classes , and (c) using Generative Adversarial Networks ({GANs}) to generate historical data and select representative exemplars during generation. We believe that the data generated by {GANs} have much less privacy issues than real images because {GANs} do not directly copy any real image patches. We evaluate the proposed method on {CIFAR}-100, Flower-102, and {MS}-Celeb-1M-Base datasets and extensive experiments demonstrate the effectiveness of our method.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wu_lite_2020" class=ref>
<summary class=citation>
<a id="wu_lite_2020">[wu_lite_2020]</a> - Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song - <a href="http://arxiv.org/abs/2004.11886" target="_blank"><cite>Lite Transformer with Long-Short Range Attention</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wu_lite_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-196" id="summaryabstract-196">Summary/Abstract</a></h1>
<div>Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile {NLP} architecture, Lite Transformer to facilitate deploying mobile {NLP} applications on edge devices. The key primitive is the Long-Short Range Attention ({LSRA}), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M {MACs}), Lite Transformer outperforms transformer on {WMT}&#x27;14 English-French by 1.2/1.7 {BLEU}, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 {BLEU} score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M {MACs}. Notably, Lite Transformer outperforms the {AutoML}-based Evolved Transformer by 0.5 higher {BLEU} for the mobile {NLP} setting without the costly architecture search that requires more than 250 {GPU} years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="xie_adversarial_nodate" class=ref>
<summary class=citation>
<a id="xie_adversarial_nodate">[xie_adversarial_nodate]</a> - Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V - <cite>Adversarial Examples Improve Image Recognition</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite xie_adversarial_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-197" id="summaryabstract-197">Summary/Abstract</a></h1>
<div>Adversarial examples are commonly viewed as a threat to {ConvNets}. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose {AdvProp}, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overﬁtting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that {AdvProp} improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying {AdvProp} to the latest {EfﬁcientNet}-B7 [28] on {ImageNet}, we achieve signiﬁcant improvements on {ImageNet} (+0.7\%), {ImageNet}-C (+6.5\%), {ImageNet}-A (+7.0\%), {StylizedImageNet} (+4.8\%). With an enhanced {EfﬁcientNet}-B8, our method achieves the state-of-the-art 85.5\% {ImageNet} top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (∼3000× more than {ImageNet}) and ∼9.4× more parameters. Models are available at https://github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="xu_system_2020" class=ref>
<summary class=citation>
<a id="xu_system_2020">[xu_system_2020]</a> - Xu, Alex - <a href="https://books.google.com/books/about/System_Design_Interview.html?id&#x3D;TZWmzQEACAAJ" target="_blank"><cite>System Design Interview: An Insider&#x27;s Guide, Volume 1</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite xu_system_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-198" id="summaryabstract-198">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="yang_finbert_2020" class=ref>
<summary class=citation>
<a id="yang_finbert_2020">[yang_finbert_2020]</a> - Yang, Yi and {UY}, Mark Christopher Siy and Huang, Allen - <a href="http://arxiv.org/abs/2006.08097" target="_blank"><cite>{FinBERT}: A Pretrained Language Model for Financial Communications</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite yang_finbert_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-199" id="summaryabstract-199">Summary/Abstract</a></h1>
<div>Contextual pretrained language models, such as {BERT} (Devlin et al., 2019), have made significant breakthrough in various {NLP} tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific {BERT} models, {FinBERT}, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of {FinBERT} over generic domain {BERT} model. The code and pretrained models are available at https://github.com/yya518/{FinBERT}. We hope this will be useful for practitioners and researchers working on financial {NLP} tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="yao_react_2023" class=ref>
<summary class=citation>
<a id="yao_react_2023">[yao_react_2023]</a> - Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan - <a href="http://arxiv.org/abs/2210.03629" target="_blank"><cite>{ReAct}: Synergizing Reasoning and Acting in Language Models</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite yao_react_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-200" id="summaryabstract-200">Summary/Abstract</a></h1>
<div>While large language models ({LLMs}) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of {LLMs} to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named {ReAct}, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering ({HotpotQA}) and fact verification (Fever), {ReAct} overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia {API}, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks ({ALFWorld} and {WebShop}), {ReAct} outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="yao_webshop_2023" class=ref>
<summary class=citation>
<a id="yao_webshop_2023">[yao_webshop_2023]</a> - Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik - <a href="http://arxiv.org/abs/2207.01206" target="_blank"><cite>{WebShop}: Towards Scalable Real-World Web Interaction with Grounded Language Agents</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite yao_webshop_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-201" id="summaryabstract-201">Summary/Abstract</a></h1>
<div>Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop {WebShop} -- a simulated e-commerce website environment with \$1.18\$ million real-world products and \$12,087\$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. {WebShop} provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over \$1,600\$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of \$29{\textbackslash}\%\$, which outperforms rule-based heuristics (\$9.6{\textbackslash}\%\$) but is far lower than human expert performance (\$59{\textbackslash}\%\$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on {WebShop} exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of {WebShop} in developing practical web-based agents that can operate in the wild.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="yasunaga_large_2024" class=ref>
<summary class=citation>
<a id="yasunaga_large_2024">[yasunaga_large_2024]</a> - Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H. and Zhou, Denny - <a href="http://arxiv.org/abs/2310.01714" target="_blank"><cite>Large Language Models as Analogical Reasoners</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite yasunaga_large_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-202" id="summaryabstract-202">Summary/Abstract</a></h1>
<div>Chain-of-thought ({CoT}) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot {CoT} and manual fewshot {CoT} in a variety of reasoning tasks, including math problem solving in {GSM}8K and {MATH}, code generation in Codeforces, and other reasoning tasks in {BIG}-Bench.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zaheer_big_2020" class=ref>
<summary class=citation>
<a id="zaheer_big_2020">[zaheer_big_2020]</a> - Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr - <a href="http://arxiv.org/abs/2007.14062" target="_blank"><cite>Big Bird: Transformers for Longer Sequences</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zaheer_big_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-203" id="summaryabstract-203">Summary/Abstract</a></h1>
<div>Transformers-based models, such as {BERT}, have been one of the most successful deep learning models for {NLP}. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, {BigBird}, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that {BigBird} is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as {CLS}), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, {BigBird} drastically improves performance on various {NLP} tasks such as question answering and summarization. We also propose novel applications to genomics data.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zelikman_star_2022" class=ref>
<summary class=citation>
<a id="zelikman_star_2022">[zelikman_star_2022]</a> - Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D. - <a href="http://arxiv.org/abs/2203.14465" target="_blank"><cite>{STaR}: Bootstrapping Reasoning With Reasoning</cite></a>. - 2022. -
<button onclick="copyToClipboard('\{\{ #cite zelikman_star_2022 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-204" id="summaryabstract-204">Summary/Abstract</a></h1>
<div>Generating step-by-step chain-of-thought rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the Self-Taught Reasoner ({STaR}), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that {STaR} significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\${\textbackslash}times\$ larger state-of-the-art language model on {CommensenseQA}. Thus, {STaR} lets a model improve itself by learning from its own generated reasoning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zelikman_quiet-star_2024" class=ref>
<summary class=citation>
<a id="zelikman_quiet-star_2024">[zelikman_quiet-star_2024]</a> - Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D. - <a href="http://arxiv.org/abs/2403.09629" target="_blank"><cite>Quiet-{STaR}: Language Models Can Teach Themselves to Think Before Speaking</cite></a>. - 2024. -
<button onclick="copyToClipboard('\{\{ #cite zelikman_quiet-star_2024 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-205" id="summaryabstract-205">Summary/Abstract</a></h1>
<div>When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner ({STaR}, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-{STaR}, a generalization of {STaR} in which {LMs} learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the {LM} does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought&#x27;s start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the {LM}&#x27;s ability to directly answer difficult questions. In particular, after continued pretraining of an {LM} on a corpus of internet text with Quiet-{STaR}, we find zero-shot improvements on {GSM}8K (5.9\%\${\textbackslash}rightarrow\$10.9\%) and {CommonsenseQA} (36.3\%\${\textbackslash}rightarrow\$47.2\%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-{STaR} marks a step towards {LMs} that can learn to reason in a more general and scalable way.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zenke_continual_2017" class=ref>
<summary class=citation>
<a id="zenke_continual_2017">[zenke_continual_2017]</a> - Zenke, Friedemann and Poole, Ben and Ganguli, Surya - <a href="https://arxiv.org/abs/1703.04200v3" target="_blank"><cite>Continual Learning Through Synaptic Intelligence</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite zenke_continual_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-206" id="summaryabstract-206">Summary/Abstract</a></h1>
<div>While deep learning has led to remarkable advances across diverse
applications, it struggles in domains where the data distribution changes over
the course of learning. In stark contrast, biological neural networks
continually adapt to changing domains, possibly by leveraging complex molecular
machinery to solve many tasks simultaneously. In this study, we introduce
intelligent synapses that bring some of this biological complexity into
artificial neural networks. Each synapse accumulates task relevant information
over time, and exploits this information to rapidly store new memories without
forgetting old ones. We evaluate our approach on continual learning of
classification tasks, and show that it dramatically reduces forgetting while
maintaining computational efficiency.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhang_dive_nodate" class=ref>
<summary class=citation>
<a id="zhang_dive_nodate">[zhang_dive_nodate]</a> - Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J - <cite>Dive into Deep Learning</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite zhang_dive_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-207" id="summaryabstract-207">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhao_how_2020" class=ref>
<summary class=citation>
<a id="zhao_how_2020">[zhao_how_2020]</a> - Zhao, Yiyun and Bethard, Steven - <a href="https://www.aclweb.org/anthology/2020.acl-main.429" target="_blank"><cite>How does {BERT}&#x27;s attention change when you fine-tune? An analysis methodology and a case study in negation scope</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zhao_how_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-208" id="summaryabstract-208">Summary/Abstract</a></h1>
<div>Large pretrained language models like {BERT}, after fine-tuning to a downstream task, have achieved high performance on a variety of {NLP} problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test {BERT} and {RoBERTa} on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning {BERT} and {RoBERTa} on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhou_least--most_2023" class=ref>
<summary class=citation>
<a id="zhou_least--most_2023">[zhou_least--most_2023]</a> - Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed - <a href="http://arxiv.org/abs/2205.10625" target="_blank"><cite>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</cite></a>. - 2023. -
<button onclick="copyToClipboard('\{\{ #cite zhou_least--most_2023 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-209" id="summaryabstract-209">Summary/Abstract</a></h1>
<div>Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difﬁcult problems than those seen in the prompts. A notable ﬁnding is that when the {GPT}-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark {SCAN} in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving {SCAN} are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhu_freelb_2019" class=ref>
<summary class=citation>
<a id="zhu_freelb_2019">[zhu_freelb_2019]</a> - Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing - <a href="http://arxiv.org/abs/1909.11764" target="_blank"><cite>{FreeLB}: Enhanced Adversarial Training for Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite zhu_freelb_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-210" id="summaryabstract-210">Summary/Abstract</a></h1>
<div>Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm - {FreeLB}, that promotes higher robustness and invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the {GLUE} benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of {BERT}-based model from 78.3 to 79.4, and {RoBERTa}-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\% and 67.75\% on {ARC}-Easy and {ARC}-Challenge. Experiments on {CommonsenseQA} benchmark further demonstrate that {FreeLB} can be generalized and boost the performance of {RoBERTa}-large model on other tasks as well.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhu_transfer_2020" class=ref>
<summary class=citation>
<a id="zhu_transfer_2020">[zhu_transfer_2020]</a> - Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu - <a href="http://arxiv.org/abs/2009.07888" target="_blank"><cite>Transfer Learning in Deep Reinforcement Learning: A Survey</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zhu_transfer_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-211" id="summaryabstract-211">Summary/Abstract</a></h1>
<div>This paper surveys the field of transfer learning in the problem setting of Reinforcement Learning ({RL}). {RL} has been a key solution to sequential decision-making problems. Along with the fast advances of {RL} in various domains, such as robotics and game-playing, transfer learning arises as an important technique to assist {RL} by leveraging and transferring external expertise to boost the learning process of {RL}. In this survey, we review the central issues of transfer learning in the {RL} domain, providing a systematic categorization of its state-of-the-art techniques. We analyze their goals, methodologies, applications, and the {RL} frameworks under which the transfer learning techniques are approachable. We discuss the relationship between transfer learning and other relevant topics from the {RL} perspective and also explore the potential challenges as well as future development directions for transfer learning in {RL}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhuang_comprehensive_2020" class=ref>
<summary class=citation>
<a id="zhuang_comprehensive_2020">[zhuang_comprehensive_2020]</a> - Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing - <a href="http://arxiv.org/abs/1911.02685" target="_blank"><cite>A Comprehensive Survey on Transfer Learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zhuang_comprehensive_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-212" id="summaryabstract-212">Summary/Abstract</a></h1>
<div>Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_using_nodate" class=ref>
<summary class=citation>
<a id="noauthor_using_nodate">[noauthor_using_nodate]</a> - N/A - <a href="https://www.confluent.io/blog/using-apache-kafka-drive-cutting-edge-machine-learning" target="_blank"><cite>Using Apache Kafka to Drive Cutting-Edge Machine Learning {\textbar} Confluent</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_using_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-213" id="summaryabstract-213">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_enforcing_nodate" class=ref>
<summary class=citation>
<a id="noauthor_enforcing_nodate">[noauthor_enforcing_nodate]</a> - N/A - <a href="https://stackoverflow.com/questions/19534896/enforcing-python-version-in-setup-py" target="_blank"><cite>Enforcing python version in setup.py</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_enforcing_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-214" id="summaryabstract-214">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_improvements_2018" class=ref>
<summary class=citation>
<a id="noauthor_improvements_2018">[noauthor_improvements_2018]</a> - N/A - <a href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" target="_blank"><cite>Improvements in Deep Q Learning: Dueling Double {DQN}, Prioritized Experience Replay, and fixed…</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_improvements_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-215" id="summaryabstract-215">Summary/Abstract</a></h1>
<div>by Thomas Simonini
<p>Improvements in Deep Q Learning: Dueling Double {DQN}, Prioritized Experience
Replay, and fixed Q-targets
{\textgreater} This article is part of Deep Reinforcement Learning Course with Tensorflow ?️.
Check the syllabus here.
[https://simoninithomas.github.io/Deep_reinforcement_learning_Course/]
In our last article about Deep Q Learning with Tensorflow
[https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8]
, we implemented an agent that learns to pla</div></p>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_more_nodate" class=ref>
<summary class=citation>
<a id="noauthor_more_nodate">[noauthor_more_nodate]</a> - N/A - <a href="http://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" target="_blank"><cite>More Efficient {NLP} Model Pre-training with {ELECTRA}</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_more_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-216" id="summaryabstract-216">Summary/Abstract</a></h1>
<div>Posted by Kevin Clark, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team   Recent advances in langu...</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_state_nodate" class=ref>
<summary class=citation>
<a id="noauthor_state_nodate">[noauthor_state_nodate]</a> - N/A - <a href="https://ruder.io/state-of-transfer-learning-in-nlp/" target="_blank"><cite>The State of Transfer Learning in {NLP}</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_state_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-217" id="summaryabstract-217">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_ganfather_nodate" class=ref>
<summary class=citation>
<a id="noauthor_ganfather_nodate">[noauthor_ganfather_nodate]</a> - N/A - <a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/" target="_blank"><cite>The {GANfather}: The man who’s given machines the gift of imagination</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_ganfather_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-218" id="summaryabstract-218">Summary/Abstract</a></h1>
<div>One night in 2014, Ian Goodfellow went drinking to celebrate with a fellow doctoral student who had just graduated. At Les 3 Brasseurs (The Three Brewers), a favorite Montreal watering hole, some friends asked for his help with a thorny project they were working on: a computer that could create photos by itself. Researchers were…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_explainable_nodate" class=ref>
<summary class=citation>
<a id="noauthor_explainable_nodate">[noauthor_explainable_nodate]</a> - N/A - <a href="https://www.darpa.mil/program/explainable-artificial-intelligence" target="_blank"><cite>Explainable Artificial Intelligence</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_explainable_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-219" id="summaryabstract-219">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_introduction_nodate" class=ref>
<summary class=citation>
<a id="noauthor_introduction_nodate">[noauthor_introduction_nodate]</a> - N/A - <a href="https://www.coursera.org/learn/negotiation/home/week/2" target="_blank"><cite>Introduction to Negotiation: A Strategic Playbook for Becoming a Principled and Persuasive Negotiator - Negotiation Caselets</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_introduction_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-220" id="summaryabstract-220">Summary/Abstract</a></h1>
<div>You&#x27;ve got the theory. Now let&#x27;s use it. I&#x27;ll show how the pie framework applies to some mini cases, or caselets. The Merger Case considers how the synergy gains from a merger will be shared by the two parties. While this is still a stylized ...</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_contrastive_nodate" class=ref>
<summary class=citation>
<a id="noauthor_contrastive_nodate">[noauthor_contrastive_nodate]</a> - N/A - <a href="https://www.amazon.science/publications/contrastive-entity-linkage-mining-variational-attributes-from-large-catalogs-for-entity-linkage" target="_blank"><cite>Contrastive entity linkage: Mining variational attributes from large catalogs for entity linkage</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_contrastive_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-221" id="summaryabstract-221">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_collective_nodate" class=ref>
<summary class=citation>
<a id="noauthor_collective_nodate">[noauthor_collective_nodate]</a> - N/A - <a href="https://www.amazon.science/publications/collective-knowledge-graph-multi-type-entity-alignment" target="_blank"><cite>Collective knowledge graph multi-type entity alignment</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_collective_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-222" id="summaryabstract-222">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_notitle_nodate" class=ref>
<summary class=citation>
<a id="noauthor_notitle_nodate">[noauthor_notitle_nodate]</a> - N/A - <cite>Not Found</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_notitle_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-223" id="summaryabstract-223">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_notitle_nodate-1" class=ref>
<summary class=citation>
<a id="noauthor_notitle_nodate-1">[noauthor_notitle_nodate-1]</a> - N/A - <cite>Not Found</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_notitle_nodate-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-224" id="summaryabstract-224">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_papers_nodate" class=ref>
<summary class=citation>
<a id="noauthor_papers_nodate">[noauthor_papers_nodate]</a> - N/A - <a href="https://paperswithcode.com/paper/profiling-entity-matching-benchmark-tasks" target="_blank"><cite>Papers with Code - Profiling Entity Matching Benchmark Tasks</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_papers_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-225" id="summaryabstract-225">Summary/Abstract</a></h1>
<div>\#2 best model for Entity Resolution on Amazon-Google (F1 (\%) metric)</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_red_2021" class=ref>
<summary class=citation>
<a id="noauthor_red_2021">[noauthor_red_2021]</a> - N/A - <a href="https://mattturck.com/data2021/" target="_blank"><cite>Red Hot: The 2021 Machine Learning, {AI} and Data ({MAD}) Landscape</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_red_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-226" id="summaryabstract-226">Summary/Abstract</a></h1>
<div>Full resolution version of the landscape image here
<p>It’s been a hot, hot year in the world of data, machine learning and {AI}. </p>
<p>Just when you thought it couldn’t grow any more explosively, the data/{AI} landscape just did: rapid pace of company creation, exciting new product and project launch</div></p>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_220402311_nodate" class=ref>
<summary class=citation>
<a id="noauthor_220402311_nodate">[noauthor_220402311_nodate]</a> - N/A - <a href="https://arxiv.org/abs/2204.02311" target="_blank"><cite>[2204.02311] {PaLM}: Scaling Language Modeling with Pathways</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_220402311_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-227" id="summaryabstract-227">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_being_nodate" class=ref>
<summary class=citation>
<a id="noauthor_being_nodate">[noauthor_being_nodate]</a> - N/A - <a href="https://www.amazon.com/Being-You-New-Science-Consciousness/dp/1524742872/ref&#x3D;tmm_hrd_swatch_0?_encoding&#x3D;UTF8&amp;qid&#x3D;1695506152&amp;sr&#x3D;1-4" target="_blank"><cite>Being You: A New Science of Consciousness: Seth, Anil: 9781524742874: Amazon.com: Books</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_being_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-228" id="summaryabstract-228">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_experience_nodate" class=ref>
<summary class=citation>
<a id="noauthor_experience_nodate">[noauthor_experience_nodate]</a> - N/A - <a href="https://www.amazon.com/Experience-Machine-Minds-Predict-Reality/dp/1524748455/ref&#x3D;tmm_hrd_swatch_0?_encoding&#x3D;UTF8&amp;qid&#x3D;1695506203&amp;sr&#x3D;1-1" target="_blank"><cite>The Experience Machine: How Our Minds Predict and Shape Reality: Clark, Andy: 9781524748456: Amazon.com: Books</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_experience_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-229" id="summaryabstract-229">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_nebulagraph_nodate" class=ref>
<summary class=citation>
<a id="noauthor_nebulagraph_nodate">[noauthor_nebulagraph_nodate]</a> - N/A - <a href="https://www.nebula-graph.io/posts/graph-RAG" target="_blank"><cite>{NebulaGraph} Launches Industry-First Graph {RAG}: Retrieval-Augmented Generation with {LLM} Based on Knowledge Graphs</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_nebulagraph_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-230" id="summaryabstract-230">Summary/Abstract</a></h1>
<div>{NebulaGraph} database&#x27;s revolutionary Graph {RAG} (Retrieval-Augmented Generation) technique, which combines knowledge graphs with a large language model to provide more cost-effective, intelligent, and precise search results.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_200208909_nodate" class=ref>
<summary class=citation>
<a id="noauthor_200208909_nodate">[noauthor_200208909_nodate]</a> - N/A - <a href="https://arxiv.org/abs/2002.08909" target="_blank"><cite>[2002.08909] {REALM}: Retrieval-Augmented Language Model Pre-Training</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_200208909_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-231" id="summaryabstract-231">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_introducing_nodate" class=ref>
<summary class=citation>
<a id="noauthor_introducing_nodate">[noauthor_introducing_nodate]</a> - N/A - <a href="https://openai.com/index/chatgpt/" target="_blank"><cite>Introducing {ChatGPT} {\textbar} {OpenAI}</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_introducing_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-232" id="summaryabstract-232">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_19_nodate" class=ref>
<summary class=citation>
<a id="noauthor_19_nodate">[noauthor_19_nodate]</a> - N/A - <a href="https://www.linkedin.com/pulse/multi-agent-ai-enables-emergent-cognition-real-time-science-buehler-tmtce/" target="_blank"><cite>(19) Multi-Agent {AI} Enables Emergent Cognition and Real-Time Knowledge Synthesis in Science and Engineering {\textbar} {LinkedIn}</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_19_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-233" id="summaryabstract-233">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="books_and_resources.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="books_and_resources.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>

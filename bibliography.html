<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Bibliography - AI Resources</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">2.</strong> Applications</a></li><li class="chapter-item expanded "><a href="multidisciplinary_approach.html"><strong aria-hidden="true">3.</strong> A Multidisciplinary Approach</a></li><li class="chapter-item expanded "><a href="approaches.html"><strong aria-hidden="true">4.</strong> Approaches</a></li><li class="chapter-item expanded "><a href="topics.html"><strong aria-hidden="true">5.</strong> ML/DL Topics</a></li><li class="chapter-item expanded "><a href="nlp.html"><strong aria-hidden="true">6.</strong> NLP</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">7.</strong> To Production</a></li><li class="chapter-item expanded "><a href="tools_and_frameworks.html"><strong aria-hidden="true">8.</strong> Frameworks</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">9.</strong> Books and Resources</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">10.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="people.html"><strong aria-hidden="true">11.</strong> People</a></li><li class="chapter-item expanded "><a href="vocabulary.html"><strong aria-hidden="true">12.</strong> Vocabulary</a></li><li class="chapter-item expanded affix "><a href="bibliography.html" class="active">Bibliography</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#bibliography" id="bibliography">Bibliography</a></h1>
<script type="text/javascript">
function defaultCopyTextToClipboard(text) {
    var textArea = document.createElement("textarea");
    textArea.value = text;

    // Avoid scrolling to bottom
    textArea.style.top = "0";
    textArea.style.left = "0";
    textArea.style.position = "fixed";

    document.body.appendChild(textArea);
    textArea.focus();
    textArea.select();

    try {
        var ok = document.execCommand('copy');
        var msg = ok ? 'was ok' : 'failed';
        console.log('Backing copy: Text copy was ' + msg);
    } catch (err) {
        console.error('Backing copy: Unable to copy text', err);
    }

    document.body.removeChild(textArea);
}

function copyToClipboard(text) {
    if (!navigator.clipboard) {
        defaultCopyTextToClipboard(text);
        return;
    }
    navigator.clipboard.writeText(text).then(function() {
        console.log('Text copied to clipboard');
    }, function(err) {
        console.error('Error copying text: ', err);
    });
}

</script>
<style></style>
<div class="bib_div">
<details data-key="de_lange_continual_2020" class=ref>
<summary class=citation>
<a id="de_lange_continual_2020">[de_lange_continual_2020]</a> - De Lange, Matthias, Aljundi, Rahaf, Masana, Marc, Parisot, Sarah, Jia, Xu, Leonardis, Ales, Slabaugh, Gregory, Tuytelaars, Tinne - <a href="http://arxiv.org/abs/1909.08383" target="_blank"><cite>A continual learning survey: Defying forgetting in classification tasks</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite de_lange_continual_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract" id="summaryabstract">Summary/Abstract</a></h1>
<div>Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced {iNaturalist} and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minaee_deep_2021" class=ref>
<summary class=citation>
<a id="minaee_deep_2021">[minaee_deep_2021]</a> - Minaee, Shervin, Kalchbrenner, Nal, Cambria, Erik, Nikzad, Narjes, Chenaghlu, Meysam, Gao, Jianfeng - <a href="http://arxiv.org/abs/2004.03705" target="_blank"><cite>Deep Learning Based Text Classification: A Comprehensive Review</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite minaee_deep_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-1" id="summaryabstract-1">Summary/Abstract</a></h1>
<div>Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kemker_fearnet_2018" class=ref>
<summary class=citation>
<a id="kemker_fearnet_2018">[kemker_fearnet_2018]</a> - Kemker, Ronald, Kanan, Christopher - <a href="https://openreview.net/forum?id&#x3D;SJ1Xmf-Rb" target="_blank"><cite>{FearNet}: Brain-Inspired Model for Incremental Learning</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite kemker_fearnet_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-2" id="summaryabstract-2">Summary/Abstract</a></h1>
<div>{FearNet} is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sun_how_2020" class=ref>
<summary class=citation>
<a id="sun_how_2020">[sun_how_2020]</a> - Sun, Chi, Qiu, Xipeng, Xu, Yige, Huang, Xuanjing - <a href="http://arxiv.org/abs/1905.05583" target="_blank"><cite>How to Fine-Tune {BERT} for Text Classification?</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite sun_how_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-3" id="summaryabstract-3">Summary/Abstract</a></h1>
<div>Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, {BERT} (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of {BERT} on text classification task and provide a general solution for {BERT} fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brunner_identifiability_2020" class=ref>
<summary class=citation>
<a id="brunner_identifiability_2020">[brunner_identifiability_2020]</a> - Brunner, Gino, Liu, Yang, Pascual, Damián, Richter, Oliver, Ciaramita, Massimiliano, Wattenhofer, Roger - <a href="http://arxiv.org/abs/1908.04211" target="_blank"><cite>On Identifiability in Transformers</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite brunner_identifiability_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-4" id="summaryabstract-4">Summary/Abstract</a></h1>
<div>In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="child_generating_2019" class=ref>
<summary class=citation>
<a id="child_generating_2019">[child_generating_2019]</a> - Child, Rewon, Gray, Scott, Radford, Alec, Sutskever, Ilya - <a href="http://arxiv.org/abs/1904.10509" target="_blank"><cite>Generating Long Sequences with Sparse Transformers</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite child_generating_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-5" id="summaryabstract-5">Summary/Abstract</a></h1>
<div>Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n {\textbackslash}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, {CIFAR}-10, and {ImageNet}-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhu_freelb_2019" class=ref>
<summary class=citation>
<a id="zhu_freelb_2019">[zhu_freelb_2019]</a> - Zhu, Chen, Cheng, Yu, Gan, Zhe, Sun, Siqi, Goldstein, Tom, Liu, Jingjing - <a href="http://arxiv.org/abs/1909.11764" target="_blank"><cite>{FreeLB}: Enhanced Adversarial Training for Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite zhu_freelb_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-6" id="summaryabstract-6">Summary/Abstract</a></h1>
<div>Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm - {FreeLB}, that promotes higher robustness and invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the {GLUE} benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of {BERT}-based model from 78.3 to 79.4, and {RoBERTa}-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\% and 67.75\% on {ARC}-Easy and {ARC}-Challenge. Experiments on {CommonsenseQA} benchmark further demonstrate that {FreeLB} can be generalized and boost the performance of {RoBERTa}-large model on other tasks as well.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="gupta_effective_2020" class=ref>
<summary class=citation>
<a id="gupta_effective_2020">[gupta_effective_2020]</a> - Gupta, Aakriti, Thadani, Kapil, O&#x27;Hare, Neil - <a href="https://www.aclweb.org/anthology/2020.coling-main.92" target="_blank"><cite>Effective Few-Shot Classification with Transfer Learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite gupta_effective_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-7" id="summaryabstract-7">Summary/Abstract</a></h1>
<div>Few-shot learning addresses the the problem of learning based on a small amount of training data. Although more well-studied in the domain of computer vision, recent work has adapted the Amazon Review Sentiment Classification ({ARSC}) text dataset for use in the few-shot setting. In this work, we use the {ARSC} dataset to study a simple application of transfer learning approaches to few-shot classification. We train a single binary classifier to learn all few-shot classes jointly by prefixing class identifiers to the input text. Given the text and class, the model then makes a binary prediction for that text/class pair. Our results show that this simple approach can outperform most published results on this dataset. Surprisingly, we also show that including domain information as part of the task definition only leads to a modest improvement in model accuracy, and zero-shot classification, without further fine-tuning on few-shot domains, performs equivalently to few-shot classification. These results suggest that the classes in the {ARSC} few-shot task, which are defined by the intersection of domain and rating, are actually very similar to each other, and that a more suitable dataset is needed for the study of few-shot text classification.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="karpathy_visualizing_2015" class=ref>
<summary class=citation>
<a id="karpathy_visualizing_2015">[karpathy_visualizing_2015]</a> - Karpathy, Andrej, Johnson, Justin, Fei-Fei, Li - <a href="http://arxiv.org/abs/1506.02078" target="_blank"><cite>Visualizing and Understanding Recurrent Networks</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite karpathy_visualizing_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-8" id="summaryabstract-8">Summary/Abstract</a></h1>
<div>Recurrent Neural Networks ({RNNs}), and specifically a variant with Long Short-Term Memory ({LSTM}), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while {LSTMs} provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the {LSTM} improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mnih_human-level_2015" class=ref>
<summary class=citation>
<a id="mnih_human-level_2015">[mnih_human-level_2015]</a> - Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjel, , Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, Hassabis, Demis - <a href="http://www.nature.com/articles/nature14236" target="_blank"><cite>Human-level control through deep reinforcement learning</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite mnih_human-level_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-9" id="summaryabstract-9">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ribeiro_beyond_2020-1" class=ref>
<summary class=citation>
<a id="ribeiro_beyond_2020-1">[ribeiro_beyond_2020-1]</a> - Ribeiro, Marco Tulio, Wu, Tongshuang, Guestrin, Carlos, Singh, Sameer - <a href="https://www.aclweb.org/anthology/2020.acl-main.442" target="_blank"><cite>Beyond Accuracy: Behavioral Testing of {NLP} Models with {CheckList}</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite ribeiro_beyond_2020-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-10" id="summaryabstract-10">Summary/Abstract</a></h1>
<div>Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of {NLP} models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce {CheckList}, a task-agnostic methodology for testing {NLP} models. {CheckList} includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of {CheckList} with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, {NLP} practitioners with {CheckList} created twice as many tests, and found almost three times as many bugs as users without it.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ruder_transfer_2019" class=ref>
<summary class=citation>
<a id="ruder_transfer_2019">[ruder_transfer_2019]</a> - Ruder, Sebastian, Peters, Matthew E., Swayamdipta, Swabha, Wolf, Thomas - <a href="https://www.aclweb.org/anthology/N19-5004" target="_blank"><cite>Transfer Learning in Natural Language Processing</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite ruder_transfer_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-11" id="summaryabstract-11">Summary/Abstract</a></h1>
<div>The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing ({NLP}) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of {NLP} tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and {ImageNet} pretraining in computer vision, and indicate that these methods will likely become a common tool in the {NLP} landscape as well as an important research direction. We will present an overview of modern transfer learning methods in {NLP}, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream {NLP} tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schaul_prioritized_2015" class=ref>
<summary class=citation>
<a id="schaul_prioritized_2015">[schaul_prioritized_2015]</a> - Schaul, Tom, Quan, John, Antonoglou, Ioannis, Silver, David - <a href="https://arxiv.org/abs/1511.05952v4" target="_blank"><cite>Prioritized Experience Replay</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite schaul_prioritized_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-12" id="summaryabstract-12">Summary/Abstract</a></h1>
<div>Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks ({DQN}), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. {DQN} with prioritized
experience replay achieves a new state-of-the-art, outperforming {DQN} with
uniform replay on 41 out of 49 games.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sukhbaatar_adaptive_2019" class=ref>
<summary class=citation>
<a id="sukhbaatar_adaptive_2019">[sukhbaatar_adaptive_2019]</a> - Sukhbaatar, Sainbayar, Grave, Edouard, Bojanowski, Piotr, Joulin, Arm,  - <a href="http://arxiv.org/abs/1905.07799" target="_blank"><cite>Adaptive Attention Span in Transformers</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite sukhbaatar_adaptive_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-13" id="summaryabstract-13">Summary/Abstract</a></h1>
<div>We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wei_nezha_2019" class=ref>
<summary class=citation>
<a id="wei_nezha_2019">[wei_nezha_2019]</a> - Wei, Junqiu, Ren, Xiaozhe, Li, Xiaoguang, Huang, Wenyong, Liao, Yi, Wang, Yasheng, Lin, Jiashu, Jiang, Xin, Chen, Xiao, Liu, Qun - <a href="http://arxiv.org/abs/1909.00204" target="_blank"><cite>{NEZHA}: Neural Contextualized Representation for Chinese Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite wei_nezha_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-14" id="summaryabstract-14">Summary/Abstract</a></h1>
<div>The pre-trained language models have achieved great successes in various natural language understanding ({NLU}) tasks due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora. In this technical report, we present our practice of pre-training language models named {NEZHA} ({NEural} {contextualiZed} representation for {CHinese} {lAnguage} understanding) on Chinese corpora and finetuning for the Chinese {NLU} tasks. The current version of {NEZHA} is based on {BERT} with a collection of proven improvements, which include Functional Relative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy, Mixed Precision Training and the {LAMB} Optimizer in training the models. The experimental results show that {NEZHA} achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including named entity recognition (People&#x27;s Daily {NER}), sentence matching ({LCQMC}), Chinese sentiment classification ({ChnSenti}) and natural language inference ({XNLI}).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="venkatesan_strategy_2017" class=ref>
<summary class=citation>
<a id="venkatesan_strategy_2017">[venkatesan_strategy_2017]</a> - Venkatesan, Ragav, Venkateswara, Hemanth, Panchanathan, Sethuraman, Li, Baoxin - <a href="http://arxiv.org/abs/1705.00744" target="_blank"><cite>A Strategy for an Uncompromising Incremental Learner</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite venkatesan_strategy_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-15" id="summaryabstract-15">Summary/Abstract</a></h1>
<div>Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these hacks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique, phantom sampling.We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets and through our strategy, we demonstrate that strict incremental learning could be achieved. We further put our strategy to test on challenging cases, including cross-domain increments and incrementing on a novel label space. We also propose a trivial extension to unbounded-continual learning and identify potential for future development.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="pan_survey_2010" class=ref>
<summary class=citation>
<a id="pan_survey_2010">[pan_survey_2010]</a> - Pan, Sinno Jialin, Yang, Qiang - <a href="http://ieeexplore.ieee.org/document/5288526/" target="_blank"><cite>A Survey on Transfer Learning</cite></a>. - 2010. -
<button onclick="copyToClipboard('\{\{ #cite pan_survey_2010 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-16" id="summaryabstract-16">Summary/Abstract</a></h1>
<div>A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sutton_reinforcement_1998" class=ref>
<summary class=citation>
<a id="sutton_reinforcement_1998">[sutton_reinforcement_1998]</a> - Sutton, Richard S., Barto, Andrew G. - <cite>Reinforcement learning: an introduction</cite>. - 1998. -
<button onclick="copyToClipboard('\{\{ #cite sutton_reinforcement_1998 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-17" id="summaryabstract-17">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="cheng_long_2016" class=ref>
<summary class=citation>
<a id="cheng_long_2016">[cheng_long_2016]</a> - Cheng, Jianpeng, Dong, Li, Lapata, Mirella - <a href="http://arxiv.org/abs/1601.06733" target="_blank"><cite>Long Short-Term Memory-Networks for Machine Reading</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite cheng_long_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-18" id="summaryabstract-18">Summary/Abstract</a></h1>
<div>In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="dai_transformer-xl:_2019" class=ref>
<summary class=citation>
<a id="dai_transformer-xl:_2019">[dai_transformer-xl:_2019]</a> - Dai, Zihang, Yang, Zhilin, Yang, Yiming, Carbonell, Jaime, Le, Quoc V., Salakhutdinov, Ruslan - <a href="http://arxiv.org/abs/1901.02860" target="_blank"><cite>Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite dai_transformer-xl:_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-19" id="summaryabstract-19">Summary/Abstract</a></h1>
<div>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-{XL} that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-{XL} learns dependency that is 80\% longer than {RNNs} and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on {WikiText}-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on {WikiText}-103, Transformer-{XL} manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and {PyTorch}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="nakkiran_deep_2019" class=ref>
<summary class=citation>
<a id="nakkiran_deep_2019">[nakkiran_deep_2019]</a> - Nakkiran, Preetum, Kaplun, Gal, Bansal, Yamini, Yang, Tristan, Barak, Boaz, Sutskever, Ilya - <a href="http://arxiv.org/abs/1912.02292" target="_blank"><cite>Deep Double Descent: Where Bigger Models and More Data Hurt</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite nakkiran_deep_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-20" id="summaryabstract-20">Summary/Abstract</a></h1>
<div>We show that a variety of modern deep learning tasks exhibit a double-descent phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhao_how_2020" class=ref>
<summary class=citation>
<a id="zhao_how_2020">[zhao_how_2020]</a> - Zhao, Yiyun, Bethard, Steven - <a href="https://www.aclweb.org/anthology/2020.acl-main.429" target="_blank"><cite>How does {BERT}&#x27;s attention change when you fine-tune? An analysis methodology and a case study in negation scope</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zhao_how_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-21" id="summaryabstract-21">Summary/Abstract</a></h1>
<div>Large pretrained language models like {BERT}, after fine-tuning to a downstream task, have achieved high performance on a variety of {NLP} problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test {BERT} and {RoBERTa} on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning {BERT} and {RoBERTa} on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wu_lite_2020" class=ref>
<summary class=citation>
<a id="wu_lite_2020">[wu_lite_2020]</a> - Wu, Zhanghao, Liu, Zhijian, Lin, Ji, Lin, Yujun, Han, Song - <a href="http://arxiv.org/abs/2004.11886" target="_blank"><cite>Lite Transformer with Long-Short Range Attention</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wu_lite_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-22" id="summaryabstract-22">Summary/Abstract</a></h1>
<div>Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile {NLP} architecture, Lite Transformer to facilitate deploying mobile {NLP} applications on edge devices. The key primitive is the Long-Short Range Attention ({LSRA}), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M {MACs}), Lite Transformer outperforms transformer on {WMT}&#x27;14 English-French by 1.2/1.7 {BLEU}, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 {BLEU} score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M {MACs}. Notably, Lite Transformer outperforms the {AutoML}-based Evolved Transformer by 0.5 higher {BLEU} for the mobile {NLP} setting without the costly architecture search that requires more than 250 {GPU} years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minsky_k-lines_1979" class=ref>
<summary class=citation>
<a id="minsky_k-lines_1979">[minsky_k-lines_1979]</a> - Minsky, Marvin - <a href="https://dspace.mit.edu/handle/1721.1/5739" target="_blank"><cite>K-Lines: A Theory of Memory</cite></a>. - 1979. -
<button onclick="copyToClipboard('\{\{ #cite minsky_k-lines_1979 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-23" id="summaryabstract-23">Summary/Abstract</a></h1>
<div>Most theories of memory suggest that when  we learn or memorize something, some  representation of that something is  constructed, stored and later retrieved. This  raises questions like: How is information  represented? How is it stored? How is it  retrieved? Then, how is it use? This paper  tries to deal with all these at once. When you  get an idea and want to remember it, you  create a K-line for it. When later activated, the  K-line induces a partial mental state  resembling the one that created it. A partial  mental state is a subset of those mental  agencies operating at one moment. This view  leads to many ideas about the development,  structure and physiology of Memory, and  about how to implement frame-like  representations in a distributed processor.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ioffe_batch_2015" class=ref>
<summary class=citation>
<a id="ioffe_batch_2015">[ioffe_batch_2015]</a> - Ioffe, Sergey, Szegedy, Christian - <a href="http://arxiv.org/abs/1502.03167" target="_blank"><cite>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite ioffe_batch_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-24" id="summaryabstract-24">Summary/Abstract</a></h1>
<div>Training Deep Neural Networks is complicated by the fact that the distribution of each layer&#x27;s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="peng_hierarchical_2019" class=ref>
<summary class=citation>
<a id="peng_hierarchical_2019">[peng_hierarchical_2019]</a> - Peng, Hao, Li, Jianxin, Gong, Qiran, Wang, Senzhang, He, Lifang, Li, Bo, Wang, Lihong, Yu, Philip S. - <a href="http://arxiv.org/abs/1906.04898" target="_blank"><cite>Hierarchical Taxonomy-Aware and Attentional Graph Capsule {RCNNs} for Large-Scale Multi-Label Text Classification</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite peng_hierarchical_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-25" id="summaryabstract-25">Summary/Abstract</a></h1>
<div>{CNNs}, {RNNs}, {GCNs}, and {CapsNets} have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent {CNNs} framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent {CNNs} for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_roberta:_2019" class=ref>
<summary class=citation>
<a id="liu_roberta:_2019">[liu_roberta:_2019]</a> - Liu, Yinhan, Ott, Myle, Goyal, Naman, Du, Jingfei, Joshi, M, ar, Chen, Danqi, Levy, Omer, Lewis, Mike, Zettlemoyer, Luke, Stoyanov, Veselin - <a href="http://arxiv.org/abs/1907.11692" target="_blank"><cite>{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite liu_roberta:_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-26" id="summaryabstract-26">Summary/Abstract</a></h1>
<div>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shaw_self-attention_2018" class=ref>
<summary class=citation>
<a id="shaw_self-attention_2018">[shaw_self-attention_2018]</a> - Shaw, Peter, Uszkoreit, Jakob, Vaswani, Ashish - <a href="http://arxiv.org/abs/1803.02155" target="_blank"><cite>Self-Attention with Relative Position Representations</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite shaw_self-attention_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-27" id="summaryabstract-27">Summary/Abstract</a></h1>
<div>Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the {WMT} 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 {BLEU} and 0.3 {BLEU} over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schmidhuber_deep_2015" class=ref>
<summary class=citation>
<a id="schmidhuber_deep_2015">[schmidhuber_deep_2015]</a> - Schmidhuber, Juergen - <a href="http://arxiv.org/abs/1404.7828" target="_blank"><cite>Deep Learning in Neural Networks: An Overview</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite schmidhuber_deep_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-28" id="summaryabstract-28">Summary/Abstract</a></h1>
<div>In recent years, deep artiﬁcial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \&amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="polyzotis_slice_2019" class=ref>
<summary class=citation>
<a id="polyzotis_slice_2019">[polyzotis_slice_2019]</a> - Polyzotis, Neoklis, Whang, Steven, Kraska, Tim Klas, Chung, Yeounoh - <a href="https://arxiv.org/pdf/1807.06068.pdf" target="_blank"><cite>Slice Finder: Automated Data Slicing for Model Validation</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite polyzotis_slice_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-29" id="summaryabstract-29">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="geiger_scaling_2020" class=ref>
<summary class=citation>
<a id="geiger_scaling_2020">[geiger_scaling_2020]</a> - Geiger, Mario, Jacot, Arthur, Spigler, Stefano, Gabriel, Franck, Sagun, Levent, d&#x27;Ascoli, Stéphane, Biroli, Giulio, Hongler, Clément, Wyart, Matthieu - <a href="http://arxiv.org/abs/1901.01608" target="_blank"><cite>Scaling description of generalization with number of parameters in deep learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite geiger_scaling_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-30" id="summaryabstract-30">Summary/Abstract</a></h1>
<div>Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N{\textasciicircum}\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \${\textbackslash}{\textbar}f\_\{N\}-{\textbackslash}bar\{f\}\_\{N\}{\textbackslash}{\textbar}{\textbackslash}sim N{\textasciicircum}\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \${\textbackslash}bar\{f\}\_\{N\}\$. These affect the generalization error \${\textbackslash}epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \${\textbackslash}epsilon\_\{{\textbackslash}infty\}\$ in a power-law fashion \${\textbackslash}sim N{\textasciicircum}\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N&#x3D;N{\textasciicircum}\{*\}\$. At this threshold, we argue that \${\textbackslash}{\textbar}f\_\{N\}{\textbackslash}{\textbar}\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N{\textasciicircum}\{*\}\$. Our results are confirmed by extensive empirical observations on the {MNIST} and {CIFAR} image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N{\textasciicircum}\{*\}\$, and averaging their outputs.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="parisi_continual_2019" class=ref>
<summary class=citation>
<a id="parisi_continual_2019">[parisi_continual_2019]</a> - Parisi, German I., Kemker, Ronald, Part, Jose L., Kanan, Christopher, Wermter, Stefan - <a href="http://arxiv.org/abs/1802.07569" target="_blank"><cite>Continual Lifelong Learning with Neural Networks: A Review</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite parisi_continual_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-31" id="summaryabstract-31">Summary/Abstract</a></h1>
<div>Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="radford_language_nodate" class=ref>
<summary class=citation>
<a id="radford_language_nodate">[radford_language_nodate]</a> - Radford, Alec, Wu, Jeffrey, Child, Rewon, Luan, David, Amodei, Dario, Sutskever, Ilya - <cite>Language Models are Unsupervised Multitask Learners</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite radford_language_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-32" id="summaryabstract-32">Summary/Abstract</a></h1>
<div>Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_generative_2019" class=ref>
<summary class=citation>
<a id="van_de_ven_generative_2019">[van_de_ven_generative_2019]</a> - van de Ven, Gido M., Tolias, Andreas S. - <a href="http://arxiv.org/abs/1809.10635" target="_blank"><cite>Generative replay with feedback connections as a general strategy for continual learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_generative_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-33" id="summaryabstract-33">Summary/Abstract</a></h1>
<div>A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted {MNIST} task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as soft targets) achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="domingos_master_2015" class=ref>
<summary class=citation>
<a id="domingos_master_2015">[domingos_master_2015]</a> - Domingos, Pedro - <cite>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</cite>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite domingos_master_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-34" id="summaryabstract-34">Summary/Abstract</a></h1>
<div>A thought-provoking and wide-ranging exploration of machine learning and the race to build computer intelligences as flexible as our {ownIn} the world&#x27;s top research labs and universities, the race is on to invent the ultimate learning algorithm: one capable of discovering any knowledge from data, and doing anything we want, before we even ask. In The Master Algorithm, Pedro Domingos lifts the veil to give us a peek inside the learning machines that power Google, Amazon, and your smartphone. He assembles a blueprint for the future universal learner--the Master Algorithm--and discusses what it will mean for business, science, and society. If data-ism is today&#x27;s philosophy, this book is its bible.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="goodfellow_deep_2016" class=ref>
<summary class=citation>
<a id="goodfellow_deep_2016">[goodfellow_deep_2016]</a> - Goodfellow, Ian, Bengio, Yoshua, Courville, Aaron - <cite>Deep Learning</cite>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite goodfellow_deep_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-35" id="summaryabstract-35">Summary/Abstract</a></h1>
<div>An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="yang_finbert_2020" class=ref>
<summary class=citation>
<a id="yang_finbert_2020">[yang_finbert_2020]</a> - Yang, Yi, {UY}, Mark Christopher Siy, Huang, Allen - <a href="http://arxiv.org/abs/2006.08097" target="_blank"><cite>{FinBERT}: A Pretrained Language Model for Financial Communications</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite yang_finbert_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-36" id="summaryabstract-36">Summary/Abstract</a></h1>
<div>Contextual pretrained language models, such as {BERT} (Devlin et al., 2019), have made significant breakthrough in various {NLP} tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific {BERT} models, {FinBERT}, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of {FinBERT} over generic domain {BERT} model. The code and pretrained models are available at https://github.com/yya518/{FinBERT}. We hope this will be useful for practitioners and researchers working on financial {NLP} tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minsky_perceptrons_1987" class=ref>
<summary class=citation>
<a id="minsky_perceptrons_1987">[minsky_perceptrons_1987]</a> - Minsky, Marvin, Papert, Seymour A. - <cite>Perceptrons: An Introduction to Computational Geometry, Expanded Edition</cite>. - 1987. -
<button onclick="copyToClipboard('\{\{ #cite minsky_perceptrons_1987 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-37" id="summaryabstract-37">Summary/Abstract</a></h1>
<div>Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades. It marked a historical turn in artificial intelligence, and it is required reading for anyone who wants to understand the connectionist counterrevolution that is going on today. Artificial-intelligence research, which for a time concentrated on the programming of ton Neumann computers, is swinging back to the idea that intelligence might emerge from the activity of networks of neuronlike entities. Minsky and Papert&#x27;s book was the first example of a mathematical analysis carried far enough to show the exact limitations of a class of computing machines that could seriously be considered as models of the brain. Now the new developments in mathematical tools, the recent interest of physicists in the theory of disordered matter, the new insights into and psychological models of how the brain works, and the evolution of fast computers that can simulate networks of automata have given Perceptrons new importance.Witnessing the swing of the intellectual pendulum, Minsky and Papert have added a new chapter in which they discuss the current state of parallel computers, review developments since the appearance of the 1972 edition, and identify new research directions related to connectionism. They note a central theoretical challenge facing connectionism: the challenge to reach a deeper understanding of how objects or agents with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called society theories of mind. Marvin L. Minsky is Donner Professor of Science in M.I.T.&#x27;s Electrical Engineering and Computer Science Department. Seymour A. Papert is Professor of Media Technology at M.I.T. .</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="huszar_quadratic_2017" class=ref>
<summary class=citation>
<a id="huszar_quadratic_2017">[huszar_quadratic_2017]</a> - Huszár, Ferenc - <a href="http://arxiv.org/abs/1712.03847" target="_blank"><cite>On Quadratic Penalties in Elastic Weight Consolidation</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite huszar_quadratic_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-38" id="summaryabstract-38">Summary/Abstract</a></h1>
<div>Elastic weight consolidation ({EWC}, Kirkpatrick et al, 2017) is a novel algorithm designed to safeguard against catastrophic forgetting in neural networks. {EWC} can be seen as an approximation to Laplace propagation (Eskin et al, 2004), and this view is consistent with the motivation given by Kirkpatrick et al (2017). In this note, I present an extended derivation that covers the case when there are more than two tasks. I show that the quadratic penalties in {EWC} are inconsistent with this derivation and might lead to double-counting data from earlier tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="doersch_tutorial_2016" class=ref>
<summary class=citation>
<a id="doersch_tutorial_2016">[doersch_tutorial_2016]</a> - Doersch, Carl - <a href="http://arxiv.org/abs/1606.05908" target="_blank"><cite>Tutorial on Variational Autoencoders</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite doersch_tutorial_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-39" id="summaryabstract-39">Summary/Abstract</a></h1>
<div>In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhu_transfer_2020" class=ref>
<summary class=citation>
<a id="zhu_transfer_2020">[zhu_transfer_2020]</a> - Zhu, Zhuangdi, Lin, Kaixiang, Zhou, Jiayu - <a href="http://arxiv.org/abs/2009.07888" target="_blank"><cite>Transfer Learning in Deep Reinforcement Learning: A Survey</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zhu_transfer_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-40" id="summaryabstract-40">Summary/Abstract</a></h1>
<div>This paper surveys the field of transfer learning in the problem setting of Reinforcement Learning ({RL}). {RL} has been a key solution to sequential decision-making problems. Along with the fast advances of {RL} in various domains, such as robotics and game-playing, transfer learning arises as an important technique to assist {RL} by leveraging and transferring external expertise to boost the learning process of {RL}. In this survey, we review the central issues of transfer learning in the {RL} domain, providing a systematic categorization of its state-of-the-art techniques. We analyze their goals, methodologies, applications, and the {RL} frameworks under which the transfer learning techniques are approachable. We discuss the relationship between transfer learning and other relevant topics from the {RL} perspective and also explore the potential challenges as well as future development directions for transfer learning in {RL}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_hasselt_deep_2015" class=ref>
<summary class=citation>
<a id="van_hasselt_deep_2015">[van_hasselt_deep_2015]</a> - van Hasselt, Hado, Guez, Arthur, Silver, David - <a href="http://arxiv.org/abs/1509.06461" target="_blank"><cite>Deep Reinforcement Learning with Double Q-learning</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite van_hasselt_deep_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-41" id="summaryabstract-41">Summary/Abstract</a></h1>
<div>The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="clark_electra_2019" class=ref>
<summary class=citation>
<a id="clark_electra_2019">[clark_electra_2019]</a> - Clark, Kevin, Luong, Minh-Thang, Le, Quoc V., Manning, Christopher D. - <a href="https://openreview.net/forum?id&#x3D;r1xMH1BtvB" target="_blank"><cite>{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite clark_electra_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-42" id="summaryabstract-42">Summary/Abstract</a></h1>
<div>Masked language modeling ({MLM}) pre-training methods such as {BERT} corrupt the input by replacing some tokens with [{MASK}] and then train a model to reconstruct the original tokens. While they produce...</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="caruana_multitask_1997" class=ref>
<summary class=citation>
<a id="caruana_multitask_1997">[caruana_multitask_1997]</a> - Caruana, Rich - <a href="http://reports-archive.adm.cs.cmu.edu/anon/1997/CMU-CS-97-203.pdf" target="_blank"><cite>Multitask Learning</cite></a>. - 1997. -
<button onclick="copyToClipboard('\{\{ #cite caruana_multitask_1997 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-43" id="summaryabstract-43">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_state_nodate" class=ref>
<summary class=citation>
<a id="noauthor_state_nodate">[noauthor_state_nodate]</a> - N/A - <a href="https://ruder.io/state-of-transfer-learning-in-nlp/" target="_blank"><cite>The State of Transfer Learning in {NLP}</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_state_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-44" id="summaryabstract-44">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brown_language_2020" class=ref>
<summary class=citation>
<a id="brown_language_2020">[brown_language_2020]</a> - Brown, Tom B., Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared, Dhariwal, Prafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Am, a, Agarwal, S, hini, Herbert-Voss, Ariel, Krueger, Gretchen, Henighan, Tom, Child, Rewon, Ramesh, Aditya, Ziegler, Daniel M., Wu, Jeffrey, Winter, Clemens, Hesse, Christopher, Chen, Mark, Sigler, Eric, Litwin, Mateusz, Gray, Scott, Chess, Benjamin, Clark, Jack, Berner, Christopher, {McC, lish}, Sam, Radford, Alec, Sutskever, Ilya, Amodei, Dario - <a href="http://arxiv.org/abs/2005.14165" target="_blank"><cite>Language Models are Few-Shot Learners</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite brown_language_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-45" id="summaryabstract-45">Summary/Abstract</a></h1>
<div>Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3&#x27;s few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="child_generating_2019-1" class=ref>
<summary class=citation>
<a id="child_generating_2019-1">[child_generating_2019-1]</a> - Child, Rewon, Gray, Scott, Radford, Alec, Sutskever, Ilya - <a href="http://arxiv.org/abs/1904.10509" target="_blank"><cite>Generating Long Sequences with Sparse Transformers</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite child_generating_2019-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-46" id="summaryabstract-46">Summary/Abstract</a></h1>
<div>Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n {\textbackslash}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, {CIFAR}-10, and {ImageNet}-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="miller_explanation_2018" class=ref>
<summary class=citation>
<a id="miller_explanation_2018">[miller_explanation_2018]</a> - Miller, Tim - <a href="http://arxiv.org/abs/1706.07269" target="_blank"><cite>Explanation in Artificial Intelligence: Insights from the Social Sciences</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite miller_explanation_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-47" id="summaryabstract-47">Summary/Abstract</a></h1>
<div>There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers&#x27; intuition of what constitutes a &#x60;good&#x27; explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="mccloskey_catastrophic_1989" class=ref>
<summary class=citation>
<a id="mccloskey_catastrophic_1989">[mccloskey_catastrophic_1989]</a> - {McCloskey}, Michael, Cohen, Neal J. - <a href="https://www.sciencedirect.com/science/article/pii/S0079742108605368" target="_blank"><cite>Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</cite></a>. - 1989. -
<button onclick="copyToClipboard('\{\{ #cite mccloskey_catastrophic_1989 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-48" id="summaryabstract-48">Summary/Abstract</a></h1>
<div>Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="gong_hierarchical_2020" class=ref>
<summary class=citation>
<a id="gong_hierarchical_2020">[gong_hierarchical_2020]</a> - Gong, Jibing, Liu, Mingsheng, Ma, Hongyuan, Teng, Zhiyong, Teng, Qi, Zhang, Hekai, Du, Linfeng, Chen, Shuai, Bhuiyan, Md, Li, Jianhua - <cite>Hierarchical Graph Transformer Based Deep Learning Model for Large-Scale Multi-Label Text Classification</cite>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite gong_hierarchical_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-49" id="summaryabstract-49">Summary/Abstract</a></h1>
<div>Traditional methods of multi-label text classification, particularly deep learning, have achieved remarkable results. However, most of these methods use word2vec technology to represent sequential text information, while ignoring the logic and internal hierarchy of the text itself. Although these approaches can learn the hypothetical hierarchy and logic of the text, it is unexplained. In addition, the traditional approach treats labels as independent individuals and ignores the relationships between them, which not only does not reflect reality but also causes significant loss of semantic information. In this paper, we propose a novel Hierarchical Graph Transformer based deep learning model for large-scale multi-label text classification. We first model the text into a graph structure that can embody the different semantics of the text and the connections between them. We then use a multi-layer transformer structure with a multi-head attention mechanism at the word, sentence, and graph levels to fully capture the features of the text and observe the importance of the separate parts. Finally, we use the hierarchical relationship of the labels to generate the representation of the labels, and design a weighted loss function based on the semantic distances of the labels. Extensive experiments conducted on three benchmark datasets demonstrated that the proposed model can realistically capture the hierarchy and logic of text and improve performance compared with the state-of-the-art methods.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_improvements_2018" class=ref>
<summary class=citation>
<a id="noauthor_improvements_2018">[noauthor_improvements_2018]</a> - N/A - <a href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" target="_blank"><cite>Improvements in Deep Q Learning: Dueling Double {DQN}, Prioritized Experience Replay, and fixed…</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_improvements_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-50" id="summaryabstract-50">Summary/Abstract</a></h1>
<div>by Thomas Simonini
<p>Improvements in Deep Q Learning: Dueling Double {DQN}, Prioritized Experience
Replay, and fixed Q-targets
{\textgreater} This article is part of Deep Reinforcement Learning Course with Tensorflow ?️.
Check the syllabus here.
[https://simoninithomas.github.io/Deep_reinforcement_learning_Course/]
In our last article about Deep Q Learning with Tensorflow
[https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8]
, we implemented an agent that learns to pla</div></p>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="masse_alleviating_2018" class=ref>
<summary class=citation>
<a id="masse_alleviating_2018">[masse_alleviating_2018]</a> - Masse, Nicolas Y., Grant, Gregory D., Freedman, David J. - <a href="https://www.pnas.org/content/115/44/E10467" target="_blank"><cite>Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite masse_alleviating_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-51" id="summaryabstract-51">Summary/Abstract</a></h1>
<div>Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks ({ANNs}) on new tasks typically causes them to forget previously learned tasks. This phenomenon is the result of “catastrophic forgetting,” in which training an {ANN} disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of {ANNs} that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows {ANNs} to maintain high performance across large numbers of sequentially presented tasks, particularly when combined with weight stabilization. We show that this method works for both feedforward and recurrent network architectures, trained using either supervised or reinforcement-based learning. This suggests that using multiple, complementary methods, akin to what is believed to occur in the brain, can be a highly effective strategy to support continual learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="bucilua_model_2006" class=ref>
<summary class=citation>
<a id="bucilua_model_2006">[bucilua_model_2006]</a> - Buciluǎ, Cristian, Caruana, Rich, Niculescu-Mizil, Alex, ru - <a href="https://doi.org/10.1145/1150402.1150464" target="_blank"><cite>Model compression</cite></a>. - 2006. -
<button onclick="copyToClipboard('\{\{ #cite bucilua_model_2006 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-52" id="summaryabstract-52">Summary/Abstract</a></h1>
<div>Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. {PDAs}), and where computational power is limited (e.g. hea-ring aids). We present a method for compressing large, complex ensembles into smaller, faster models, usually without significant loss in performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kirkpatrick_optimization_1983" class=ref>
<summary class=citation>
<a id="kirkpatrick_optimization_1983">[kirkpatrick_optimization_1983]</a> - Kirkpatrick, S., Gelatt, C. D., Vecchi, M. P. - <a href="https://science.sciencemag.org/content/220/4598/671" target="_blank"><cite>Optimization by Simulated Annealing</cite></a>. - 1983. -
<button onclick="copyToClipboard('\{\{ #cite kirkpatrick_optimization_1983 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-53" id="summaryabstract-53">Summary/Abstract</a></h1>
<div>There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="li_learning_2017" class=ref>
<summary class=citation>
<a id="li_learning_2017">[li_learning_2017]</a> - Li, Zhizhong, Hoiem, Derek - <a href="http://arxiv.org/abs/1606.09282" target="_blank"><cite>Learning without Forgetting</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite li_learning_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-54" id="summaryabstract-54">Summary/Abstract</a></h1>
<div>When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network ({CNN}), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="keitakurita_intuitive_2018" class=ref>
<summary class=citation>
<a id="keitakurita_intuitive_2018">[keitakurita_intuitive_2018]</a> - keitakurita, Author - <a href="http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/" target="_blank"><cite>An Intuitive Explanation of Why Batch Normalization Really Works (Normalization in Deep Learning Part 1)</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite keitakurita_intuitive_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-55" id="summaryabstract-55">Summary/Abstract</a></h1>
<div>Batch normalization is one of the reasons why deep learning has made such outstanding progress in recent years. Batch normalization enables the use of higher learning rates, greatly accelerating th…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sejnowski_deep_2018" class=ref>
<summary class=citation>
<a id="sejnowski_deep_2018">[sejnowski_deep_2018]</a> - Sejnowski, Terrence J. - <cite>The Deep Learning Revolution</cite>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite sejnowski_deep_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-56" id="summaryabstract-56">Summary/Abstract</a></h1>
<div>How deep learning—from Google Translate to driverless cars to personal cognitive assistants—is changing our lives and transforming every sector of the economy.The deep learning revolution has brought us driverless cars, the greatly improved Google Translate, fluent conversations with Siri and Alexa, and enormous profits from automated trading on the New York Stock Exchange. Deep learning networks can play poker better than professional poker players and defeat a world champion at Go. In this book, Terry Sejnowski explains how deep learning went from being an arcane academic field to a disruptive technology in the information economy.Sejnowski played an important role in the founding of deep learning, as one of a small group of researchers in the 1980s who challenged the prevailing logic-and-symbol based version of {AI}. The new version of {AI} Sejnowski and others developed, which became deep learning, is fueled instead by data. Deep networks learn from data in the same way that babies experience the world, starting with fresh eyes and gradually acquiring the skills needed to navigate novel environments. Learning algorithms extract information from raw data; information can be used to create knowledge; knowledge underlies understanding; understanding leads to wisdom. Someday a driverless car will know the road better than you do and drive with more skill; a deep learning network will diagnose your illness; a personal cognitive assistant will augment your puny human brain. It took nature many millions of years to evolve human intelligence; {AI} is on a trajectory measured in decades. Sejnowski prepares us for a deep learning future.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="al-rfou_character-level_2018" class=ref>
<summary class=citation>
<a id="al-rfou_character-level_2018">[al-rfou_character-level_2018]</a> - Al-Rfou, Rami, Choe, Dokook, Constant, Noah, Guo, M, y, Jones, Llion - <a href="http://arxiv.org/abs/1808.04444" target="_blank"><cite>Character-Level Language Modeling with Deeper Self-Attention</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite al-rfou_character-level_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-57" id="summaryabstract-57">Summary/Abstract</a></h1>
<div>{LSTMs} and other {RNN} variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms {RNN} variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ba_layer_2016" class=ref>
<summary class=citation>
<a id="ba_layer_2016">[ba_layer_2016]</a> - Ba, Jimmy Lei, Kiros, Jamie Ryan, Hinton, Geoffrey E. - <a href="http://arxiv.org/abs/1607.06450" target="_blank"><cite>Layer Normalization</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite ba_layer_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-58" id="summaryabstract-58">Summary/Abstract</a></h1>
<div>Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tavakoli_prioritizing_2019" class=ref>
<summary class=citation>
<a id="tavakoli_prioritizing_2019">[tavakoli_prioritizing_2019]</a> - Tavakoli, Arash, Levdik, Vitaly, Islam, Riashat, Kormushev, Petar - <a href="http://arxiv.org/abs/1811.11298" target="_blank"><cite>Prioritizing Starting States for Reinforcement Learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite tavakoli_prioritizing_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-59" id="summaryabstract-59">Summary/Abstract</a></h1>
<div>Online, off-policy reinforcement learning algorithms are able to use an experience memory to remember and replay past experiences. In prior work, this approach was used to stabilize training by breaking the temporal correlations of the updates and avoiding the rapid forgetting of possibly rare experiences. In this work, we propose a conceptually simple framework that uses an experience memory to help exploration by prioritizing the starting states from which the agent starts acting in the environment, importantly, in a fashion that is also compatible with on-policy algorithms. Given the capacity to restart the agent in states corresponding to its past observations, we achieve this objective by (i) enabling the agent to restart in states belonging to significant past experiences (e.g., nearby goals), and (ii) promoting faster coverage of the state space through starting from a more diverse set of states. While, using a good priority measure to identify significant past transitions, we expect case (i) to more considerably help exploration in certain domains (e.g., sparse reward tasks), we hypothesize that case (ii) will generally be beneficial, even without any prioritization. We show empirically that our approach improves learning performance for both off-policy and on-policy deep reinforcement learning methods, with most notable gains in highly sparse reward tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="clark_what_2019" class=ref>
<summary class=citation>
<a id="clark_what_2019">[clark_what_2019]</a> - Clark, Kevin, Kh, elwal, Urvashi, Levy, Omer, Manning, Christopher D. - <a href="http://arxiv.org/abs/1906.04341" target="_blank"><cite>What Does {BERT} Look At? An Analysis of {BERT}&#x27;s Attention</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite clark_what_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-60" id="summaryabstract-60">Summary/Abstract</a></h1>
<div>Large pre-trained neural networks such as {BERT} have had great recent success in {NLP}, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to {BERT}. {BERT}&#x27;s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in {BERT}&#x27;s attention.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sanh_distilbert_2020" class=ref>
<summary class=citation>
<a id="sanh_distilbert_2020">[sanh_distilbert_2020]</a> - Sanh, Victor, Debut, Lys, re, Chaumond, Julien, Wolf, Thomas - <a href="http://arxiv.org/abs/1910.01108" target="_blank"><cite>{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite sanh_distilbert_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-61" id="summaryabstract-61">Summary/Abstract</a></h1>
<div>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="salimans_weight_2016" class=ref>
<summary class=citation>
<a id="salimans_weight_2016">[salimans_weight_2016]</a> - Salimans, Tim, Kingma, Diederik P. - <a href="http://arxiv.org/abs/1602.07868" target="_blank"><cite>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite salimans_weight_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-62" id="summaryabstract-62">Summary/Abstract</a></h1>
<div>We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as {LSTMs} and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="cappelletti_polyadic_2020" class=ref>
<summary class=citation>
<a id="cappelletti_polyadic_2020">[cappelletti_polyadic_2020]</a> - Cappelletti, William, Erbanni, Rebecca, Keller, Joaquín - <a href="http://arxiv.org/abs/2007.14044" target="_blank"><cite>Polyadic Quantum Classifier</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite cappelletti_polyadic_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-63" id="summaryabstract-63">Summary/Abstract</a></h1>
<div>We introduce here a supervised quantum machine learning algorithm for multi-class classification on {NISQ} architectures. A parametric quantum circuit is trained to output a specific bit string corresponding to the class of the input datapoint. We train and test it on an {IBMq} 5-qubit quantum computer and the algorithm shows good accuracy –compared to a classical machine learning model– for ternary classification of the Iris dataset and an extension of the {XOR} problem. Furthermore, we evaluate with simulations how the algorithm fares for a binary and a quaternary classification on resp. a known binary dataset and a synthetic dataset.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_three_2019" class=ref>
<summary class=citation>
<a id="van_de_ven_three_2019">[van_de_ven_three_2019]</a> - van de Ven, Gido M., Tolias, Andreas S. - <a href="http://arxiv.org/abs/1904.07734" target="_blank"><cite>Three scenarios for continual learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_three_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-64" id="summaryabstract-64">Summary/Abstract</a></h1>
<div>Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and–in case it is not–whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted {MNIST} task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_three_2019-1" class=ref>
<summary class=citation>
<a id="van_de_ven_three_2019-1">[van_de_ven_three_2019-1]</a> - van de Ven, Gido M., Tolias, Andreas S. - <a href="http://arxiv.org/abs/1904.07734" target="_blank"><cite>Three scenarios for continual learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_three_2019-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-65" id="summaryabstract-65">Summary/Abstract</a></h1>
<div>Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted {MNIST} task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sculley_hidden_2015" class=ref>
<summary class=citation>
<a id="sculley_hidden_2015">[sculley_hidden_2015]</a> - Sculley, D., Holt, Gary, Golovin, Daniel, Davydov, Eugene, Phillips, Todd, Ebner, Dietmar, Chaudhary, Vinay, Young, Michael, Crespo, Jean-François, Dennison, Dan - <a href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf" target="_blank"><cite>Hidden Technical Debt in Machine Learning Systems</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite sculley_hidden_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-66" id="summaryabstract-66">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="hinton_distilling_2015" class=ref>
<summary class=citation>
<a id="hinton_distilling_2015">[hinton_distilling_2015]</a> - Hinton, Geoffrey, Vinyals, Oriol, Dean, Jeff - <a href="http://arxiv.org/abs/1503.02531" target="_blank"><cite>Distilling the Knowledge in a Neural Network</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite hinton_distilling_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-67" id="summaryabstract-67">Summary/Abstract</a></h1>
<div>A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lample_cross-lingual_2019" class=ref>
<summary class=citation>
<a id="lample_cross-lingual_2019">[lample_cross-lingual_2019]</a> - Lample, Guillaume, Conneau, Alexis - <a href="http://arxiv.org/abs/1901.07291" target="_blank"><cite>Cross-lingual Language Model Pretraining</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite lample_cross-lingual_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-68" id="summaryabstract-68">Summary/Abstract</a></h1>
<div>Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models ({XLMs}): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On {XNLI}, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 {BLEU} on {WMT}&#x27;16 German-English, improving the previous state of the art by more than 9 {BLEU}. On supervised machine translation, we obtain a new state of the art of 38.5 {BLEU} on {WMT}&#x27;16 Romanian-English, outperforming the previous best approach by more than 4 {BLEU}. Our code and pretrained models will be made publicly available.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_linformer_2020" class=ref>
<summary class=citation>
<a id="wang_linformer_2020">[wang_linformer_2020]</a> - Wang, Sinong, Li, Belinda Z., Khabsa, Madian, Fang, Han, Ma, Hao - <a href="http://arxiv.org/abs/2006.04768" target="_blank"><cite>Linformer: Self-Attention with Linear Complexity</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wang_linformer_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-69" id="summaryabstract-69">Summary/Abstract</a></h1>
<div>Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="tan_survey_2018" class=ref>
<summary class=citation>
<a id="tan_survey_2018">[tan_survey_2018]</a> - Tan, Chuanqi, Sun, Fuchun, Kong, Tao, Zhang, Wenchang, Yang, Chao, Liu, Chunfang - <a href="http://arxiv.org/abs/1808.01974" target="_blank"><cite>A Survey on Deep Transfer Learning</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite tan_survey_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-70" id="summaryabstract-70">Summary/Abstract</a></h1>
<div>As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wang_dueling_2016" class=ref>
<summary class=citation>
<a id="wang_dueling_2016">[wang_dueling_2016]</a> - Wang, Ziyu, Schaul, Tom, Hessel, Matteo, van Hasselt, Hado, Lanctot, Marc, de Freitas, N, o - <a href="http://arxiv.org/abs/1511.06581" target="_blank"><cite>Dueling Network Architectures for Deep Reinforcement Learning</cite></a>. - 2016. -
<button onclick="copyToClipboard('\{\{ #cite wang_dueling_2016 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-71" id="summaryabstract-71">Summary/Abstract</a></h1>
<div>In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, {LSTMs}, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our {RL} agent to outperform the state-of-the-art on the Atari 2600 domain.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ribeiro_beyond_2020" class=ref>
<summary class=citation>
<a id="ribeiro_beyond_2020">[ribeiro_beyond_2020]</a> - Ribeiro, Marco Tulio, Wu, Tongshuang, Guestrin, Carlos, Singh, Sameer - <a href="http://arxiv.org/abs/2005.04118" target="_blank"><cite>Beyond Accuracy: Behavioral Testing of {NLP} models with {CheckList}</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite ribeiro_beyond_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-72" id="summaryabstract-72">Summary/Abstract</a></h1>
<div>Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of {NLP} models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce {CheckList}, a task-agnostic methodology for testing {NLP} models. {CheckList} includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of {CheckList} with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, {NLP} practitioners with {CheckList} created twice as many tests, and found almost three times as many bugs as users without it.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_brain-inspired_2020" class=ref>
<summary class=citation>
<a id="van_de_ven_brain-inspired_2020">[van_de_ven_brain-inspired_2020]</a> - van de Ven, Gido M., Siegelmann, Hava T., Tolias, Andreas S. - <a href="https://www.nature.com/articles/s41467-020-17866-2" target="_blank"><cite>Brain-inspired replay for continual learning with artificial neural networks</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_brain-inspired_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-73" id="summaryabstract-73">Summary/Abstract</a></h1>
<div>Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as ‘generative replay’, which can successfully – and surprisingly efficiently – prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network’s own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on {CIFAR}-100) without storing data, and it provides a novel model for replay in the brain.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="schwarz_progress_2018" class=ref>
<summary class=citation>
<a id="schwarz_progress_2018">[schwarz_progress_2018]</a> - Schwarz, Jonathan, Luketina, Jelena, Czarnecki, Wojciech M., Grabska-Barwinska, Agnieszka, Teh, Yee Whye, Pascanu, Razvan, Hadsell, Raia - <a href="http://arxiv.org/abs/1805.06370" target="_blank"><cite>Progress \&amp; Compress: A scalable framework for continual learning</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite schwarz_progress_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-74" id="summaryabstract-74">Summary/Abstract</a></h1>
<div>We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress \&amp; compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="akama_elements_2015" class=ref>
<summary class=citation>
<a id="akama_elements_2015">[akama_elements_2015]</a> - Akama, Seiki - <a href="http://link.springer.com/10.1007/978-3-319-08284-4" target="_blank"><cite>Elements of Quantum Computing</cite></a>. - 2015. -
<button onclick="copyToClipboard('\{\{ #cite akama_elements_2015 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-75" id="summaryabstract-75">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="metz_genius_2021" class=ref>
<summary class=citation>
<a id="metz_genius_2021">[metz_genius_2021]</a> - Metz, Cade - <cite>Genius Makers: The Mavericks Who Brought {AI} to Google, Facebook, and the World</cite>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite metz_genius_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-76" id="summaryabstract-76">Summary/Abstract</a></h1>
<div>This colorful page-turner puts artificial intelligence into a human perspective. Through the lives of Geoff Hinton and other major players, Metz explains this transformative technology and makes the quest thrilling.—Walter Isaacson, author of The Code {BreakerRecipient} of starred reviews in both Kirkus and Library {JournalTHE} {UNTOLD} {TECH} {STORY} {OF} {OUR} {TIME}   What does it mean to be smart? To be human? What do we really want from life and the intelligence we have, or might create?   With deep and exclusive reporting, across hundreds of interviews, New York Times Silicon Valley journalist Cade Metz brings you into the rooms where these questions are being answered. Where an extraordinarily powerful new artificial intelligence has been built into our biggest companies, our social discourse, and our daily lives, with few of us even noticing.     Long dismissed as a technology of the distant future, artificial intelligence was a project consigned to the fringes of the scientific community. Then two researchers changed everything. One was a sixty-four-year-old computer science professor who didn’t drive and didn’t fly because he could no longer sit down—but still made his way across North America for the moment that would define a new age of technology. The other was a thirty-six-year-old neuroscientist and chess prodigy who laid claim to being the greatest game player of all time before vowing to build a machine that could do anything the human brain could do.   They took two very different paths to that lofty goal, and they disagreed on how quickly it would arrive. But both were soon drawn into the heart of the tech industry. Their ideas drove a new kind of arms race, spanning Google, Microsoft, Facebook, and {OpenAI}, a new lab founded by Silicon Valley kingpin Elon Musk. But some believed that China would beat them all to the finish line.   Genius Makers dramatically presents the fierce conflict between national interests, shareholder value, the pursuit of scientific knowledge, and the very human concerns about privacy, security, bias, and prejudice. Like a great Victorian novel, this world of eccentric, brilliant, often unimaginably yet suddenly wealthy characters draws you into the most profound moral questions we can ask. And like a great mystery, it presents the story and facts that lead to a core, vital question:   How far will we let it go?</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="raffel_exploring_2019" class=ref>
<summary class=citation>
<a id="raffel_exploring_2019">[raffel_exploring_2019]</a> - Raffel, Colin, Shazeer, Noam, Roberts, Adam, Lee, Katherine, Narang, Sharan, Matena, Michael, Zhou, Yanqi, Li, Wei, Liu, Peter J. - <a href="http://arxiv.org/abs/1910.10683" target="_blank"><cite>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite raffel_exploring_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-77" id="summaryabstract-77">Summary/Abstract</a></h1>
<div>Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our dataset, pre-trained models, and code.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="van_de_ven_generative_2019-1" class=ref>
<summary class=citation>
<a id="van_de_ven_generative_2019-1">[van_de_ven_generative_2019-1]</a> - van de Ven, Gido M., Tolias, Andreas S. - <a href="http://arxiv.org/abs/1809.10635" target="_blank"><cite>Generative replay with feedback connections as a general strategy for continual learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite van_de_ven_generative_2019-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-78" id="summaryabstract-78">Summary/Abstract</a></h1>
<div>A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted {MNIST} task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as soft targets) achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wooldridge_brief_2021" class=ref>
<summary class=citation>
<a id="wooldridge_brief_2021">[wooldridge_brief_2021]</a> - Wooldridge, Michael - <cite>A Brief History of Artificial Intelligence: What It Is, Where We Are, and Where We Are Going</cite>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite wooldridge_brief_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-79" id="summaryabstract-79">Summary/Abstract</a></h1>
<div>From Oxford&#x27;s leading {AI} researcher comes a fun and accessible tour through the history and future of one of the most cutting edge and misunderstood field in science: Artificial {IntelligenceThe} somewhat ill-defined long-term aim of {AI} is to build machines that are conscious, self-aware, and sentient; machines capable of the kind of intelligent autonomous action that currently only people are capable of. As an {AI} researcher with 25 years of experience, professor Mike Wooldridge has learned to be obsessively cautious about such claims, while still promoting an intense optimism about the future of the field. There have been genuine scientific breakthroughs that have made {AI} systems possible in the past decade that the founders of the field would have hailed as miraculous. Driverless cars and automated translation tools are just two examples of {AI} technologies that have become a practical, everyday reality in the past few years, and which will have a huge impact on our world.While the dream of conscious machines remains, Professor Wooldridge believes, a distant prospect, the floodgates for {AI} have opened. Wooldridge&#x27;s A Brief History of Artificial Intelligence is an exciting romp through the history of this groundbreaking field--a one-stop-shop for {AI}&#x27;s past, present, and world-changing future.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kirkpatrick_overcoming_2017" class=ref>
<summary class=citation>
<a id="kirkpatrick_overcoming_2017">[kirkpatrick_overcoming_2017]</a> - Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A., Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka, Hassabis, Demis, Clopath, Claudia, Kumaran, Dharshan, Hadsell, Raia - <a href="http://arxiv.org/abs/1612.00796" target="_blank"><cite>Overcoming catastrophic forgetting in neural networks</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite kirkpatrick_overcoming_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-80" id="summaryabstract-80">Summary/Abstract</a></h1>
<div>The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the {MNIST} hand written digit dataset and by learning several Atari 2600 games sequentially.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="brittain_prioritized_nodate" class=ref>
<summary class=citation>
<a id="brittain_prioritized_nodate">[brittain_prioritized_nodate]</a> - Brittain, Marc, Bertram, Josh, Yang, Xuxi, Wei, Peng - <cite>Prioritized Sequence Experience Replay</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite brittain_prioritized_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-81" id="summaryabstract-81">Summary/Abstract</a></h1>
<div>Experience replay is widely used in deep reinforcement learning algorithms and allows agents to remember and learn from experiences from the past. In an effort to learn more efﬁciently, researchers proposed prioritized experience replay ({PER}) which samples important transitions more frequently. In this paper, we propose Prioritized Sequence Experience Replay ({PSER}) a framework for prioritizing sequences of experience in an attempt to both learn more efﬁciently and to obtain better performance. We compare the performance of {PER} and {PSER} sampling techniques in a tabular Q-learning environment and in {DQN} on the Atari 2600 benchmark. We prove theoretically that {PSER} is guaranteed to converge faster than {PER} and empirically show {PSER} substantially improves upon {PER}.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="karpathy_large-scale_nodate" class=ref>
<summary class=citation>
<a id="karpathy_large-scale_nodate">[karpathy_large-scale_nodate]</a> - Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, Fei-Fei, Li - <cite>Large-scale Video Classiﬁcation with Convolutional Neural Networks</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite karpathy_large-scale_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-82" id="summaryabstract-82">Summary/Abstract</a></h1>
<div>Convolutional Neural Networks ({CNNs}) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of {CNNs} on largescale video classiﬁcation using a new dataset of 1 million {YouTube} videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a {CNN} in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display signiﬁcant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the {UCF}101 Action Recognition dataset and observe signiﬁcant performance improvements compared to the {UCF}-101 baseline model (63.3\% up from 43.9\%).</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shoeybi_megatron-lm:_2019" class=ref>
<summary class=citation>
<a id="shoeybi_megatron-lm:_2019">[shoeybi_megatron-lm:_2019]</a> - Shoeybi, Mohammad, Patwary, Mostofa, Puri, Raul, {LeGresley}, Patrick, Casper, Jared, Catanzaro, Bryan - <a href="https://arxiv.org/abs/1909.08053v3" target="_blank"><cite>Megatron-{LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite shoeybi_megatron-lm:_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-83" id="summaryabstract-83">Summary/Abstract</a></h1>
<div>Recent work in unsupervised language modeling demonstrates that training
large neural language models advances the state of the art in Natural Language
Processing applications. However, for very large models, memory constraints
limit the size of models that can be practically trained. Model parallelism
allows us to train larger models, because the parameters can be split across
multiple processors. In this work, we implement a simple, efficient intra-layer
model parallel approach that enables training state of the art transformer
language models with billions of parameters. Our approach does not require a
new compiler or library changes, is orthogonal and complimentary to pipeline
model parallelism, and can be fully implemented with the insertion of a few
communication operations in native {PyTorch}. We illustrate this approach by
converging an 8.3 billion parameter transformer language model using 512 {GPUs},
making it the largest transformer model ever trained at 24x times the size of
{BERT} and 5.6x times the size of {GPT}-2. We sustain up to 15.1 {PetaFLOPs} per
second across the entire application with 76\% scaling efficiency, compared to a
strong single processor baseline that sustains 39 {TeraFLOPs} per second, which
is 30\% of peak {FLOPs}. The model is trained on 174GB of text, requiring 12
{ZettaFLOPs} over 9.2 days to converge. Transferring this language model achieves
state of the art ({SOTA}) results on the {WikiText}103 (10.8 compared to {SOTA}
perplexity of 16.4) and {LAMBADA} (66.5\% compared to {SOTA} accuracy of 63.2\%)
datasets. We release training and evaluation code, as well as the weights of
our smaller portable model, for reproducibility.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="biesialska_continual_2020" class=ref>
<summary class=citation>
<a id="biesialska_continual_2020">[biesialska_continual_2020]</a> - Biesialska, Magdalena, Biesialska, Katarzyna, Costa-jussà, Marta R. - <a href="http://arxiv.org/abs/2012.09823" target="_blank"><cite>Continual Lifelong Learning in Natural Language Processing: A Survey</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite biesialska_continual_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-84" id="summaryabstract-84">Summary/Abstract</a></h1>
<div>Continual learning ({CL}) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, {CL} is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of {CL} through the lens of various {NLP} tasks. Our survey discusses major challenges in {CL} and current methods applied in neural network models. We also provide a critical review of the existing {CL} evaluation methods and datasets in {NLP}. Finally, we present our outlook on future research directions.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="xie_adversarial_nodate" class=ref>
<summary class=citation>
<a id="xie_adversarial_nodate">[xie_adversarial_nodate]</a> - Xie, Cihang, Tan, Mingxing, Gong, Boqing, Wang, Jiang, Yuille, Alan, Le, Quoc V - <cite>Adversarial Examples Improve Image Recognition</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite xie_adversarial_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-85" id="summaryabstract-85">Summary/Abstract</a></h1>
<div>Adversarial examples are commonly viewed as a threat to {ConvNets}. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose {AdvProp}, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overﬁtting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that {AdvProp} improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying {AdvProp} to the latest {EfﬁcientNet}-B7 [28] on {ImageNet}, we achieve signiﬁcant improvements on {ImageNet} (+0.7\%), {ImageNet}-C (+6.5\%), {ImageNet}-A (+7.0\%), {StylizedImageNet} (+4.8\%). With an enhanced {EfﬁcientNet}-B8, our method achieves the state-of-the-art 85.5\% {ImageNet} top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (∼3000× more than {ImageNet}) and ∼9.4× more parameters. Models are available at https://github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="ruffy_state_2019" class=ref>
<summary class=citation>
<a id="ruffy_state_2019">[ruffy_state_2019]</a> - Ruffy, Fabian, Chahal, Karanbir - <a href="https://arxiv.org/abs/1912.10850v1" target="_blank"><cite>The State of Knowledge Distillation for Classification</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite ruffy_state_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-86" id="summaryabstract-86">Summary/Abstract</a></h1>
<div>We survey various knowledge distillation ({KD}) strategies for simple
classification tasks and implement a set of techniques that claim
state-of-the-art accuracy. Our experiments using standardized model
architectures, fixed compute budgets, and consistent training schedules
indicate that many of these distillation results are hard to reproduce. This is
especially apparent with methods using some form of feature distillation.
Further examination reveals a lack of generalizability where these techniques
may only succeed for specific architectures and training settings. We observe
that appropriately tuned classical distillation in combination with a data
augmentation training scheme gives an orthogonal improvement over other
techniques. We validate this approach and open-source our code.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wu_incremental_2018" class=ref>
<summary class=citation>
<a id="wu_incremental_2018">[wu_incremental_2018]</a> - Wu, Yue, Chen, Yinpeng, Wang, Lijuan, Ye, Yuancheng, Liu, Zicheng, Guo, Y, ong, Zhang, Zhengyou, Fu, Yun - <a href="http://arxiv.org/abs/1802.00853" target="_blank"><cite>Incremental Classifier Learning with Generative Adversarial Networks</cite></a>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite wu_incremental_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-87" id="summaryabstract-87">Summary/Abstract</a></h1>
<div>In this paper, we address the incremental classifier learning problem, which suffers from catastrophic forgetting. The main reason for catastrophic forgetting is that the past data are not available during learning. Typical approaches keep some exemplars for the past classes and use distillation regularization to retain the classification capability on the past classes and balance the past and new classes. However, there are four main problems with these approaches. First, the loss function is not efficient for classification. Second, there is unbalance problem between the past and new classes. Third, the size of pre-decided exemplars is usually limited and they might not be distinguishable from unseen new classes. Forth, the exemplars may not be allowed to be kept for a long time due to privacy regulations. To address these problems, we propose (a) a new loss function to combine the cross-entropy loss and distillation loss, (b) a simple way to estimate and remove the unbalance between the old and new classes , and (c) using Generative Adversarial Networks ({GANs}) to generate historical data and select representative exemplars during generation. We believe that the data generated by {GANs} have much less privacy issues than real images because {GANs} do not directly copy any real image patches. We evaluate the proposed method on {CIFAR}-100, Flower-102, and {MS}-Celeb-1M-Base datasets and extensive experiments demonstrate the effectiveness of our method.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zenke_continual_2017-1" class=ref>
<summary class=citation>
<a id="zenke_continual_2017-1">[zenke_continual_2017-1]</a> - Zenke, Friedemann, Poole, Ben, Ganguli, Surya - <a href="https://arxiv.org/abs/1703.04200v3" target="_blank"><cite>Continual Learning Through Synaptic Intelligence</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite zenke_continual_2017-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-88" id="summaryabstract-88">Summary/Abstract</a></h1>
<div>While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="synced_google_2019" class=ref>
<summary class=citation>
<a id="synced_google_2019">[synced_google_2019]</a> - Synced - <a href="https://medium.com/syncedreview/google-t5-explores-the-limits-of-transfer-learning-a87afbf2615b" target="_blank"><cite>Google T5 Explores the Limits of Transfer Learning</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite synced_google_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-89" id="summaryabstract-89">Summary/Abstract</a></h1>
<div>A Google research team recently published the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="blank_quantum_2020" class=ref>
<summary class=citation>
<a id="blank_quantum_2020">[blank_quantum_2020]</a> - Blank, Carsten, Park, Daniel K., Rhee, June-Koo Kevin, Petruccione, Francesco - <a href="https://www.nature.com/articles/s41534-020-0272-6" target="_blank"><cite>Quantum classifier with tailored quantum kernel</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite blank_quantum_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-90" id="summaryabstract-90">Summary/Abstract</a></h1>
<div>Kernel methods have a wide spectrum of applications in machine learning. Recently, a link between quantum computing and kernel theory has been formally established, opening up opportunities for quantum techniques to enhance various existing machine-learning methods. We present a distance-based quantum classifier whose kernel is based on the quantum state fidelity between training and test data. The quantum kernel can be tailored systematically with a quantum circuit to raise the kernel to an arbitrary power and to assign arbitrary weights to each training data. Given a specific input state, our protocol calculates the weighted power sum of fidelities of quantum data in quantum parallel via a swap-test circuit followed by two single-qubit measurements, requiring only a constant number of repetitions regardless of the number of data. We also show that our classifier is equivalent to measuring the expectation value of a Helstrom operator, from which the well-known optimal quantum state discrimination can be derived. We demonstrate the performance of our classifier via classical simulations with a realistic noise model and proof-of-principle experiments using the {IBM} quantum cloud platform.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="klein_opennmt_2017" class=ref>
<summary class=citation>
<a id="klein_opennmt_2017">[klein_opennmt_2017]</a> - Klein, Guillaume, Kim, Yoon, Deng, Yuntian, Senellart, Jean, Rush, Alex, er - <a href="http://aclweb.org/anthology/P17-4012" target="_blank"><cite>{OpenNMT}: Open-Source Toolkit for Neural Machine Translation</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite klein_opennmt_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-91" id="summaryabstract-91">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_ganfather_nodate" class=ref>
<summary class=citation>
<a id="noauthor_ganfather_nodate">[noauthor_ganfather_nodate]</a> - N/A - <a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/" target="_blank"><cite>The {GANfather}: The man who’s given machines the gift of imagination</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_ganfather_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-92" id="summaryabstract-92">Summary/Abstract</a></h1>
<div>One night in 2014, Ian Goodfellow went drinking to celebrate with a fellow doctoral student who had just graduated. At Les 3 Brasseurs (The Three Brewers), a favorite Montreal watering hole, some friends asked for his help with a thorny project they were working on: a computer that could create photos by itself. Researchers were…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zhuang_comprehensive_2020" class=ref>
<summary class=citation>
<a id="zhuang_comprehensive_2020">[zhuang_comprehensive_2020]</a> - Zhuang, Fuzhen, Qi, Zhiyuan, Duan, Keyu, Xi, Dongbo, Zhu, Yongchun, Zhu, Hengshu, Xiong, Hui, He, Qing - <a href="http://arxiv.org/abs/1911.02685" target="_blank"><cite>A Comprehensive Survey on Transfer Learning</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zhuang_comprehensive_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-93" id="summaryabstract-93">Summary/Abstract</a></h1>
<div>Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_using_nodate" class=ref>
<summary class=citation>
<a id="noauthor_using_nodate">[noauthor_using_nodate]</a> - N/A - <a href="https://www.confluent.io/blog/using-apache-kafka-drive-cutting-edge-machine-learning" target="_blank"><cite>Using Apache Kafka to Drive Cutting-Edge Machine Learning {\textbar} Confluent</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_using_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-94" id="summaryabstract-94">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_enforcing_nodate" class=ref>
<summary class=citation>
<a id="noauthor_enforcing_nodate">[noauthor_enforcing_nodate]</a> - N/A - <a href="https://stackoverflow.com/questions/19534896/enforcing-python-version-in-setup-py" target="_blank"><cite>Enforcing python version in setup.py</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_enforcing_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-95" id="summaryabstract-95">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="liu_multi-task_2019" class=ref>
<summary class=citation>
<a id="liu_multi-task_2019">[liu_multi-task_2019]</a> - Liu, Xiaodong, He, Pengcheng, Chen, Weizhu, Gao, Jianfeng - <a href="https://www.aclweb.org/anthology/P19-1441" target="_blank"><cite>Multi-Task Deep Neural Networks for Natural Language Understanding</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite liu_multi-task_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-96" id="summaryabstract-96">Summary/Abstract</a></h1>
<div>In this paper, we present a Multi-Task Deep Neural Network ({MT}-{DNN}) for learning representations across multiple natural language understanding ({NLU}) tasks. {MT}-{DNN} not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. {MT}-{DNN} extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as {BERT} (Devlin et al., 2018). {MT}-{DNN} obtains new state-of-the-art results on ten {NLU} tasks, including {SNLI}, {SciTail}, and eight out of nine {GLUE} tasks, pushing the {GLUE} benchmark to 82.7\% (2.2\% absolute improvement) as of February 25, 2019 on the latest {GLUE} test set. We also demonstrate using the {SNLI} and {SciTail} datasets that the representations learned by {MT}-{DNN} allow domain adaptation with substantially fewer in-domain labels than the pre-trained {BERT} representations. Our code and pre-trained models will be made publicly available.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="keskar_ctrl:_nodate" class=ref>
<summary class=citation>
<a id="keskar_ctrl:_nodate">[keskar_ctrl:_nodate]</a> - Keskar, Nitish Shirish, {McCann}, Bryan, Varshney, Lav R, Xiong, Caiming, Socher, Richard - <cite>{CTRL}: A {CONDITIONAL} {TRANSFORMER} {LANGUAGE} {MODEL} {FOR} {CONTROLLABLE} {GENERATION}</cite>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite keskar_ctrl:_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-97" id="summaryabstract-97">Summary/Abstract</a></h1>
<div>Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release {CTRL}, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-speciﬁc behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow {CTRL} to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of {CTRL} at https://github.com/salesforce/ctrl.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="arute_quantum_2019" class=ref>
<summary class=citation>
<a id="arute_quantum_2019">[arute_quantum_2019]</a> - Arute, Frank, Arya, Kunal, Babbush, Ryan, Bacon, Dave, Bardin, Joseph C., Barends, Rami, Biswas, Rupak, Boixo, Sergio, Br, ao, Fern, o G. S. L., Buell, David A., Burkett, Brian, Chen, Yu, Chen, Zijun, Chiaro, Ben, Collins, Roberto, Courtney, William, Dunsworth, Andrew, Farhi, Edward, Foxen, Brooks, Fowler, Austin, Gidney, Craig, Giustina, Marissa, Graff, Rob, Guerin, Keith, Habegger, Steve, Harrigan, Matthew P., Hartmann, Michael J., Ho, Alan, Hoffmann, Markus, Huang, Trent, Humble, Travis S., Isakov, Sergei V., Jeffrey, Evan, Jiang, Zhang, Kafri, Dvir, Kechedzhi, Kostyantyn, Kelly, Julian, Klimov, Paul V., Knysh, Sergey, Korotkov, Alex, er, Kostritsa, Fedor, L, huis, David, Lindmark, Mike, Lucero, Erik, Lyakh, Dmitry, M, rà, Salvatore, {McClean}, Jarrod R., {McEwen}, Matthew, Megrant, Anthony, Mi, Xiao, Michielsen, Kristel, Mohseni, Masoud, Mutus, Josh, Naaman, Ofer, Neeley, Matthew, Neill, Charles, Niu, Murphy Yuezhen, Ostby, Eric, Petukhov, Andre, Platt, John C., Quintana, Chris, Rieffel, Eleanor G., Roushan, Pedram, Rubin, Nicholas C., Sank, Daniel, Satzinger, Kevin J., Smelyanskiy, Vadim, Sung, Kevin J., Trevithick, Matthew D., Vainsencher, Amit, Villalonga, Benjamin, White, Theodore, Yao, Z. Jamie, Yeh, Ping, Zalcman, Adam, Neven, Hartmut, Martinis, John M. - <a href="http://www.nature.com/articles/s41586-019-1666-5" target="_blank"><cite>Quantum supremacy using a programmable superconducting processor</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite arute_quantum_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-98" id="summaryabstract-98">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="keitakurita_building_2019" class=ref>
<summary class=citation>
<a id="keitakurita_building_2019">[keitakurita_building_2019]</a> - keitakurita, Author - <a href="https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/" target="_blank"><cite>Building the Transformer {XL} from Scratch</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite keitakurita_building_2019 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-99" id="summaryabstract-99">Summary/Abstract</a></h1>
<div>With the release of {XLNet}, the Transformer {XL} is the new cool kid on the block. Although the Transformer {XL} is simple in concept, actually understanding the details is harder than might meet the ey…</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zaheer_big_2020" class=ref>
<summary class=citation>
<a id="zaheer_big_2020">[zaheer_big_2020]</a> - Zaheer, Manzil, Guruganesh, Guru, Dubey, Avinava, Ainslie, Joshua, Alberti, Chris, Ontanon, Santiago, Pham, Philip, Ravula, Anirudh, Wang, Qifan, Yang, Li, Ahmed, Amr - <a href="http://arxiv.org/abs/2007.14062" target="_blank"><cite>Big Bird: Transformers for Longer Sequences</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite zaheer_big_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-100" id="summaryabstract-100">Summary/Abstract</a></h1>
<div>Transformers-based models, such as {BERT}, have been one of the most successful deep learning models for {NLP}. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, {BigBird}, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that {BigBird} is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as {CLS}), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, {BigBird} drastically improves performance on various {NLP} tasks such as question answering and summarization. We also propose novel applications to genomics data.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="shin_continual_2017" class=ref>
<summary class=citation>
<a id="shin_continual_2017">[shin_continual_2017]</a> - Shin, Hanul, Lee, Jung Kwon, Kim, Jaehong, Kim, Jiwon - <a href="http://arxiv.org/abs/1705.08690" target="_blank"><cite>Continual Learning with Deep Generative Replay</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite shin_continual_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-101" id="summaryabstract-101">Summary/Abstract</a></h1>
<div>Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (generator) and a task solving model (solver). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="goodfellow_generative_2014" class=ref>
<summary class=citation>
<a id="goodfellow_generative_2014">[goodfellow_generative_2014]</a> - Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, Bengio, Yoshua - <a href="http://arxiv.org/abs/1406.2661" target="_blank"><cite>Generative Adversarial Networks</cite></a>. - 2014. -
<button onclick="copyToClipboard('\{\{ #cite goodfellow_generative_2014 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-102" id="summaryabstract-102">Summary/Abstract</a></h1>
<div>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="vaswani_attention_2017" class=ref>
<summary class=citation>
<a id="vaswani_attention_2017">[vaswani_attention_2017]</a> - Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N., Kaiser, Lukasz, Polosukhin, Illia - <a href="http://arxiv.org/abs/1706.03762" target="_blank"><cite>Attention Is All You Need</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite vaswani_attention_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-103" id="summaryabstract-103">Summary/Abstract</a></h1>
<div>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="wolfram_class_2020" class=ref>
<summary class=citation>
<a id="wolfram_class_2020">[wolfram_class_2020]</a> - Wolfram, Stephen - <a href="http://arxiv.org/abs/2004.08210" target="_blank"><cite>A Class of Models with the Potential to Represent Fundamental Physics</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite wolfram_class_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-104" id="summaryabstract-104">Summary/Abstract</a></h1>
<div>A class of models intended to be as minimal and structureless as possible is introduced. Even in cases with simple rules, rich and complex behavior is found to emerge, and striking correspondences to some important core known features of fundamental physics are seen, suggesting the possibility that the models may provide a new approach to finding a fundamental theory of physics.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="minsky_framework_1974" class=ref>
<summary class=citation>
<a id="minsky_framework_1974">[minsky_framework_1974]</a> - Minsky, Marvin - <a href="https://web.media.mit.edu/~minsky/papers/Frames/frames.html" target="_blank"><cite>A Framework for Representing Knowledge</cite></a>. - 1974. -
<button onclick="copyToClipboard('\{\{ #cite minsky_framework_1974 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-105" id="summaryabstract-105">Summary/Abstract</a></h1>
<div>N/A</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="parisi_continual_2019-1" class=ref>
<summary class=citation>
<a id="parisi_continual_2019-1">[parisi_continual_2019-1]</a> - Parisi, German I., Kemker, Ronald, Part, Jose L., Kanan, Christopher, Wermter, Stefan - <a href="http://arxiv.org/abs/1802.07569" target="_blank"><cite>Continual Lifelong Learning with Neural Networks: A Review</cite></a>. - 2019. -
<button onclick="copyToClipboard('\{\{ #cite parisi_continual_2019-1 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-106" id="summaryabstract-106">Summary/Abstract</a></h1>
<div>Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="real_automl-zero_2020" class=ref>
<summary class=citation>
<a id="real_automl-zero_2020">[real_automl-zero_2020]</a> - Real, Esteban, Liang, Chen, So, David R., Le, Quoc V. - <a href="http://arxiv.org/abs/2003.03384" target="_blank"><cite>{AutoML}-Zero: Evolving Machine Learning Algorithms From Scratch</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite real_automl-zero_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-107" id="summaryabstract-107">Summary/Abstract</a></h1>
<div>Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as {AutoML}, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that {AutoML} can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. {CIFAR}-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="alyafeai_survey_2020" class=ref>
<summary class=citation>
<a id="alyafeai_survey_2020">[alyafeai_survey_2020]</a> - Alyafeai, Zaid, {AlShaibani}, Maged Saeed, Ahmad, Irfan - <a href="http://arxiv.org/abs/2007.04239" target="_blank"><cite>A Survey on Transfer Learning in Natural Language Processing</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite alyafeai_survey_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-108" id="summaryabstract-108">Summary/Abstract</a></h1>
<div>Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging {NLP} tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of {NLP}. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="kitaev_reformer_2020" class=ref>
<summary class=citation>
<a id="kitaev_reformer_2020">[kitaev_reformer_2020]</a> - Kitaev, Nikita, Kaiser, Łukasz, Levskaya, Anselm - <a href="http://arxiv.org/abs/2001.04451" target="_blank"><cite>Reformer: The Efficient Transformer</cite></a>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite kitaev_reformer_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-109" id="summaryabstract-109">Summary/Abstract</a></h1>
<div>Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L{\textasciicircum}2\$) to O(\$L{\textbackslash}log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="noauthor_more_nodate" class=ref>
<summary class=citation>
<a id="noauthor_more_nodate">[noauthor_more_nodate]</a> - N/A - <a href="http://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" target="_blank"><cite>More Efficient {NLP} Model Pre-training with {ELECTRA}</cite></a>. - N/A. -
<button onclick="copyToClipboard('\{\{ #cite noauthor_more_nodate \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-110" id="summaryabstract-110">Summary/Abstract</a></h1>
<div>Posted by Kevin Clark, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team   Recent advances in langu...</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="sutton_reinforcement_2018" class=ref>
<summary class=citation>
<a id="sutton_reinforcement_2018">[sutton_reinforcement_2018]</a> - Sutton, Richard S., Barto, Andrew G. - <cite>Reinforcement Learning, second edition: An Introduction</cite>. - 2018. -
<button onclick="copyToClipboard('\{\{ #cite sutton_reinforcement_2018 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-111" id="summaryabstract-111">Summary/Abstract</a></h1>
<div>The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field&#x27;s key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including {UCB}, Expected Sarsa, and Double Learning. Part {II} extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part {III} has new chapters on reinforcement learning&#x27;s relationships to psychology and neuroscience, as well as an updated case-studies chapter including {AlphaGo} and {AlphaGo} Zero, Atari game playing, and {IBM} Watson&#x27;s wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="lopez-paz_gradient_2017" class=ref>
<summary class=citation>
<a id="lopez-paz_gradient_2017">[lopez-paz_gradient_2017]</a> - Lopez-Paz, David, Ranzato, Marc&#x27;Aurelio - <a href="http://arxiv.org/abs/1706.08840" target="_blank"><cite>Gradient Episodic Memory for Continual Learning</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite lopez-paz_gradient_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-112" id="summaryabstract-112">Summary/Abstract</a></h1>
<div>One major obstacle towards {AI} is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory ({GEM}) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the {MNIST} and {CIFAR}-100 datasets demonstrate the strong performance of {GEM} when compared to the state-of-the-art.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="zenke_continual_2017" class=ref>
<summary class=citation>
<a id="zenke_continual_2017">[zenke_continual_2017]</a> - Zenke, Friedemann, Poole, Ben, Ganguli, Surya - <a href="https://arxiv.org/abs/1703.04200v3" target="_blank"><cite>Continual Learning Through Synaptic Intelligence</cite></a>. - 2017. -
<button onclick="copyToClipboard('\{\{ #cite zenke_continual_2017 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-113" id="summaryabstract-113">Summary/Abstract</a></h1>
<div>While deep learning has led to remarkable advances across diverse
applications, it struggles in domains where the data distribution changes over
the course of learning. In stark contrast, biological neural networks
continually adapt to changing domains, possibly by leveraging complex molecular
machinery to solve many tasks simultaneously. In this study, we introduce
intelligent synapses that bring some of this biological complexity into
artificial neural networks. Each synapse accumulates task relevant information
over time, and exploits this information to rapidly store new memories without
forgetting old ones. We evaluate our approach on continual learning of
classification tasks, and show that it dramatically reduces forgetting while
maintaining computational efficiency.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="luo_have_2021" class=ref>
<summary class=citation>
<a id="luo_have_2021">[luo_have_2021]</a> - Luo, Ziyang - <a href="http://arxiv.org/abs/2102.07926" target="_blank"><cite>Have Attention Heads in {BERT} Learned Constituency Grammar?</cite></a>. - 2021. -
<button onclick="copyToClipboard('\{\{ #cite luo_have_2021 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-114" id="summaryabstract-114">Summary/Abstract</a></h1>
<div>With the success of pre-trained language models in recent years, more and more researchers focus on opening the black box of these models. Following this interest, we carry out a qualitative and quantitative analysis of constituency grammar in attention heads of {BERT} and {RoBERTa}. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist heads that can induce some grammar types much better than baselines, suggesting that some heads act as a proxy for constituency grammar. We also analyze how attention heads&#x27; constituency grammar inducing ({CGI}) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity ({SMS}) tasks and natural language inference ({NLI}) tasks. Our results suggest that {SMS} tasks decrease the average {CGI} ability of upper layers, while {NLI} tasks increase it. Lastly, we investigate the connections between {CGI} ability and natural language understanding ability on {QQP} and {MNLI} tasks.</div>
</section>
</details>
</div>
<br/>
<div class="bib_div">
<details data-key="pearl_book_2020" class=ref>
<summary class=citation>
<a id="pearl_book_2020">[pearl_book_2020]</a> - Pearl, Judea, Mackenzie, Dana - <cite>The Book of Why: The New Science of Cause and Effect</cite>. - 2020. -
<button onclick="copyToClipboard('\{\{ #cite pearl_book_2020 \}\}')">Copy citation_key</button>
</summary>
<section class=abstract>
<h1><a class="header" href="#summaryabstract-115" id="summaryabstract-115">Summary/Abstract</a></h1>
<div>A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence Correlation is not causation. This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl&#x27;s work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.</div>
</section>
</details>
</div>
<br/>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="vocabulary.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="vocabulary.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>

<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ML/DL Topics - AI Resources</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="applications.html"><strong aria-hidden="true">2.</strong> Applications</a></li><li class="chapter-item expanded "><a href="multidisciplinary_approach.html"><strong aria-hidden="true">3.</strong> A Multidisciplinary Approach</a></li><li class="chapter-item expanded "><a href="approaches.html"><strong aria-hidden="true">4.</strong> Approaches</a></li><li class="chapter-item expanded "><a href="topics.html" class="active"><strong aria-hidden="true">5.</strong> ML/DL Topics</a></li><li class="chapter-item expanded "><a href="nlp.html"><strong aria-hidden="true">6.</strong> NLP</a></li><li class="chapter-item expanded "><a href="productionizing.html"><strong aria-hidden="true">7.</strong> To Production</a></li><li class="chapter-item expanded "><a href="tools_and_frameworks.html"><strong aria-hidden="true">8.</strong> Frameworks</a></li><li class="chapter-item expanded "><a href="books_and_resources.html"><strong aria-hidden="true">9.</strong> Books and Resources</a></li><li class="chapter-item expanded "><a href="conferences.html"><strong aria-hidden="true">10.</strong> Conferences</a></li><li class="chapter-item expanded "><a href="vocabulary.html"><strong aria-hidden="true">11.</strong> Vocabulary</a></li><li class="chapter-item expanded "><a href="people.html"><strong aria-hidden="true">12.</strong> People</a></li><li class="chapter-item expanded "><a href="history.html"><strong aria-hidden="true">13.</strong> Timeline/History</a></li><li class="chapter-item expanded affix "><a href="bibliography.html">Bibliography</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">AI Resources</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#machinedeep-learning-topics" id="machinedeep-learning-topics">Machine/Deep Learning Topics</a></h1>
<h2><a class="header" href="#artificial-general-inteligence" id="artificial-general-inteligence">(Artificial) General Inteligence</a></h2>
<p>This has been, is, and will be for some more time at least, the dream of scientist/researchers/engineers in the field of artificial intelligence.</p>
<p>In short, achieving a general artificial intelligence assumes the ability of an agent to learn new tasks whilst maintaining a general capability on fulfilling previous learned tasks.</p>
<p>This artificial general intelligence requires that the agent does not forget what it has learn -what is called <a href="#catastrophic_forgetting">catastrophic forgetting</a></p>
<p>in the literature- and assumes that the agent will continue learning new tasks -called <a href="#continual-learning">continual or lifelong learning</a>.</p>
<p>Since the advent of ChatGPT on November 30th of 2022, there's been a lot of hype about this.</p>
<h2><a class="header" href="#loss-function" id="loss-function">Loss Function</a></h2>
<h3><a class="header" href="#mean-squared-error-mse" id="mean-squared-error-mse">Mean Squared Error (MSE)</a></h3>
<p>MSE measures the average squared difference between the actual values and the predicted values, penalizing larger errors more significantly due to the squaring of the differences.</p>
<p>$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$</p>
<ul>
<li>$n$ is the number of data points.</li>
<li>$y_i$ is the true value for the $i$ th data point.</li>
<li>$\hat{y}_i$ is the predicted value for the $i$ th data point.</li>
</ul>
<p><strong>Advantages</strong>: Easy to compute and differentiate. It is suitable for regression problems where the goal is to minimize the squared difference between predicted and actual values.</p>
<p><strong>Disadvantages</strong>: Sensitive to outliers, as large errors are squared, potentially leading to  overemphasis on outliers.</p>
<h3><a class="header" href="#cross-entropy-loss" id="cross-entropy-loss">Cross-Entropy Loss</a></h3>
<p>Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1. It increases as the predicted probability diverges from the actual label.</p>
<p>The formula for cross-entropy loss, often used in classification tasks, particularly with softmax output in neural networks, is given by:</p>
<p>For binary classification:</p>
<p>$$
\text{Cross-Entropy Loss} = - \left[ y \cdot \log(p) + (1 - y) \cdot \log(1 - p) \right]
$$</p>
<p>For multi-class classification:</p>
<p>$$
\text{Cross-Entropy Loss} = - \sum_{i=1}^{C} y_i \cdot \log(p_i)
$$</p>
<ul>
<li>$y$ (in binary classification) is the true label, either 0 or 1.</li>
<li>$p$ is the predicted probability of the positive class.</li>
<li>$y_i$ (in multi-class classification) is a binary indicator (0 or 1) if class $i$ is the correct classification.</li>
<li>$p_i$ is the predicted probability of class $i$.</li>
<li>$C$ is the total number of classes.</li>
</ul>
<p><strong>Advantages</strong>: Ideal for classification problems, particularly binary and multi-class classification. It measures the performance of a classification model whose output is a probability value between 0 and 1.</p>
<p><strong>Disadvantages</strong>: Can be sensitive to class imbalance and may lead to slower convergence in some cases.</p>
<h3><a class="header" href="#hinge-loss" id="hinge-loss">Hinge Loss</a></h3>
<p>The hinge loss penalizes predictions that are on the wrong side of the decision boundary or too close to it. For a correct prediction with a margin greater than 1, the hinge loss is zero; otherwise, the loss increases linearly as the prediction moves further away from the correct classification.</p>
<p>$$
\text{Hinge Loss} = \max(0, 1 - y \cdot f(x))
$$</p>
<ul>
<li>$y$ is the true label of the data point, which is typically either +1 or -1.</li>
<li>$f(x)$ is the predicted value (or the decision function output) for the input $x$.</li>
</ul>
<p><strong>Advantages</strong>: Commonly used with Support Vector Machines (SVMs) for classification tasks. It encourages correct classification with a margin.</p>
<p><strong>Disadvantages</strong>: Not suitable for regression tasks. It can be sensitive to the choice of the margin.</p>
<h4><a class="header" href="#papers" id="papers">Papers</a></h4>
<ul>
<li><a href="https://arxiv.org/abs/2006.13593">Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks (KDD 2020)</a>
Learns from prior training experiences in the form of DNN model states during to guide weight updates and improve DNN 
training performance. The retrospective loss seeks to ensure that the predictions at a particular training step are more 
similar to the ground truth than to the predictions from a previous training step (which has relatively
poorer performance). As training proceeds, minimizing this loss along with the task-specific loss, encourages the 
network parameters to move towards the optimal parameter state by pushing the training into tighter spaces around the 
optimum. Claims implementation is 3 lines of Pytorch code. Interesting paper</li>
</ul>
<h2><a class="header" href="#transfer-learning" id="transfer-learning">Transfer Learning</a></h2>
<p>Recent success of DL has been produced, among other reasons, for the big amount of labeled training data.
However, in general, this is not the approach to follow for AI solving different tasks.
Humans are good at learning from very few examples, so scientifically there's still &quot;something&quot; we still need to understand.</p>
<p>Approach of modern Transfer-Learning: </p>
<ol>
<li>Pre-train in large genral corpus, usually unsupervised (e.g. as in BERT)</li>
<li>Fine-Tunning on specific tasks with smaller (supervised) training sets.</li>
</ol>
<p>Approach of Hierarchical-Multilevel Classification:</p>
<ul>
<li><a href="https://www.aclweb.org/anthology/P19-1633/">Hierarchical Transfer Learning for Multi-label Text Classification (ACL 2019)</a></li>
<li><a href="https://arxiv.org/abs/2005.10996">Multi-Source Deep Domain Adaptation with Weak Supervision for Time-Series Sensor Data (KDD 2020)</a>
Interesting</li>
</ul>
<h3><a class="header" href="#surveys" id="surveys">Surveys</a></h3>
<ul>
<li><a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?ts=5c8d09e7#slide=id.g5888218f39_364_0">HuggingFace Presentation on Transfer Learning</a></li>
<li>2010 <a href="https://www.cse.ust.hk/%7Eqyang/Docs/2009/tkde_transfer_learning.pdf">A Survey on Transfer Learning</a></li>
<li>2018 <a href="https://arxiv.org/abs/1808.01974">A Survey on Deep Transfer Learning</a></li>
<li>2020 <a href="https://arxiv.org/abs/2007.04239">A Survey on Transfer Learning in Natural Language Processing</a></li>
</ul>
<h3><a class="header" href="#few-shot-learning-meta-learning" id="few-shot-learning-meta-learning">Few-shot Learning (Meta-learning)</a></h3>
<p>Meta-learning aims to train a general model in several learning tasks. The goal is that the resulting model has to be able to solve unseen tasks by using just 
a few training examples.</p>
<p>Concept of [shortcut learning]: You don't learn a task by completely understanding it but by taking &quot;shortcuts&quot; imitating.
e.g. when training on some math exercises in highschool because every year they had the same structure. </p>
<ul>
<li><a href="https://arxiv.org/abs/2004.07780">Shortcut Learning in Deep Neural Networks</a></li>
</ul>
<p>Meta-Learning -learning (how) to learn-: Find an algorithm <img src="https://render.githubusercontent.com/render/math?math=A"> that from a small input data (few-shot examples) <img src="https://render.githubusercontent.com/render/math?math=DS_{train}(x_i,y_i)"> can predict the output <img src="https://render.githubusercontent.com/render/math?math=y'"> of a new input 
<img src="https://render.githubusercontent.com/render/math?math=x'"></p>
<ul>
<li>
<p>Prototypical Networks <a href="">Prototypical Networks for Few-shot Learning</a>
Nearest centroid classification</p>
</li>
<li>
<p>MAML <a href="">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> 
Model-agnostic meta-learning algorithm compatible with any model trained with GD and applicable to a variety of different learning problems, including 
classification, regression, and reinforcement learning.</p>
</li>
<li>
<p><a href="">Selecting Relevant Features from a Multi-Domain Representation for Few-shot Learning</a></p>
</li>
</ul>
<p>Work from Hugo Larochelle at Google Brain:</p>
<ul>
<li><a href="">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</a></li>
<li><a href="">A Universal Representation Transformer Layer for Few-shot Image Classification</a></li>
</ul>
<h2><a class="header" href="#calibration" id="calibration">Calibration</a></h2>
<p>A measure of the confidence of the predictions of a model. Concept comes from weather forecast.</p>
<p>Main Methods:</p>
<ul>
<li>Platt Scaling</li>
<li>Matrix Vector Scaling</li>
<li>Temperature scaling</li>
</ul>
<p>Papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks (ICML 2017)</a></li>
<li><a href="https://papers.nips.cc/paper/9397-beyond-temperature-scaling-obtaining-well-calibrated-multi-class-probabilities-with-dirichlet-calibration.pdf">Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration (NeurIPS 2019)</a></li>
</ul>
<details>
  <summary>Blogs</summary>
 * [How and When to Use a Calibrated Classification Model with scikit-learn](https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/)
 * [Prediction & Calibration Techniques to Optimize Performance of Machine Learning Models](https://towardsdatascience.com/calibration-techniques-of-machine-learning-models-d4f1a9c7a9cf)
 * [Calibration in Machine Learning](https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555#:~:text=In%20this%20blog%20we%20will%20learn%20what%20is%20calibration%20and,when%20we%20should%20use%20it.&text=We%20calibrate%20our%20model%20when,output%20given%20by%20a%20system.)
 * [Calibration Tutorial (KDD 2020)](http://kdd2020.nplan.io/presentation) [Github](https://github.com/nplan-io/kdd2020-calibration)
</details>
<details>
  <summary>Videos</summary>
 * [Calibration Tutorial](https://www.youtube.com/watch?v=rhnqZV6eKlg&feature=youtu.be)
</details>
<h2><a class="header" href="#causality" id="causality">Causality</a></h2>
<ul>
<li><a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why (Judea Perl)</a></li>
<li><a href="https://github.com/DataForScience/CausalInference">Causal Inference Intro with Exercises</a></li>
<li><a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a>
Learning paradigm to estimate invariant correlations across multiple training distributions. IRM learns a data representation such that the optimal classifier, 
on top of that data representation, matches for all training distributions.</li>
</ul>
<h2><a class="header" href="#a-namecontinual_learningacontinual-learning-and-catastrophic-forgetting" id="a-namecontinual_learningacontinual-learning-and-catastrophic-forgetting"><a name="continual_learning"></a>Continual Learning and Catastrophic Forgetting</a></h2>
<p>In biology, <em>Continual Learning</em> refers to the process of continually gather, update, and transfer skills/knowledge throughout life (lifespan).</p>
<p>In ML, it is still a major research problem to solve the fact that neural networks use to catastrophically forget previously learned tasks when they are trained in new ones. This fact it is the main obstacle that prevents the equivalent of continual learning to be implemented in the field of artificial neural networks.</p>
<p><a href="https://www.aclweb.org/anthology/2020.coling-main.574.pdf">This is a summary of the recent (2021) advances in continual learning in NLP.</a></p>
<p><a href="people.html#terrence-sejnowsky">Sejnowski</a> et al. {{ cite tsuda_modeling_2020 }} have developed a NN
architecture that shows how
hierarchical gating supports adaptive learning while preserving memories from prior experience. 
They show also how when introducing damages in the model, it recapitulates disorders found on the 
human Prefrontal Cortex.</p>
<h3><a class="header" href="#a-namecatastrophic_forgettingaprotocolsstrategies-for-solving-catastrophic-forgetting-cf" id="a-namecatastrophic_forgettingaprotocolsstrategies-for-solving-catastrophic-forgetting-cf"><a name="catastrophic_forgetting"></a>Protocols/Strategies for Solving Catastrophic Forgetting (CF)</a></h3>
<p>One problem with all the different strategies proposed for solving CF is that the field lacks a framework for comparing the effectiveness of the techniques. This has been addressed by studies like <a href="refs.html#vandeven2019">vandeven2019</a> and <a href="refs.html#vandeven2019b">vandeven2019b</a>.</p>
<p>The approaches to solve Catastrophic Forgetting can be classified in:</p>
<h4><a class="header" href="#regularization-approaches" id="regularization-approaches">Regularization Approaches</a></h4>
<p>https://arxiv.org/pdf/1612.00796.pdf</p>
<h4><a class="header" href="#generative-replay" id="generative-replay">Generative Replay</a></h4>
<ul>
<li><a href="refs.html#parisi2020">Continual Lifelong Learning with Neural Networks:A Review</a></li>
<li><a href="https://www.nature.com/articles/s41467-020-17866-2.epdf?sharing_token=bkJqxr4qptypBkYehsw_FtRgN0jAjWel9jnR3ZoTv0NoUJpE84DVnSx_jyG1N8KQimOuCCtJtaDabIpjOWE47UccZTsgeeOekV8ng2BR-omuTPXahD4aCOiCIIfIO2IOB-qJOABLKf7BlAYsTBE8rCeZYZcKd0yuWJjlzAEc1G8%3D">Brain-inspired replay for continual learning with artiﬁcial neural networks (Nature, 2020)</a></li>
<li><a href="https://arxiv.org/pdf/1809.10635v2.pdf">Generative replay with feedback connections as a general strategy for continual learning (ICLR2019)</a></li>
<li><a href="https://arxiv.org/pdf/1904.07734.pdf">Three Scenarios for Continual Learning</a></li>
<li><a href="https://openreview.net/pdf?id=rklnDgHtDS">Compositional language continual learning</a></li>
</ul>
<h1><a class="header" href="#gans-and-creativity" id="gans-and-creativity">GANs and Creativity</a></h1>
<h1><a class="header" href="#research-topics" id="research-topics">Research Topics</a></h1>
<p>Unsupervised Learning</p>
<p>Reinforcement Learning</p>
<ul>
<li>Unsupervised RL</li>
<li>Meta-Reinforcement Learning</li>
</ul>
<h1><a class="header" href="#personalization" id="personalization">Personalization</a></h1>
<h2><a class="header" href="#problems" id="problems">Problems</a></h2>
<p>Model &lt;-&gt; User interaction</p>
<ul>
<li>In general a model recommends items, and user actions based on those recomendations are used as training data for improving the model.</li>
<li>Missrepresentation of New items - a group that suffers from algorithmic bias.</li>
<li>It's interesting to study how recommendation feedback loops disproportionally hurt users with minority preferences</li>
<li>Features:
<ul>
<li>Empiric (CTR)</li>
<li>User history</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#cold-start" id="cold-start">Cold start</a></h3>
<ul>
<li>Can be looked as a fairness problems</li>
<li>Metrics should be tailored to use cases</li>
<li>Fairness methods as a solution.</li>
<li>Fairness in new advertisers: measure the severity of the advertiser cold start using fairness metrics and use 
fairness methods to mitigate it.</li>
</ul>
<p>Individual fairness -&gt; Similar individuals should be treated similar by the algo
Group fairness -&gt; Individuals from the protected group should get similar treatment as individuals without the protected
attribute. Protected attribute, gender, race...</p>
<p>Methods: Balance for positive class and Calibration</p>
<p>Fairness correction techniques:</p>
<ul>
<li>Preprocessing: Correct the training data</li>
<li>inprocessing: Penalty term to the loss function</li>
<li>postprocessing: Apply corrections</li>
</ul>
<p>Address multi-side fairness: Satisfy constraints of all stakeholders (e.g. old/new advertisers). We take the side of the
new items.</p>
<p>Fairness in Ranking</p>
<p>Small changes in scores can lead to large changes in exposure
Static fairness constraints may cause harm in fairness over time
Decomposition of fairness in complex systems: Candidate Generation -&gt; Engagement A model, Engagement B model...</p>
<ul>
<li>Fairness doesn't necessarily decompose</li>
</ul>
<p>Faire recommendations with Biased Data (Thorsten Joachims)
History of ranking dates back to the 1960 for finding books in a library
In 1994 with search engines that moved to finding everything
Maximize the utility of the rankings for the users
In 2020, still we look for Maximize the utility of the rankings. But there are two sides for the utility: 1) for the users (buyers, listeners, readers) but also for the items 2) 
(sellers, artists, writers), etc.! That is the variety of use cases is more diverse</p>
<p>However utility maximization it may not be fair for many candidates, specially if the probability of the top candidates it's very close together</p>
<p>Fairness: If two items has similar merits, their exposure should be the same. There are endogenous and exogenous factors:
Fairness of exposure: Endogenous (merit), Exogenous (biases)</p>
<h1><a class="header" href="#recommendation-feedback-loops-disproportionally-hurt-users-with-minority-preferences" id="recommendation-feedback-loops-disproportionally-hurt-users-with-minority-preferences">Recommendation feedback loops disproportionally hurt users with minority preferences</a></h1>
<ul>
<li>Called &quot;Algorithmic confounding&quot;. Perspective
Users:
<ul>
<li>&quot;I don't get whwat I'm looking for&quot;</li>
<li>&quot;This sistem sucks&quot;
Company:</li>
<li>User segmentation
Technical:</li>
<li>Bad training and evaluation protocols</li>
</ul>
</li>
</ul>
<p>Recommendation feedback:</p>
<ul>
<li>Provokes homogeneization of user behaviour</li>
<li>Users experience losses in utility</li>
<li>Amplifies the impact of the rec system on the distribution of the item consumption</li>
</ul>
<p>Initial data may not be enough 
Poorly tuned models hurt user with minority preferences recommending items further from their preferences
A/B test can weaken overall performance. Too many of them can delay recommending the most relevant items.</p>
<p>Matrix factorization</p>
<h2><a class="header" href="#causal-modeling-applied-to-messaging-at-netflix" id="causal-modeling-applied-to-messaging-at-netflix">Causal modeling applied to messaging at Netflix</a></h2>
<h1><a class="header" href="#conversational-recommmendation-with-natural-language" id="conversational-recommmendation-with-natural-language">Conversational Recommmendation with Natural Language</a></h1>
<p>We want:</p>
<ul>
<li>Speaking the user language is essential for a conversational recommendation</li>
<li>Model that understands natural requests: &quot;I'd like to watch something relaxing&quot;</li>
</ul>
<p>Soft attribbutes: Property of an item that</p>
<ul>
<li>is not a verifiable fact</li>
<li>can be universally agreed on</li>
<li>meaningful to compare</li>
<li>we can say that one is great than the other</li>
</ul>
<p>Answers to questions</p>
<ul>
<li>Polar yes/no question
<ul>
<li>Direct answer</li>
<li>Indirect answer (e.g. for being polite). Can include more information. - Do you have kids? - I have 2 daughers
Richness of language</li>
</ul>
</li>
</ul>
<p>Explainable User Models (Why?)</p>
<ul>
<li>What does the system know about me</li>
<li>How does the system interprets that</li>
<li>How does it go from user model to recommendations</li>
</ul>
<p>Observing the language of the user requires new approaches for data collection and models</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="approaches.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="nlp.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="approaches.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="nlp.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
